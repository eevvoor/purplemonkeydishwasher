<!DOCTYPE html><html>
<head>
<title>\thechapter Introduction</title>
<!--Generated on Thu Sep 28 15:53:50 2017 by LaTeXML (version 0.8.2) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on September 28, 2017.-->

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../latexml/LaTeXML.css" type="text/css">
<link rel="stylesheet" href="../latexml/ltx-article.css" type="text/css">
<link rel="stylesheet" href="../latexml/ltx-ulem.css" type="text/css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Weighted Finite-State Methods for
Spell-Checking
and Correction
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tommi A Pirinen
</span></span>
</div>
<div class="ltx_date ltx_role_creation">September 28, 2017</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>

<p class="ltx_p">This dissertation is a large-scale study of spell/checking and correction
using finite/state technology. Finite-state spell/checking is a key method
for handling morphologically complex languages in a computationally
efficient manner. This dissertation discusses the technological and practical
considerations that are required for finite/state spell-checkers to be at
the same level as state-of-the-art non-finite/state spell-checkers.</p>

<p class="ltx_p">Three aspects of spell/checking are considered in the thesis: modelling of
correctly written words and word-forms with finite/state language
models, applying statistical information to finite/state language models
with a specific focus on morphologically complex languages, and modelling
misspellings and typing errors using finite/state automata-based error
models.</p>

<p class="ltx_p">The usability of finite/state spell-checkers as a viable alternative to
traditional non-finite-state solutions is demonstrated in a large-scale
evaluation of spell-checking speed and the quality using languages with
morphologically different natures. The selected languages display a
full range of typological complexity, from isolating English to
polysynthetic Greenlandic with agglutinative Finnish and the Saami languages
somewhere in between.</p>

<p class="ltx_p"><span class="ltx_text ltx_font_bold">Tiivistelmä</span></p>
    <span class="ltx_ERROR undefined">\textfinnish</span>

<p class="ltx_p">Tässä väitöskirjassa tutkin äärellistilaisten menetelmien käyttöä
oikaisuluvussa. Äärellistilaiset menetelmät mahdollistavat sananmuodostukseltaan
monimutkaisempien kielten, kuten suomen tai grönlannin, sanaston sujuvan
käsittelyn oikaisulukusovelluksissa. Käsittelen tutkielmassani tieteellisiä
ja käytännöllisiä toteutuksia, jotka ovat tarpeen, jotta tällaisia
sananmuodostukseltaan monimutkallisempia kieliä voisi käsitellä oikaisuluvussa
yhtä tehokkaasti kuin yksinkertaisempia kieliä, kuten englantia tai
muita indo-eurooppalaisia kieliä nyt käsitellään.</p>

<p class="ltx_p">Tutkielmassa esitellään kolme keskeistä tutkimusongelmaa, jotka koskevat
oikaisuluvun toteuttamista sanarakenteeltaan monimutkaisemmille kielille:
miten mallintaa oikeinkirjoitetut sanamuodot äärellistilaisin mallein,
miten soveltaa tilastollista mallinnusta monimutkaisiin sanarakenteisiin kuten
yhdyssanoihin, ja miten mallintaa kirjoitusvirheitä äärellistilaisin mentelmin.</p>

<p class="ltx_p">Tutkielman tuloksena esitän äärellistilaisia oikaisulukumenetelmiä soveltuvana
vaihtoehtona nykyisille oikaisulukimille, tämän todisteena esitän
mittaustuloksia, jotka näyttävät, että käyttämäni menetelmät toimivat niin
rakenteellisesti yksinkertaisille kielille kuten englannille yhtä hyvin kuin
nykyiset menetelmät että rakenteellisesti monimutkaisemmille kielille kuten
suomelle, saamelle ja jopa grönlannille riittävän hyvin tullakseen käytetyksi
tyypillisissä oikaisulukimissa.</p>

</div>
<span class="ltx_ERROR undefined">\setdefaultlanguage</span>
<div id="p1" class="ltx_para">
<p class="ltx_p">english<span class="ltx_ERROR undefined">\setotherlanguages</span>finnish,russian<span class="ltx_ERROR undefined">\makeglossaries</span><span class="ltx_ERROR undefined">\authorcontact</span><a href="tommi.pirinen@helsinki.fi" title="" class="ltx_ref ltx_url ltx_font_typewriter">tommi.pirinen@helsinki.fi</a><a href="http://www.helsinki.fi/%7etapirine" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.helsinki.fi/%7etapirine</a><span class="ltx_ERROR undefined">\pubtime</span>January2014<span class="ltx_ERROR undefined">\reportno</span>0<span class="ltx_ERROR undefined">\isbnpaperback</span>978-952-10-9694-5<span class="ltx_ERROR undefined">\isbnpdf</span>978-952-10-9695-2<span class="ltx_ERROR undefined">\printhouse</span>PicasetOy<span class="ltx_ERROR undefined">\pubpages</span>94<span class="ltx_ERROR undefined">\supervisorlist</span>KristerLindén,Helsinginyliopisto,Finland<span class="ltx_ERROR undefined">\preexaminera</span>Prof.MikelL.Forcada,Universitatd’Alacant,Spain<span class="ltx_ERROR undefined">\preexaminerb</span>Prof.LarsBorin,Göteborgsuniversitet,Sweden<span class="ltx_ERROR undefined">\opponent</span>Prof.LarsBorin,Göteborgsuniversitet,Sweden<span class="ltx_ERROR undefined">\custos</span>Prof.LauriCarlson,Helsinginyliopisto,Finland<span class="ltx_ERROR undefined">\generalterms</span>thesis,finite-state,spell-checking,languagemodel,morphology<span class="ltx_ERROR undefined">\additionalkeywords</span>statisticallanguagemodels,morphologicallycomplexlanguages<span class="ltx_ERROR undefined">\crcshort</span>F.1.1,I.2.7,I.7.1<span class="ltx_ERROR undefined">\crclong</span>
<span class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_p">F.1.1</span>
<span id="p1.p1" class="ltx_para">
<span class="ltx_p">Finite-StateAutomata
<span class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_p">I.3.1</span>
<span id="p1.p1.p1" class="ltx_para">
<span class="ltx_p">NaturalLanguageProcessing
<span class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_p">I.7.1</span>
<span id="p1.p1.p1.p1" class="ltx_para">
<span class="ltx_p">Spelling<span class="ltx_ERROR undefined">\permissionnotice</span>Academicdissertationtobepubliclydiscussed,byduepermissionoftheFacultyofArtsattheUniversityofHelsinkiinauditoriumXII(inlectureroomPIII),onthe<math id="p1.p1.p1.p1.m1" class="ltx_Math" alttext="28^{th}" display="inline"><msup><mn>28</mn><mrow><mi>t</mi><mo>⁢</mo><mi>h</mi></mrow></msup></math>ofFebruaryat12o’clock.</span>
</span></span></span>
</span></span></span>
</span></span></p>
</div>
<span class="ltx_ERROR undefined">\mainmatter</span>
<section id="thechapter-at-ID" class="ltx_chapter">
<h2 class="ltx_title ltx_title_chapter">Preface</h2>

<div id="thechapter-at-ID.p1" class="ltx_para">
<p class="ltx_p">Spell-checkers are very basic-level, commonly used practical programs. They ar
ubiquitous enough that nowadays they are in everyday use for most of us,
whether in office software suites, Facebook text fields or a mobile phone
autocorrect utility. The practical motivation for this thesis comes first and
foremost from the disappointment in contemporary applications for smaller, more
complex languages than English, such as my native language, Finnish. Why does
my telephone not know <em class="ltx_emph">my Finnish</em>? Even I could implement this better!
And so I did.</p>
</div>
<div id="thechapter-at-ID.p2" class="ltx_para">
<p class="ltx_p">The scientific background of the thesis builds on the research of the
computational linguistics scene at the University of Helsinki, starting from my
master’s thesis work, which was a finite/state model of the Finnish language.
Using this language model as part of a practical piece of end-user software,
and advancing towards the scientific treatment of problems when doing so, was a
very obvious and motivating incentive for a doctoral dissertation. While the
hard science behind the thesis is, in my opinion, delightfully simple, I
believe the findings are interesting for anyone working in computational
linguistics, or even natural language engineering, especially when it concerns
languages different from English, in terms of their complexity, availability
of resources, and so on.</p>
</div>
<div id="thechapter-at-ID.p3" class="ltx_para">
<p class="ltx_p">The improvements on, and methods for, spell/checking and correction presented
in the thesis should provide a good base for more efficient spell/checking in
the future, but they are not yet included in popular end-user applications –
because I believe the productification and social engineering required to make
it happen is not part of the research work. The current approaches have been
tested on colleagues, Helsinki students, and a select few brave alpha-testers
from Greenland, whose native language is well-known to be trickier to handle
than most languages. As the results of the testing were encouraging, we can
hope that the future brings better spell-checkers and autocorrections for those
of us with slightly more tricky native languages.</p>
</div>
<section id="Sx1" class="ltx_section">
<h3 class="ltx_title ltx_title_section">Acknowledgements</h3>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">Official thanks go to the FIN-CLARIN project for funding my research, and to
Yliopistotutkintolautakunta (University Examination Board), for providing
spelling error materials.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p class="ltx_p">On the academic level, there is an endless number of colleagues and
acquaintances met in numerous conferences, workshops and other academic events
throughout the world who have contributed to the thesis by giving ideas,
challenging my theories and writing supporting or rivalling software and
methods. I am grateful to each and everyone. What follows is a list of
specific contributions I can recall at the moment, by no means an exhaustive
list of thanks, and in no particular order. The first obvious contribution for
the whole thesis project comes from the research group and other colleagues at
the University of Helsinki, the HFST project for working on software, and the
FinnTreeBank and Finnish Wordnet project for providing that extra bit of help
with Finnish language model building. Specifically I would like to thank my
original master’s thesis advisor Kimmo Koskenniemi for introducing me to this
line of work, and my PhD thesis advisor and HFST project leader Krister Lindén
for keeping the goal in sight throughout the thesis project and helping me
manage my time and ideas. In the HFST team I owe gratitude to Sam Hardwick for
doing the majority of the software work in the <span class="ltx_text ltx_font_typewriter">hfstospell</span> branch of
the project, Miikka Silfverberg for providing the statistical and context-based
finite/state models that go beyond my expertise, and Erik Axelson for
maintaining the whole software behemoth. I am very grateful to Per Langgård
for picking up my contribution in LREC 2011 as a way to get the next version of
the Greenlandic speller to people and thus introducing me to the whole
intriguing problematic field of the computational modelling of poly-synthetic
languages. The thesis and its focus would be quite different without it. I
thank Jack Rueter for numerous fruitful discussions on all topics related to
Uralic languages, amongst others. I thank the whole of the Divvun /
Giellatekno project at the University of Tromsø for the computational
morphologies, which we have been slowly turning into spell-checkers, especially
Sjur Moshagen and Trond Trosterud for co-operation in this project. I also
thank the Apertium project and contributors for another fine source of
linguistic materials to turn into spell-checkers, and Francis Tyers for all his
help with Apertium-related issues, proofreading, and numerous fruitful
discussions. I thank Don Killian for proofreading, and all the discussions on
linguistic aspects of my studies. Finally, I would like to thank the
pre-examiners for their insightful feedback.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p class="ltx_p">On a personal level I am thankful to my family, my friends at the university of
Helsinki, especially those in student organisations like Aspekti and Hyrmy, and
the one deity who shares the initials with finite/state machines,
FSM – without them, none of this would have been possible.</p>
</div>
</section>
</section>
<section id="thechapter-at-IDa" class="ltx_chapter">
<h2 class="ltx_title ltx_title_chapter">Original Papers</h2>

<div id="thechapter-at-IDa.p1" class="ltx_para">
<p class="ltx_p">This thesis consists of an introduction and the following peer-reviewed
publications, which are referred to as Papers I–X in the text. These
publications are reproduced at the end of the print version of the thesis.</p>
</div>
<div id="thechapter-at-IDa.p2" class="ltx_para">
<ol id="I1" class="ltx_enumerate">
<li id="I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate"><cite class="ltx_cite ltx_citemacro_citealias"><a href="#bib.bib1" title="Weighted finite-state morphological analysis of Finnish compounds with HFST-LEXC" class="ltx_ref">I</a></cite></span>
<div id="I1.ix1.p1" class="ltx_para">
<p class="ltx_p">Krister Lindén and Tommi Pirinen.

<span class="ltx_bibblock">Weighted finite-state morphological analysis of Finnish
compounding with HFST-LEXC.

<span class="ltx_bibblock">In <em class="ltx_emph">Nodalida</em>, Odense, Denmark, 2009.
</span>
</span></p>
</div>
</li>
<li id="I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate"><cite class="ltx_cite ltx_citemacro_citealias"><a href="#bib.bib2a" title="Weighting finite-state morphological analyzers using HFST tools" class="ltx_ref">II</a></cite></span>
<div id="I1.ix2.p1" class="ltx_para">
<p class="ltx_p">Krister Lindén and Tommi Pirinen.

<span class="ltx_bibblock">Weighting finite-state morphological analyzers using HFST
tools.

<span class="ltx_bibblock">In <em class="ltx_emph">Finite-State Methods in Natural Language Processing</em>,
Pretoria, South Africa, 2009.

</span>
</span></p>
</div>
</li>
<li id="I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate"><cite class="ltx_cite ltx_citemacro_citealias"><a href="#bib.bib4a" title="Finite-state spell-checking with weighted language and error models" class="ltx_ref">III</a></cite></span>
<div id="I1.ix3.p1" class="ltx_para">
<p class="ltx_p">Tommi A Pirinen and Krister Lindén.

<span class="ltx_bibblock">Finite-state spell-checking with weighted language and error
models.

<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of the Seventh SaLTMiL workshop on
creation and use of basic lexical resources for less-resourced
languages</span>, Valletta, Malta, 2010.
</span>
</span></p>
</div>
</li>
<li id="I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate"><cite class="ltx_cite ltx_citemacro_citealias"><a href="#bib.bib3a" title="Building and using existing Hunspell dictionaries and TeX hyphenators as finite-state automata" class="ltx_ref">IV</a></cite></span>
<div id="I1.ix4.p1" class="ltx_para">
<p class="ltx_p">Tommi A Pirinen and Krister Lindén.

<span class="ltx_bibblock">Building and using existing Hunspell dictionaries and TeX
hyphenators as finite-state automata.

<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proccedings of Computational Linguistics -
Applications, 2010</span>, Wisła, Poland, 2010.
</span>
</span></p>
</div>
</li>
<li id="I1.ix5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate"><cite class="ltx_cite ltx_citemacro_citealias"><a href="#bib.bib5a" title="Creating and weighting Hunspell dictionaries as finite-state automata" class="ltx_ref">V</a></cite></span>
<div id="I1.ix5.p1" class="ltx_para">
<p class="ltx_p">Tommi A Pirinen and Krister Lindén.

<span class="ltx_bibblock">Creating and weighting Hunspell dictionaries as finite-state
automata.

<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Investigationes Linguisticæ</span>, 21, 2010.
</span>
</span></p>
</div>
</li>
<li id="I1.ix6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate"><cite class="ltx_cite ltx_citemacro_citealias"><a href="#bib.bib6" title="Modularisation of Finnish finite-state language descriptionâtowards wide collaboration in open source development of morphological analyser" class="ltx_ref">VI</a></cite></span>
<div id="I1.ix6.p1" class="ltx_para">
<p class="ltx_p">Tommi A Pirinen.

<span class="ltx_bibblock">Modularisation of Finnish finite-state language
description—towards wide collaboration in open-source development of
morphological analyser.

<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of Nodalida</span>, volume 18 of <span class="ltx_text ltx_font_italic">NEALT
proceedings</span>, Rīga, Latvia, 2011.

</span>
</span></p>
</div>
</li>
<li id="I1.ix7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate"><cite class="ltx_cite ltx_citemacro_citealias"><a href="#bib.bib7" title="Compiling Apertium morphological dictionaries with HFST and using them in HFST applications" class="ltx_ref">VII</a></cite></span>
<div id="I1.ix7.p1" class="ltx_para">
<p class="ltx_p">Tommi A Pirinen and Francis M. Tyers.

<span class="ltx_bibblock">Compiling apertium morphological dictionaries with HFST and
using them in HFST applications.

<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic">Language Technology for Normalisation of Less-Resourced
Languages</span>, 2012.
</span>
</span></p>
</div>
</li>
<li id="I1.ix8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate"><cite class="ltx_cite ltx_citemacro_citealias"><a href="#bib.bib9a" title="Improving finite-state spell-checker suggestions with part of speech n-grams" class="ltx_ref">VIII</a></cite></span>
<div id="I1.ix8.p1" class="ltx_para">
<p class="ltx_p">Tommi A Pirinen, Miikka Silfverberg, and Krister Lindén.

<span class="ltx_bibblock">Improving finite-state spell-checker suggestions with part of
speech <math id="I1.ix8.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams.

<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Computational Linguistics and Intelligent Text
Processing <math id="I1.ix8.p1.m2" class="ltx_Math" alttext="13^{\mathrm{th}}" display="inline"><msup><mn mathvariant="normal">13</mn><mi mathvariant="normal">th</mi></msup></math> International Conference</span>, 2012.
</span>
</span></p>
</div>
</li>
<li id="I1.ix9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate"><cite class="ltx_cite ltx_citemacro_citealias"><a href="#bib.bib8a" title="Effect of language and error models on efficiency of finite-state spell-checking and correction" class="ltx_ref">IX</a></cite></span>
<div id="I1.ix9.p1" class="ltx_para">
<p class="ltx_p">Tommi A Pirinen and Sam Hardwick.

<span class="ltx_bibblock">Effect of language and error models on
efficiency of finite-state spell-checking.

<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic">Proceedings of FSMNLP</span>, Donostia–San Sebastían, Spain, 2012.
</span>
</span></p>
</div>
</li>
<li id="I1.ix10" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate"><cite class="ltx_cite ltx_citemacro_citealias"><a href="#bib.bib10" title="Quality and speed trade-offs in weighted finite-state spell-checking" class="ltx_ref">X</a></cite></span>
<div id="I1.ix10.p1" class="ltx_para">
<p class="ltx_p">Tommi A Pirinen.

<span class="ltx_bibblock">Quality and Speed Trade-Offs in
Finite-State Spell-Checking and Correction, <span class="ltx_text ltx_font_italic">forthcoming</span>

</span></p>
</div>
</li>
</ol>
</div>
<div id="thechapter-at-IDa.p3" class="ltx_para">
<p class="ltx_p">The implementations and data for reproducing the results are available
in the version control system of the HFST
project.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><a href="http://svn.code.sf.net/p/hfst/code/trunk/articles/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://svn.code.sf.net/p/hfst/code/trunk/articles/</a></span></span></span></p>
</div>
<div class="ltx_TOC ltx_role_contents">
<h6>Contents:</h6>
<ul class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_chapter">
<a href="#thechapter-at-IDb" title="Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_ERROR undefined">\thechapter</span> </span>Introduction</span></a>
<ul class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S1" title="1 Components of Spell-Checking and Correction ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Components of Spell-Checking and Correction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S2" title="2 Morphological Complexity ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Morphological Complexity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S3" title="3 Finite-State Technology in Natural Language Processing ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Finite-State Technology in Natural Language Processing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S4" title="4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Overview of Thesis Articles</span></a></li>
</ul>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a href="#thechapter-at-IDc" title="Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_ERROR undefined">\thechapter</span> </span>Background</span></a>
<ul class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S5" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Brief History and Prior Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S6" title="6 Related Subfields and Research Results ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Related Subfields and Research Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S7" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Theory of Finite-State Models</span></a></li>
</ul>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a href="#thechapter-at-IDd" title="Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_ERROR undefined">\thechapter</span> </span>Language Models</span></a>
<ul class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S8" title="8 Sketch of Generic Finite-State Formula for Morphology ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Sketch of Generic Finite-State Formula for Morphology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S9" title="9 Compiling Hunspell Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Compiling Hunspell Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S10" title="10 Using Rule-Based Machine Translation Dictionaries ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Using Rule-Based Machine Translation Dictionaries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S11" title="11 Other Possibilities for Generic Morphological Formula ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">11 </span>Other Possibilities for Generic Morphological Formula</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S12" title="12 Maintenance of the Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">12 </span>Maintenance of the Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S13" title="13 Conclusions ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">13 </span>Conclusions</span></a></li>
</ul>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a href="#thechapter-at-IDe" title="Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_ERROR undefined">\thechapter</span> </span>Statistical Language Models</span></a>
<ul class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S14" title="14 Theory of Statistical Finite-State Models ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">14 </span>Theory of Statistical Finite-State Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S15" title="15 Language Models for Languages with Compounding ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">15 </span>Language Models for Languages with Compounding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S16" title="16 The Statistical Training of Compounding Language Models ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">16 </span>The Statistical Training of Compounding Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S17" title="17 Weighting Hunspell as Finite-State Automata ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">17 </span>Weighting Hunspell as Finite-State Automata</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S18" title="18 Lesser-Resourced Languages in Statistical Spelling Correction ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">18 </span>Lesser-Resourced Languages in Statistical Spelling Correction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S19" title="19 Conclusions ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">19 </span>Conclusions</span></a></li>
</ul>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a href="#thechapter-at-IDf" title="Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_ERROR undefined">\thechapter</span> </span>Error Models</span></a>
<ul class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S20" title="20 The Application of Finite-State Error Models ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">20 </span>The Application of Finite-State Error Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S21" title="21 Edit Distance Measures ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">21 </span>Edit Distance Measures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S22" title="22 Hunspell Error Correction ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">22 </span>Hunspell Error Correction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S23" title="23 Phonemic Key Corrections ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">23 </span>Phonemic Key Corrections</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S24" title="24 Context-Aware Spelling Correction ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">24 </span>Context-Aware Spelling Correction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S25" title="25 Other Finite-State Error Models ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">25 </span>Other Finite-State Error Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S26" title="26 Conclusions ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">26 </span>Conclusions</span></a></li>
</ul>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a href="#thechapter-at-IDg" title="Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_ERROR undefined">\thechapter</span> </span>Efficiency of Finite-State Spell-Checking</span></a>
<ul class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S27" title="27 Speed of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">27 </span>Speed of Finite-State Spell-Checking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S28" title="28 Precision of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">28 </span>Precision of Finite-State Spell-Checking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S29" title="29 Conclusions ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">29 </span>Conclusions</span></a></li>
</ul>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter">
<a href="#thechapter-at-IDh" title="Chapter \thechapter Conclusion ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_ERROR undefined">\thechapter</span> </span>Conclusion</span></a>
<ul class="ltx_toclist ltx_toclist_chapter">
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S30" title="30 Scholarly Contributions ‣ Chapter \thechapter Conclusion ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">30 </span>Scholarly Contributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S31" title="31 Practical Contributions ‣ Chapter \thechapter Conclusion ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">31 </span>Practical Contributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S32" title="32 Future Work ‣ Chapter \thechapter Conclusion ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">32 </span>Future Work</span></a></li>
</ul>
</li>
<li class="ltx_tocentry ltx_tocentry_chapter"><a href="#thechapter-at-IDi" title="Chapter \thechapter Appendices ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_ERROR undefined">\thechapter</span> </span>Appendices</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a href="#A1" title="Appendix A Copyrights of Introduction and Articles ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Copyrights of Introduction and Articles</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a href="#A2" title="Appendix B Original Articles ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Original Articles</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a href="#A3" title="Appendix C Licence ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Licence</span></a></li>
</ul>
</div>
<div class="ltx_TOC ltx_role_tables">
<h6>List of Tables:</h6>
<ul class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_table"><a href="#S5.T1" title="Table 1 ‣ 5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>History of spell-checker applications. Abbreviation ED…</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a href="#S9.T2" title="Table 2 ‣ 9.3 Results ‣ 9 Compiling Hunspell Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Compiled Hunspell automata sizes in HFST.…</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a href="#S10.T3" title="Table 3 ‣ 10.3 Results ‣ 10 Using Rule-Based Machine Translation Dictionaries ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Efficiency of spelling correction in an…</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a href="#S24.T4" title="Table 4 ‣ 24.3 Results ‣ 24 Context-Aware Spelling Correction ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Precision of suggestion algorithms with real…</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a href="#S27.T5" title="Table 5 ‣ 27.3 Results ‣ 27 Speed of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Effect of language and error models…</span></a></li>
<li class="ltx_tocentry ltx_tocentry_table"><a href="#S28.T6" title="Table 6 ‣ 28.2 Results ‣ 28 Precision of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Effect of different language and error…</span></a></li>
</ul>
</div>
<div class="ltx_TOC ltx_role_figures">
<h6>List of Figures:</h6>
<ul class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#S4.F1" title="Figure 1 ‣ 4.5 Chapter Structure ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Articles and build order</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#S21.F2" title="Figure 2 ‣ 21 Edit Distance Measures ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>A non-deterministic unweighted transducer implementing edit…</span></a></li>
<li class="ltx_tocentry ltx_tocentry_figure"><a href="#S21.F3" title="Figure 3 ‣ 21 Edit Distance Measures ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>A Weighted automaton for measuring arbitrary…</span></a></li>
</ul>
</div>
<span class="ltx_ERROR undefined">\listoftodos</span><span class="ltx_ERROR undefined">\newacronym</span>
<div id="thechapter-at-IDa.p4" class="ltx_para">
<p class="ltx_p">ltLTlanguage technology</p>
</div>
<span class="ltx_ERROR undefined">\newacronym</span>
<div id="thechapter-at-IDa.p5" class="ltx_para">
<p class="ltx_p">clCLcomputational linguistics</p>
</div>
<span class="ltx_ERROR undefined">\newacronym</span>
<div id="thechapter-at-IDa.p6" class="ltx_para">
<p class="ltx_p">nleNLEnatural language engineering</p>
</div>
<span class="ltx_ERROR undefined">\newacronym</span>
<div id="thechapter-at-IDa.p7" class="ltx_para">
<p class="ltx_p">lmLMlanguage model</p>
</div>
<span class="ltx_ERROR undefined">\newacronym</span>
<div id="thechapter-at-IDa.p8" class="ltx_para">
<p class="ltx_p">emEMerror model</p>
</div>
<span class="ltx_ERROR undefined">\newacronym</span>
<div id="thechapter-at-IDa.p9" class="ltx_para">
<p class="ltx_p">fsmFSMfinite-state machine</p>
</div>
<span class="ltx_ERROR undefined">\newacronym</span>
<div id="thechapter-at-IDa.p10" class="ltx_para">
<p class="ltx_p">[longplural=finite-state automata]fsaFSAfinite-state automaton</p>
</div>
<span class="ltx_ERROR undefined">\newacronym</span>
<div id="thechapter-at-IDa.p11" class="ltx_para">
<p class="ltx_p">fstFSTfinite-state transducer</p>
</div>
<span class="ltx_ERROR undefined">\newacronym</span>
<div id="thechapter-at-IDa.p12" class="ltx_para">
<p class="ltx_p">wfsaWFSAweighted finite-state automaton</p>
</div>
<span class="ltx_ERROR undefined">\newacronym</span>
<div id="thechapter-at-IDa.p13" class="ltx_para">
<p class="ltx_p">wfsmWFSMweighted finite-state machine</p>
</div>
<span class="ltx_ERROR undefined">\newacronym</span>
<div id="thechapter-at-IDa.p14" class="ltx_para">
<p class="ltx_p">wfstWFSTweighted finite-state transducer</p>
</div>
<span class="ltx_ERROR undefined">\newacronym</span>
<div id="thechapter-at-IDa.p15" class="ltx_para">
<p class="ltx_p">hfstHFSTHelsinki finite-state technology</p>
</div>
<span class="ltx_ERROR undefined">\newacronym</span>
<div id="thechapter-at-IDa.p16" class="ltx_para">
<p class="ltx_p">flossFLOSSfree and libre open-source software</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p17" class="ltx_para">
<p class="ltx_p">language technology
name=language technology,
description=is a scientific discipline for study of language and
linguistic phenomena using formal models and computational methods</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p18" class="ltx_para">
<p class="ltx_p">computational linguistics
name=computational linguistics,
description=is a branch of language technology leaning towards
the linguistic aspects of the computational handling of language</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p19" class="ltx_para">
<p class="ltx_p">natural language engineering
name=natural language engineering,
description=is a branch of language technology leaning towards
the software engineering aspects of the computational handling of
language</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p20" class="ltx_para">
<p class="ltx_p">finite-state automaton
name=finite-state automaton,
description=is a graph-based data structure used in computational
linguistics to encode a number of linguistic structures efficiently,
plural=finite-state automata</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p21" class="ltx_para">
<p class="ltx_p">finite-state machine
name=finite-state machine,
description=is synonymous to finite-state automaton, used to be explicit
that all forms of finite-state automata are included, regardless of
tapes or weights</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p22" class="ltx_para">
<p class="ltx_p">finite-state acceptor
name=finite-state acceptor,
description=is a finite-state machine, where symbols are single
characters of a language, used in the context of this dissertation to
encode dictionaries
</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p23" class="ltx_para">
<p class="ltx_p">finite-state transducer
name=finite-state transducer,
description=is a finite-state machine, where symbols are pairs of
characters of a language, used in context of this dissertation to
encode error-correction models and morphophonological variations</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p24" class="ltx_para">
<p class="ltx_p">finite-state morphology
name=finite-state morphology,
description=is used in this dissertation two ways; firstly to refer
any finite-state approaches to morphology and its use to building
finite-state language models, but secondly to refer very specific
methods and software for writing finite-state language models as made
known by Xerox and described by <cite class="ltx_cite ltx_citemacro_citet">Beesley and Karttunen (<a href="#bib.bib8" title="Finite state morphology" class="ltx_ref">2003</a>)</cite></p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p25" class="ltx_para">
<p class="ltx_p">language model
name=language model,
description=is any software implementation or formal language capable of
describing whether a given word form <em class="ltx_emph">s</em> is correct in terms
of spell-checking or not. This usage differs slightly from
established usage in natural language engineering, where the language
model refers to a statistical <math id="thechapter-at-IDa.p25.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram model, which is just one
sub-class of my usage</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p26" class="ltx_para">
<p class="ltx_p">error model
name=error model,
description=is any software implementation or formal language capable of
enumerating a sorted list of correctly spelled word forms
given a potentially misspelled word form</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p27" class="ltx_para">
<p class="ltx_p">morphology
name=morphology,
description=is a branch of linguistics studying word formation, the
structure of word forms</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p28" class="ltx_para">
<p class="ltx_p">morphological complexity
name=morphological complexity,
description=is used in this dissertation in a very limited sense of
complexity of a language that is relevant to spell-checking and
correction and manifested in terms of the number and predictability of
word forms the
language contains</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p29" class="ltx_para">
<p class="ltx_p">word
name=word,
description=refers to a dictionary word, an abstraction covering a
set of word forms. E.g. <em class="ltx_emph">cat</em> and <em class="ltx_emph">cats</em> are forms of
word <em class="ltx_emph">cat</em>; a lexical unit, or a lexeme</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p30" class="ltx_para">
<p class="ltx_p">word form
name=word form,
description=is a string in text separated by white space or punctuation
that is not part of a word in a given language. This definition stems
from how spell-checking libraries actually work</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p31" class="ltx_para">
<p class="ltx_p">tropical semi-ring
name=tropical semi-ring,
description=is a mathematical, algebraic structure used in weighted
finite-state automata to encode probabilities or penalty weights</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p32" class="ltx_para">
<p class="ltx_p">machine
name=<math id="thechapter-at-IDa.p32.m1" class="ltx_Math" alttext="\mathcal{M}" display="inline"><mi class="ltx_font_mathcaligraphic">ℳ</mi></math>,
description=is an arbitrary finite-state machine. I use subscript indices
to describe the type of automaton. E.g.,
<math id="thechapter-at-IDa.p32.m2" class="ltx_Math" alttext="\mathcal{M}_{\mathrm{L}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi mathvariant="normal">L</mi></msub></math> for language model, <math id="thechapter-at-IDa.p32.m3" class="ltx_Math" alttext="\mathcal{M}_{\mathrm{E}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi mathvariant="normal">E</mi></msub></math>
for error model, <math id="thechapter-at-IDa.p32.m4" class="ltx_Math" alttext="\mathcal{M}_{\mathrm{things}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>things</mi></msub></math> for an automaton
encoding the disjunction of all things,
sort=zzzm</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p33" class="ltx_para">
<p class="ltx_p">collection
name=<math id="thechapter-at-IDa.p33.m1" class="ltx_Math" alttext="\mathcal{D}" display="inline"><mi class="ltx_font_mathcaligraphic">𝒟</mi></math>,
description=is a collection of data, such as a set of word forms, morphs,
typing errors. I use subscript indices to describe the collection.
E.g., <math id="thechapter-at-IDa.p33.m2" class="ltx_Math" alttext="\mathcal{D}_{\mathrm{wordforms}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>wordforms</mi></msub></math> for all
word forms in a text corpus, <math id="thechapter-at-IDa.p33.m3" class="ltx_Math" alttext="\mathcal{D}_{\mathrm{errors}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>errors</mi></msub></math> for all typing
errors in an error corpus, <math id="thechapter-at-IDa.p33.m4" class="ltx_Math" alttext="\mathcal{D}_{\mathrm{morphs}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>morphs</mi></msub></math> for all morphs
in a natural language,
sort=zzzd</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p34" class="ltx_para">
<p class="ltx_p">spell-checker
name=spell-checker,
description=is software capable of detecting and correcting spelling
errors in word forms</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p35" class="ltx_para">
<p class="ltx_p">grammar-checker
name=grammar-checker,
description=is software capable of detecting grammar errors in word forms
and their combinations, and their relation to whole sentences</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p36" class="ltx_para">
<p class="ltx_p">spell-checking
name=spell-checking,
description=is the task of verifying that the word forms of the text are
correctly
written word forms in the language of the spell-checker. Spell-checker
can, however, refer to software capable of both spell-checking and
correction</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p37" class="ltx_para">
<p class="ltx_p">spelling correction
name=spelling correction,
description=is the task of correcting misspelled word forms in the text
by correct ones, or suggesting alternatives in an interactive
application</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p38" class="ltx_para">
<p class="ltx_p">formal language
name=formal language,
description=is a language in terms of mathematics, that is, an arbitrary
set of sequences of symbols</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p39" class="ltx_para">
<p class="ltx_p">natural language
name=natural language,
description=is a language spoken, written or otherwise used by people as
means of communication</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p40" class="ltx_para">
<p class="ltx_p">morph
name=morph,
description=is a minimal indivisible segment split from a word form
carrying meaning or purpose for a computational linguistic
description; a morph can be segmented out of a word form in a text</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p41" class="ltx_para">
<p class="ltx_p">morpheme
name=morpheme,
description=is an abstract concept grouping related morphs together by
means of semantic, phonological or otherwise practical similarity in
a manner that is usable for a computational linguistic description;
a morpheme is not a concrete segment of a running text</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p42" class="ltx_para">
<p class="ltx_p">morphotactics
name=morphotactics,
description=is a set of rules governing what combinations of morphs form
correctly spelled word forms; also: morphotax
</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p43" class="ltx_para">
<p class="ltx_p">real-word error
name=real-word error,
description=a spelling error where the mistyped string is another valid
word form in a dictionary of the language</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p44" class="ltx_para">
<p class="ltx_p">non-word error
name=non-word error,
description=a spelling error where the mistyped string is not a valid
word form in a dictionary of the language</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p45" class="ltx_para">
<p class="ltx_p">scraping
name=scraping,
description=is a crude technical method of obtaining data from text corpora
for computational linguistic systems, usually harvesting all potential
linguistic data without human intervention</p>
</div>
<span class="ltx_ERROR undefined">\newglossaryentry</span>
<div id="thechapter-at-IDa.p46" class="ltx_para">
<p class="ltx_p">formalism
name=formalism,
description=is used by many computational linguists including myself to
refer to scripting languages and data formats used by a natural
language processing system. For example, lexc, twolc and xfst together
make a Xerox formalism for Finite-state morphologies</p>
</div>
<span class="ltx_ERROR undefined">\printglossaries</span>
</section>
<section id="thechapter-at-IDb" class="ltx_chapter">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter <span class="ltx_ERROR undefined">\thechapter</span> </span>Introduction</h2>
<span class="ltx_ERROR undefined">\Gls</span>
<div id="thechapter-at-IDb.p1" class="ltx_para">
<p class="ltx_p">spell-checking and <span class="ltx_ERROR undefined">\glslink</span>spelling correctioncorrection are among
the more practical, well-understood subjects in <span class="ltx_ERROR undefined">\gls</span>computational
linguistics, and earlier, computer science. The task of detecting spelling
mistakes in different texts – written, scanned, or otherwise – is a very
simple concept to grasp. Moreover, the computational handling of the problem
has been the subject of research for almost as long as computers have been
capable of processing texts, with the first influential academic works
published in the 1960’s, e.g. <cite class="ltx_cite ltx_citemacro_cite">Damerau (<a href="#bib.bib26" title="A technique for computer detection and correction of spelling errors" class="ltx_ref">1964</a>)</cite>, and the field has
developed greatly since.</p>
</div>
<div id="thechapter-at-IDb.p2" class="ltx_para">
<p class="ltx_p">The purpose of this thesis is to research one specific approach to
spell/checking – that of finite/state technology. Finite-state technology
has its roots in the mathematical theory of <span class="ltx_ERROR undefined">\glspl</span>formal language, which I
will refer to as <em class="ltx_emph">string sets</em> throughout this thesis to avoid confusion
with the more common meaning of the term, <span class="ltx_ERROR undefined">\glspl</span>natural language. The
theoretical beauty of this approach will be recapped more closely in the later
subsections. The practical circumstances of this approach are the following:
it gained popularity among computational linguists interested in
<span class="ltx_ERROR undefined">\glslink</span>morphological complexitymorphologically complex languages around
the 1980’s and onwards, and it is thought by some of us to be the only way to
handle more morphologically complex languages. This sets the direction of this
thesis to <em class="ltx_emph">implement spell/checking for languages of varying
morphological complexity</em> efficiently.</p>
</div>
<div id="thechapter-at-IDb.p3" class="ltx_para">
<p class="ltx_p">Finite-state approaches have traditionally been popular for the computational
handling of Finnish, cf. <cite class="ltx_cite ltx_citemacro_citet">Koskenniemi (<a href="#bib.bib61" title="Two-level morphology: a general computational model for word-form recognition and production" class="ltx_ref">1983</a>)</cite>. Finnish is my native
language and for practical reasons a recurring theme in this thesis. Whilst
concentrating on scientific contributions made over the years, this thesis is
in a way also a book which describes the past years of the development of,
language independent, <span class="ltx_ERROR undefined">\gls</span>spell-checker software<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>The software is
available as free and open source, like all good scientific research
products.</span></span></span> which is also used in the writing of this very thesis. So, it should
be clear that the contribution of this thesis is meant to be a very concrete,
should I even say a final – albeit beta-quality – system for spell/checking
and correction. There are some practical implications when using one’s native
language to evaluate and develop new and improved methods, and the motivation
for spending immeasurable amounts of spare time is there.</p>
</div>
<div id="thechapter-at-IDb.p4" class="ltx_para">
<p class="ltx_p">One important practical feature of this thesis is that the end product the
scientific methods are built for is usable and available as an end-user
spell-checker at some point in the future. For this reason I aim to ground the
theoretical advances so that they are usable for a larger audience of end users
and this includes limiting resources required to build the systems experimented
with to freely-available and open-source materials and techniques. Indeed, if
a spell/checker sub-system is mentioned in this thesis, you should at least
be able to download a functional prototype. All our software and data is
available in our version management
system.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><a href="http://svn.code.sf.net/p/hfst" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://svn.code.sf.net/p/hfst</a></span></span></span> <span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup>A mono-spaced
font is used for URLs and program code segments.</span></span></span> I also strongly believe that
this approach entailing not only <span class="ltx_ERROR undefined">\gls</span>floss for the scientific software and
results, but also properly conducted open scientific research will fulfil the
basic requirement of reproducibility.</p>
</div>
<div id="thechapter-at-IDb.p5" class="ltx_para">
<p class="ltx_p">Another perspective in the thesis is the quality of the spell/checking for
<span class="ltx_ERROR undefined">\glslink</span>morphological complexitymorphologically complex languages. For
English, the vast majority of research on the topic has already shown
impressive figures for the quality of spell/checking and corrections with
statistical approaches. It almost seems like a scientific consensus on the
solution. The common intuition, however, raises some suspicion that the
morphologically more complex languages may not be so favourable to simple
statistical approaches. The contribution of this thesis is also in showing the
limits of these approaches for morphologically complex, and resource-poor
languages, exploring possible modifications that can be used to salvage the
situation.</p>
</div>
<div id="thechapter-at-IDb.p6" class="ltx_para">
<p class="ltx_p">The remainder of this first chapter is dedicated to an informal introduction to
the thesis topic. It consists of closer definitions of the practical and
scientific concepts central to building a spell/checking system as a research
problem, some very down-to-earth rationals for this approach, and the
background details that motivate the research of finite/state methods for
spell/checking even when spell/checking itself has already been almost
researched to death. Finally, as this is an article-based thesis, I give an
overview of the articles of the thesis, and describe how the thesis is
structured.</p>
</div>
<section id="S1" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Components of Spell-Checking and Correction</h3>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Spell/checking and correction, as the title suggests, can already be
separated into two components. Furthermore both of the components can be
divided, classified and analysed into many separate pieces. In this section I
try to cover different existing and practical conceptualisations, and introduce
the relevant terminology in a neat and systematic manner.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">The primary division of labour that is present throughout this thesis, and is shown
in the title, is the division into spell/checking and correction.
<span class="ltx_ERROR undefined">\Gls</span>spell-checking is the task of <em class="ltx_emph">detecting</em> errors in texts,
whereas <span class="ltx_ERROR undefined">\gls</span>spelling correction is the task of <em class="ltx_emph">suggesting</em> the most
likely correct <span class="ltx_ERROR undefined">\glspl</span>word form to replace the error. It is of great
importance to treat these two tasks as separate, as the implementations made
for each of them can be quite freely mixed and matched. While there are
systems that use the same methods and approaches for both, this is not by any
means a necessity. A practical system giving suggestions usually does not have
access to the context of the word form it is correcting and to the criteria by
which the word form was chosen. These are relevant constraints in some of the
approaches I present in the thesis and for some of the concepts I present in
this section.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">The methods for detecting errors can be divided according to the data they use
for evidence of the word being <em class="ltx_emph">misspelt</em>. The basic division is whether the
system looks only at the <span class="ltx_ERROR undefined">\gls</span>word form itself, in an <em class="ltx_emph">isolated</em> spelling
error detection, or if it actually looks at the <em class="ltx_emph">context</em> as well.
Traditionally, the isolated error detection has been based on lookup from
word form lists to check if the word form is valid. The only additional notion
I have in terms of this thesis, talking about finite/state spell/checking,
is that our <em class="ltx_emph">dictionaries</em> are systems which are capable of representing an infinite
number of word forms, i.e., they contain not only the dictionary words, but
also the derivations and compounds necessary to reasonably spell-check a
morphologically complex language. Sometimes, this spell/checking is backed up
with statistical information about words: the words that are in a dictionary,
but are very rare, might be marked as errors if they are one typing error away
from a very common word, such as <span class="ltx_ERROR undefined">\markoverwith</span><span class="ltx_ERROR undefined">\textcolor</span>red<span class="ltx_text" style="position:relative; bottom:-3.5pt;"><span class="ltx_ERROR undefined">\sixly</span>:</span><span class="ltx_ERROR undefined">\ULon</span>lave,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup>I will use this
special emphasis for misspellings throughout the thesis as most of the
users of contemporary spell-checkers should be familiar with
it.</span></span></span> <span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><em class="ltx_emph">lave</em> in case you were as unfamiliar with this rare word
as I am, it is defined in Wiktionary as: “1. (obsolete) To pour or throw
out, as water; lade out; bail; bail out. 2. To draw, as water; drink in. 3.
To give bountifully; lavish. 4. To run down or gutter, as a candle. 5.
(dialectal) To hang or flap down. 6. (archaic) To wash.”
<a href="http://en.wiktionary.org/w/index.php?title=lave&amp;oldid=21286981" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://en.wiktionary.org/w/index.php?title=lave&amp;oldid=21286981</a></span></span></span> instead
of <em class="ltx_emph">leave</em> <cite class="ltx_cite ltx_citemacro_citep">(Kukich, <a href="#bib.bib63" title="Techniques for automatically correcting words in text" class="ltx_ref">1992</a>)</cite>. The spelling errors of this
kind, where the resulting word is an existing word in the dictionary, are
called <span class="ltx_ERROR undefined">\glspl</span>real-word error. The shortcoming of isolated approaches
to spell/checking is that they will basically only recognise those spelling
errors which result in a word form that is not in the dictionary,
so-called <span class="ltx_ERROR undefined">\glspl</span>non-word error. Real-word spelling errors
will almost always require the use of a context to obtain some evidence that
something is wrong. Here again, the simplest approach is to use statistics; if
the word we are inspecting does not usually appear in the current context of
two to three words, it may be an error. A more elaborate context-based
method for detecting errors is to use a full-fledged natural language
processing system that can parse morphology, syntax or other features of the
running text. Such a system can usually recognise sentences or phrases that
are unusual, or even grammatically wrong; these systems are usually
called <em class="ltx_emph"><span class="ltx_ERROR undefined">\glspl</span>grammar-checker</em>,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup>A word of warning about this
terminology: some practical systems, such as office software, will call any
spelling checker that uses any context-based approach, as opposed to an
isolated approach, a grammar checker. This terminological confusion is not
carried over to Microsoft Word’s recent incarnations, but at the time of
writing Apache OpenOffice.Org and LibreOffice still follow the confused
terminology.</span></span></span> rather than spelling checkers, and are not a
central topic of this thesis, although I do try to clarify what kind of
extensions could be made to turn my spell-checkers into such grammar-checkers.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">The error-correction task is divided most naturally by the types of errors that
are made. Many different classifications with different names exist, but most
will agree that there is a category of errors that is based on unintentional
mistakes, such as mistyping at the keyboard. This is the type of spelling error
that is the main target for correction suggestion systems, and the common
research results have claimed that between 80 and 95% of all spelling errors
can be attributed to having <em class="ltx_emph">a single</em> typing error in a
word form <cite class="ltx_cite ltx_citemacro_citep">(Kukich, <a href="#bib.bib63" title="Techniques for automatically correcting words in text" class="ltx_ref">1992</a>)</cite>. The rest of the spelling errors are
more or less based on <em class="ltx_emph">competence</em>. Such errors can be caused by not
knowing how to spell a word form, which is especially common in orthographies like
English, or not knowing the correct way to inflect a word, which is common when
writing in a non-native language. Increasingly common are errors caused by
limited input methods, such as dropping accents or otherwise transcribing text
to match e.g. a virtual keyboard of a mobile phone – generally these errors
are indistinguishable from typos.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Morphological Complexity</h3>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph"><span class="ltx_ERROR undefined">\Gls</span>morphology</em> is the subfield of linguistics that deals with the
structure of a word form. A <em class="ltx_emph"><span class="ltx_ERROR undefined">\gls</span>word form</em>, for the purposes of this
dissertation, is defined as a segment of a text that is separated by whitespace
symbols or other similar orthographic conventions. While from a linguistic
point of view this definition is not without problems, it is one that today’s
spell-checking software has to cope with. In fact, many spell-checkers today
are restricted to handling word forms that some other software has decided are
word forms like this. To give one example, in English ‘cat’ is a word form, as
is ‘dogs’, or ‘involuntarily’. The structure of a word form is composed of one
or more morphs. A <em class="ltx_emph"><span class="ltx_ERROR undefined">\gls</span>morph</em> is a segment of a word that has been
defined by linguistic theories as a minimal unit carrying meaning, or more
vaguely, purpose. In ‘cat’ there is one morph, the root word meaning cat, in
‘dogs’ there is the root morph ‘dog’ and another morph ‘s’ marking plurality in
English. In the word ‘foxes’, there is a root morph ‘fox’ and a plural morph
‘es’. Sometimes when describing morphology, abstractions called
<span class="ltx_ERROR undefined">\glspl</span>morpheme are used to group similar morphs together, for example in
English it could be said based on these examples that the plural morpheme is
‘(e)s’.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph">Complexity</em> in morphological features, is studied in the field of
linguistic <em class="ltx_emph">typology</em>. There are no commonly agreed measures of
morphological complexity, and this dissertation does not intend to be a
contribution to typological studies. Rather, I try, as a practical issue, to
define morphological complexity as it relates to spell-checking and correction.
In this section I will thus attempt to explain my understanding of the issue,
partially because I have used terms such as <em class="ltx_emph">morphologically complex</em> in
the thesis articles. It is therefore necessary to remember to read the word
pair <em class="ltx_emph">morphologically complex</em> in this thesis as a concept which refers to
aspects of languages’ writing systems, dictionary formation and such features
described in this section, rather than as contested definitions in linguistics
at large.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">One theme of my thesis was to bring effective and high-quality spell/checking
and correction to languages that are of varying <span class="ltx_ERROR undefined">\gls</span>morphological complexity.
To substantiate this claim I will first try to define what I mean by
morphological complexity. There is a measure of complexity that affects the
building and use of correct word form recognisers in terms of computational and
practical resources, and there are only a few easily measurable factors in this
playing field. In linguistics studies began
with <cite class="ltx_cite ltx_citemacro_citet">Greenberg (<a href="#bib.bib33" title="A quantitative approach to the morphological typology of language" class="ltx_ref">1960</a>)</cite> and continued up to the contemporary
endeavour known as the world atlas of language structures <cite class="ltx_cite ltx_citemacro_citep">(Haspelmath<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib122" title="The World Atlas of Language Structures" class="ltx_ref">2005</a>)</cite>, which
has been widely used for measurements of similarity, complexity, and so on.
The morphological complexity of a computational spell-checker is based on,
e.g., the complexity of the word forms in terms of <span class="ltx_ERROR undefined">\glspl</span>morph. This is
measurable as the average count of morphs in the word or the
<em class="ltx_emph">morph-to-word</em> ratio. The morphs I use here refer to the smallest
meaningful unit in the word that is used so regularly that it can be
practically used in the language description – that is, a productive morph.
Another factor of morphological complexity that is important is the variation
that creates a set of related morphs. This variation can be realised in terms
of the number of <em class="ltx_emph">morphemes</em> per language or <em class="ltx_emph">morphs-per-morpheme</em>.
Since a spell-checker’s main task is to predict all correct word forms, both
the number of potential word forms created by morph chains and the amount of
variation within these morphs are good indicators to estimate the computational
complexity of that particular spell-checker.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p">These measures lend themselves nicely to a view of spell/checking
software where the languages that have significantly higher values
include those that have their special implementations in the
world of open-source spell/checkers: Finnish and
Voikko,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><a href="http://voikko.puimula.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://voikko.puimula.org</a></span></span></span> Hungarian and
Hunspell,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><a href="http://hunspell.sf.net" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://hunspell.sf.net</a></span></span></span> Turkish and
Zemberek,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><a href="http://code.google.com/p/zemberek/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://code.google.com/p/zemberek/</a></span></span></span> and so forth.
This tendency suggests that people trying to build spell-checkers with the
limitations of almost only word-list or statistics-based systems will not be
able to reach satisfying results, and other approaches are in fact necessary.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p class="ltx_p">Perhaps the most important form of morphological complexity relevant to the
topic of this thesis, and one that is often ignored, is the rate of productive
derivation and compounding that produces new <em class="ltx_emph">ad hoc</em> words and word forms
that are difficult to predict using finite word lists or even simple finite
combinations of words and morphs pasted together. For a language where these
processes are common, spell/checking greatly benefits from a dictionary
capable of predicting a large number of words which have not been seen before.
Surprisingly many of both existing systems, and those described in recent
research, e.g. in <cite class="ltx_cite ltx_citemacro_cite">Hassan<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib40" title="Language independent text correction using finite state automata" class="ltx_ref">2008</a>); Watson (<a href="#bib.bib117" title="A new algorithm for the construction of minimal acyclic DFAs" class="ltx_ref">2003</a>)</cite>, simply do not
acknowledge the need. Some gloss over the fact as a possible extension to the
system, some ignore it altogether, but usually without any example of an
implementation and an evaluation strictly based on English or a few other
popular Indo-European languages with similar features. For many languages an
easy way to produce dictionaries with reasonable coverage is a system which
predicts infinite amounts of compound and derivation combinations.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p class="ltx_p">To illustrate the practical issues with morphologically complex languages, we
use the <em class="ltx_emph">de facto</em> open-source spell/checking system, Hunspell. Of the
aforementioned morphological features, Hunspell supports up to 2 affixes per
word, or 3 morphemes. For languages going beyond that, including Hungarian, the
creator of a dictionary will at least need to combine multiple morphemes into
one to create a working system. The morpheme count in Hunspell is realised as
affix sets, of which Hunspell supports up to 65,000 (the exact number supported
may vary somewhat in different versions, but it seems to be less than 65,535,
which would have been expected for the two-byte coding of suffix flags that was
used in the system). Hunspell does, however, include support for some
compounding.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p class="ltx_p">As a concrete example of limitations with e.g. Hunspell, there has been at
least one failed attempt to build a spelling-checker for Finnish using Hunspell
<cite class="ltx_cite ltx_citemacro_citep">(Pitkänen, <a href="#bib.bib95" title="Hunspell-in kesäkoodi 2006: final report" class="ltx_ref">2006</a>)</cite>, whereas we know from the early days of the
finite/state methods in computational linguistics that Finnish is
implementable, in fact it has often been suggested that the initial
presentation of the automatic morphological analysis of Finnish
by <cite class="ltx_cite ltx_citemacro_citet">Koskenniemi (<a href="#bib.bib61" title="Two-level morphology: a general computational model for word-form recognition and production" class="ltx_ref">1983</a>)</cite> is one of the main factors in the popularity
of finite/state methods among many related languages. Many of the languages
with similar morphological features, however, have been also implemented in the
Hunspell <span class="ltx_ERROR undefined">\gls</span>formalism, indeed as even the name suggests, the formalism
originated as a set of extensions to older *spell<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup>Specifically,
myspell, c.f. <a href="http://www.openoffice.org/lingucomponent/dictionary.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.openoffice.org/lingucomponent/dictionary.html</a></span></span></span>
formalisms to support Hungarian. For North Saami, a language from Uralic
family I commonly use for test cases to compare with Finnish, an
implementation of a spell-checker for Hunspell
exists.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><a href="http://divvun.no" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://divvun.no</a></span></span></span> The languages that fall into the more
complex side of the morphological complexity are usually noted as very hard to
properly implement with the current Hunspell formalism and its limitations,
though often the results are found tolerable. E.g.
Gīkūyū <cite class="ltx_cite ltx_citemacro_citet">Chege<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib20" title="Developing an open source spell-checker for GÄ«kÅ«yÅ«" class="ltx_ref">2010</a>)</cite> used 19,000 words with extensive affix rules
as a spell-checker, for which they say they attained a recall of 84%. The
same does not seem to apply for Greenlandic, where it was found that a
collection of 350,000 word forms only covered 25% of the word forms in
newspaper
texts.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><a href="http://oqaaserpassualeriffik.org/a-bit-of-history/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://oqaaserpassualeriffik.org/a-bit-of-history/</a>
</span></span></span></p>
</div>
<div id="S2.p8" class="ltx_para">
<p class="ltx_p">The sizes of wordlists lead to the final, perhaps most practical metric of
measurement in the morphological complexity of languages in the context of
computational applications. That is, the size of the dictionary when it is in
the memory, whatever may be the method of encoding it or compressing it; this
is easily measurable and a very important factor for any practical application.
For example for a word-list approach, even uncompressed the wordlist of a
million words with an average word length of <math id="S2.p8.m1" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math> is just <math id="S2.p8.m2" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math> megabytes, whereas
storing the list as a finite/state automaton, suffix trie or hash is
considerably less. Even though most wordlist-based software do store the data
in one of the more advanced data structures, it is the limitations of Hunspell
such as the two-affix limit, that will force major parts of wordform lists to
be included in an inefficient format on disk, and will greatly hinder the speed
of Hunspell operation in practice. With combinations of derivation and
compounding the memory requirements tend to go up rapidly, even with
finite-state approaches, so there is a practical correlation measure between
the complexity of the language defined in theoretical terms earlier; in a worst
case scenario this is obviously a limiting factor, e.g. some versions of my
Greenlandic experiment with finite-state automata took up almost 3 gigabytes of
memory, which is clearly unacceptable for a spelling checker in an average 2013
end-user computer system.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>On the Infinity of a Dictionary</h4>

<div id="S2.SS1.p1" class="ltx_para">
<p class="ltx_p">While I have tried my best to avoid hotly debated aspects of <span class="ltx_ERROR undefined">\gls</span>morphological
complexity, there is one central differentiating factor between the
finite-state approaches and wordlist approaches to language that I cannot
avoid, and that is the expression power capable of predicting an infinite
number of word forms. The fact that cyclic automata encode infinite lexicons is
mentioned numerous times in the articles of my thesis and a few of them
specifically deal with the problems arising. I provide here some rationale why
I consider infinity a necessary feature.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p class="ltx_p">Now, the concept of infinity in itself, as an abstraction is quite hard to
grasp in many cases, especially if you have not studied mathematics of
transfinite numbers. The basic example used in school mathematics is a set of
numbers, for example, there is an infinite number of non-negative integers
<math id="S2.SS1.p2.m1" class="ltx_Math" alttext="\mathbb{N}_{+}=0,1,2,3,\ldots" display="inline"><mrow><msub><mi>ℕ</mi><mo>+</mo></msub><mo>=</mo><mrow><mn>0</mn><mo>,</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mn>3</mn><mo>,</mo><mi mathvariant="normal">…</mi></mrow></mrow></math>, and this is uncontested. Intuitively, it
is also somewhat easy to grasp, for example, for any given number you can read
out loud, there is always going to be larger numbers, which you can create by
adding <math id="S2.SS1.p2.m2" class="ltx_Math" alttext="1" display="inline"><mn>1</mn></math> to it, or any other positive integer for that matter (but not <math id="S2.SS1.p2.m3" class="ltx_Math" alttext="0" display="inline"><mn>0</mn></math>,
for adding zero will not make a larger number). Now, in Finnish, numbers are
written as compounds, that is, without spaces in between the number words. For
example, 3 is ‘kolme’, 33 is ‘kolmekymmentäkolme’, and 33333 is
‘kolmekymmentäkolmetuhattakolmesataakolmekymmentäkolme’. So, we have a mapping
from non-negative integers to words that belong to a dictionary of the Finnish
language, and therefore the size of a Finnish language dictionary is at least
equal to the size of the non-negative number set.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p class="ltx_p">In practice, this is still debated, even when using the definition of words and
word forms that only includes space-separated strings, whether this infinity is
really needed. The main argument here is that, even if there was a theoretical
infinity of these words, we can encode a certain finite subset of them, and
always get a virtual 100% coverage of all words that actual people use in
real-world texts. And this is a very good argument, and one with which I fully
agree. However, especially considering our number example, what is the subset
of numbers we can select and be sure that no one will pick a number outside
that set and write that as a word? Style guides will usually suggest something
along the lines of writing words that have up to five compound parts. However,
this is also an infinite set, since only the digits that are non-zero are read
as numbers in Finnish, e.g., 3,000,000,000,000,000,000,003 is
‘kolmetriljardiakolme’. After that, we can probably trust
non-malicious<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup>I say non-malicious here because certainly someone, such
as myself, could write a number consisting of more parts, for example
‘kolmesataakolmekymmentäkolmemiljoonaakolmesataakolmekymmentäkolmetuhattakolmesataakolmekymmentäkolme’
in a written and published text, such as this one, just to prove a point.</span></span></span>
writers to stick with reasonably small numbers, say under one centillion, that
is a number that has one, and six hundred zeroes after it.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup>Finnish
uses long scale system where systemic large numerals are made of the latin
prefix, <em class="ltx_emph">mi</em>, <em class="ltx_emph">bi</em>, <em class="ltx_emph">tri</em>, …, <em class="ltx_emph">centi</em>, …, and
either <em class="ltx_emph">-(i)llion</em> or <em class="ltx_emph">-(i)lliard</em> suffix to it.</span></span></span> Even with these,
quite reasonable limitations, the number of common words to list gets quite
large, and we have not even touched the tricky topic that Finnish numeral
compounds have case agreement in inflection <cite class="ltx_cite ltx_citemacro_citep">(Karttunen, <a href="#bib.bib44" title="Numbers and Finnish numerals" class="ltx_ref">2006</a>)</cite>. This
is one of the places where cyclic, infinite finite-state lexicons really show
their power. If you encode these number words as a <span class="ltx_ERROR undefined">\gls</span>finite-state
automaton and allow compounds of up to five components, you need to create
five copies of the network, making it five times the size that all the numeral
words as individual components need. However, if you make that network cyclic,
allowing any number of combinations of number words, even infinite, the size of
the network is almost exactly the same as the network that allows only one
number word without compounding.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup>There is one arc added for the cycle,
but if you consider Finnish specifically, some constraints are also needed to
control the inflection of compounds; these are detailed
in <cite class="ltx_cite ltx_citemacro_citet">Karttunen (<a href="#bib.bib44" title="Numbers and Finnish numerals" class="ltx_ref">2006</a>)</cite>.</span></span></span></p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p class="ltx_p">Numerals are not very interesting as a linguistic example, and perhaps also
somewhat suspicious, as it is easy to come up with new numbers. However,
non-number word compounding is not very different in practice. We have a set of
words that can be compounded together, and we have to at least predict which
combinations the user might use. Speakers of language come up with new <em class="ltx_emph">ad
hoc</em> compounds every day, and making good guesses or limits to this creativity
would be a very difficult task. As a good experimental example, in my
implementation of Finnish morphology,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup>Open Source Morphology of
Finnish, <a href="http://code.google.com/p/omorfi/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://code.google.com/p/omorfi/</a></span></span></span> I have opted to encode all
compounds that are attested as “known” compounds. If today I pick up the
first article on a Finnish news
site,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">18</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">18</sup><a href="http://yle.fi/uutiset/miksi_keva_kokousti_kuusi_tuntia_en_kerro_vaikka_kuinka_tenttaat/6950229" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://yle.fi/uutiset/miksi_keva_kokousti_kuusi_tuntia_en_kerro_vaikka_kuinka_tenttaat/6950229</a></span></span></span>
I still get one compound that has not been encoded as such, even after
<span class="ltx_ERROR undefined">\gls</span>scraping large corpora like Wikipedia for compounds:
“toimitus+johtaja+sopimus” (CEO’s contract). The logic of trying to predict
such forms is not very easy and statistics will not help either. We could again
make some approximations to cover most of the expected cases, e.g. five total
words and perhaps encode some semantics. This is a good approach, however, it
requires a lot of manual work and the resulting dictionary will be massive.
Including all combinations of five arbitrary nouns requires <math id="S2.SS1.p4.m1" class="ltx_Math" alttext="N^{5}" display="inline"><msup><mi>N</mi><mn>5</mn></msup></math>, where <math id="S2.SS1.p4.m2" class="ltx_Math" alttext="N" display="inline"><mi>N</mi></math> is
the size of the noun dictionary (analogously to amount of digit combinations in
five-digit numbers <math id="S2.SS1.p4.m3" class="ltx_Math" alttext="10^{5}" display="inline"><msup><mn>10</mn><mn>5</mn></msup></math>), and even as a finite-state automaton this is 5
times the size of finite-state automaton of nouns, or, indeed, a nominal
finite-state automaton with infinitely long compounds. So all in all, the
decision to support infinitely long compounds is based on the fact that it does
have better predictive power and takes up less space, and is easier to
implement in terms of work required. It is not maximally ideal for spelling
correction, a problem we will deal with later in this thesis.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p class="ltx_p">One argument against the infinity of the lexicon is that it is an artefact of
the writing system. And this I wholeheartedly agree with. The Finnish
example even shows it explicitly. Considering the agreement of inflection with
numerals, we have inflection patterns like: “kolme+sataa+kolme” :
“kolme-lle+sada-lle+kolme-lle” : “kolme-ksi+sada-ksi+kolme-ksi”, (303,
singular nominative, allative and translative forms). This is not different
from noun phrases in Finnish, which are written with spaces: “vihreä uni” :
“vihreä-lle une-lle” : “vihreä-ksi une-ksi” (green dream, same forms as
before). Here we have exactly same agreement features and structure of word
combinations, but a space is added between the words. While we cannot change
the quirks of writing system conventions very easily, it would make sense to
handle both cases in the same way, and by extension, all morphology in the same
way regardless of the writing system, which may or may not use whitespace and
punctuation to delimit morphological units. This is also a feature I argue for
in some of my articles included in this thesis as well. However, the fact
remains that contemporary software does not let us work on units of text like
that, and especially interfaces for spell-checking in popular real-world
software rely on spell-checking to deal with space and punctuation-separated
strings. As it is something that cannot be changed by a doctoral dissertation.
I am left with the task of dealing with the problems that arise from
infinite compounds and from recurring derivation that arise from the writing
systems not using spaces or punctuation to separate morphological units. I
have, for the most part, attempted to formulate the systems in a manner that
will be usable if in the future we have systems and frameworks that let us
handle texts as a succession of linguistic units rather than as chunks
separated by whitespace.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p class="ltx_p">As a side-note, it is not only compounding as the form of copying and
concatenating certain word forms, it is also derivation that can cause loops
that go towards infinity. What it means is that the parts of words that attach,
e.g., to the beginning or the end of the word, can have long, unpredictable
combinations. This is true for example for Greenlandic, where derivation is the
contributing factor to the average word length and thus to <span class="ltx_ERROR undefined">\gls</span>morphological
complexity as I define it in this thesis. Another aspect of derivation is
that it interfaces with compounding in a way that makes it clear that it is not
just pasting all words together a few times in certain forms that is sufficient
to predict all new words. Nor is it pasting together all the morphs in
succession that is sufficient to predict new words. What is really needed is
rather advanced combinations of morphs and compounds that form new words. For
example in Finnish compounds should be made of certain forms of nouns, so one
might naively expect to be able to predict all compounds by concatenating those
nouns together. However, it is also all the verbs derived into nouns that can
appear as parts of compounds, e.g., there is a word <em class="ltx_emph">meren-käynti</em>
(seafaring, from <em class="ltx_emph">meri + käydä + -nti</em> sea + go + suffix for making nouns
from verbs). For a more recent example of how productive and creative this is,
I analysed some of my text logs from an internet relay chat service called IRC
to find multipart compounds with verbs in them, and I found this particular
example rather illuminating: <em class="ltx_emph">känny-irkkailu</em> (habitual mobile IRCing,
from <em class="ltx_emph">känny + irkki + -ata + -illa + -u</em>, literally mobile phone + IRC +
suffix to verb nouns with + suffix marking frequent or habitual doing + suffix
for making nouns from verbs). This example exhibited a noun neologism derived
into a verb, derived into another verb derived into a noun. This kind of
creativity is not easily predicted by just concatenating word forms or morphs.</p>
</div>
<div id="S2.SS1.p7" class="ltx_para">
<p class="ltx_p">To summarise, I have opted to enable the possibility of infinite compounds and
derivations in word forms of dictionaries used in spell-checking in this
dissertation for the following reasons. Firstly, contemporary spelling checkers
are mainly forced to work with the orthographic concept of space-separated
tokens as word forms. Secondly, in finite-state structures it is typically more
efficient to work with unbounded combinations. Thirdly, the art of picking and
choosing the finite good combinations of compound parts and derivations is
a lifetime’s work in lexicography (cf. the Greenlandic lexicon’s history
mentioned earlier in Footnote <a href="#S2" title="2 Morphological Complexity ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) to reach a sufficient
coverage for a spell-checker. For these reasons, and because it seemed
sensible for my own work with my Finnish dictionary, I chose to investigate the
intriguing problematics of infinite lexicons in finite-state spell-checkers as
one major part of my dissertation. I believe I have managed to formulate the
research in a manner that will be sustainable even if the world of
spell-checking and language technology change to support different definitions
of a word and its parts.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Finite-State Technology in Natural Language Processing</h3>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">After considering the limitations and problems of wordlists and other simple
approaches, we come to the second part of the thesis topic: finite/state
methods. In this dissertation I have selected to use <span class="ltx_ERROR undefined">\glspl</span>finite-state
automaton as a general framework for handling spell-checking dictionaries
because of its success in handling morphologically varied
languages <cite class="ltx_cite ltx_citemacro_citep">(Beesley and Karttunen, <a href="#bib.bib8" title="Finite state morphology" class="ltx_ref">2003</a>)</cite>. This approach is sufficient to cover
the infinite dictionaries of the kind mentioned in the previous section. It is
commonly thought that the extents of finite-state technologies are sufficient
to cover most of the natural languages’ word forms found in the running text,
that is, a spell-checker’s dictionary implemented with these technologies could
know all correctly written word forms without technology being the limiting
factor. There are some counter-arguments to this, e.g.,
<cite class="ltx_cite ltx_citemacro_citet">Culy (<a href="#bib.bib25" title="The complexity of the vocabulary of Bambara" class="ltx_ref">1987</a>)</cite> presents features of Bambara vocabulary that would
require expressive powers beyond regular and even context-free grammars to
predict Bambara’s word forms. It seems to me that there are two limitations to
this specification that allow finite-state approximations of Bambaran
vocabulary: firstly it appears that Culy argues that despite the fact that
these lexical elements are written with spaces in between them, they should be
handled on the vocabulary side. This already relieves finite-state
spell-checking system from the burden of recognising such lexical elements due
to the fact that finite-state spell-checkers are forced to operate on
space-separated strings. This definition does allow for Bambara to have
ungrammatical forms in a spell-checked text, such as <span class="ltx_ERROR undefined">\markoverwith</span><span class="ltx_ERROR undefined">\textcolor</span>red<span class="ltx_text" style="position:relative; bottom:-3.5pt;"><span class="ltx_ERROR undefined">\sixly</span>:</span><span class="ltx_ERROR undefined">\ULon</span><em class="ltx_emph">wulunyina
o wulufilela</em> (dog searcher dog watcher) mentioned in the text. This, however
is akin to a spell-checker allowing the English phrase <span class="ltx_ERROR undefined">\markoverwith</span><span class="ltx_ERROR undefined">\textcolor</span>red<span class="ltx_text" style="position:relative; bottom:-3.5pt;"><span class="ltx_ERROR undefined">\sixly</span>:</span><span class="ltx_ERROR undefined">\ULon</span><em class="ltx_emph">I sees
he</em> or Finnish <span class="ltx_ERROR undefined">\markoverwith</span><span class="ltx_ERROR undefined">\textcolor</span>red<span class="ltx_text" style="position:relative; bottom:-3.5pt;"><span class="ltx_ERROR undefined">\sixly</span>:</span><span class="ltx_ERROR undefined">\ULon</span><em class="ltx_emph">punaiselle kukasta</em> (to red from flower),
something that in contemporary software is relayed from spell-checking to
grammar checking. The second saving aspect for finite-state technology is its
capability to describe all finite subsets of languages higher in the hierarchy.
What this means is that, should there be a language having a productive
word-formation like Bambara without intervening white spaces, it is still
possible to capture the word forms that have less than infinite components in
them matched in a manner that would require greater expressive powers. For
example, one could create an <span class="ltx_ERROR undefined">\gls</span>fsa that counts that if there are two or
five or thirty-three matching components that fulfil the requirements, it will
always be possible to select a number high enough that covers a large enough
part of the words used in a real-world context that the spell-checker of such
form would be enjoyable to use. There have already been studies on syntax that
show that the expressive power can be limited to a rather small number of such
dependencies that would otherwise be beyond finite-state
technology <cite class="ltx_cite ltx_citemacro_citep">(Karlsson, <a href="#bib.bib54" title="Constraints on multiple center-embedding of clauses" class="ltx_ref">2007</a>)</cite>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">The latest developments in the finite/state approach to natural language
processing has been the concept of weights in finite/state automata. I have
experimented with some approaches to bring the expressive power of
statistical language models to traditional rule-based dictionaries. One
significant part of the contributions in this thesis studies the notions of
applying statistical approaches in conjunction with morphologically complex
languages using finite/state methods.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">The concept of modelling typing errors is not so widely accepted as part of
finite/state technology. While for example in speech applications, trying to
map a representation of the spoken audio signal into written language is quite
common, it is relatively new in the mapping of typing errors into correctly
typed text. The contribution to finite/state methods in this thesis provides
further research on the application of finite/state models of typing errors
as a practical component in a full spelling correction system.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Overview of Thesis Articles</h3>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">This dissertation is a collection of articles I have written during the
development of different parts of our finite/state spell/checking system. It
covers a wide range of topics, all tied together by the goal of getting things
to work nicely, and investigates the quality and speed of spell-checkers
for languages like English. My articles, in chronological order are presented
in <em class="ltx_emph">Original Papers</em> on page <a href="#thechapter-at-IDa" title="Original Papers ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title">Original Papers</span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Chronological Order</h4>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">The development shown in the articles is easy to follow in
<span class="ltx_text ltx_font_bold">chronological order</span>, almost tells its own story. In the beginning we
have a Finnish morphological analyser <cite class="ltx_cite ltx_citemacro_citep">(Pirinen, <a href="#bib.bib93" title="Suomen kielen Ã¤Ã¤rellistilainen automaattinen morfologinen analyysi avoimen lÃ¤hdekoodin menetelmin" class="ltx_ref">2008</a>)</cite> which cannot
yet be turned into a statistical dictionary as it contains too rich a
morphological productivity. In articles
<cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib1" title="Weighted finite-state morphological analysis of Finnish compounds with HFST-LEXC" class="ltx_ref"><span class="ltx_ERROR undefined">\al@pirinen2009weighted,pirinen2009weighting</span></a>; <a href="#bib.bib2a" title="Weighting finite-state morphological analyzers using HFST tools" class="ltx_ref"><span class="ltx_ERROR undefined">\al@pirinen2009weighted,pirinen2009weighting</span></a>)</cite> we explore some advanced
weighting options of Finnish morphology that could provide better treatment of
compounds and derived word forms in morphologically complex languages. In
<cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib4a" title="Finite-state spell-checking with weighted language and error models" class="ltx_ref">III</a>)</cite> we try to tackle the problem of scarcity of
corpus resources especially in conjunction with morphologically complex
languages, using a crowd-sourcing option provided by Wikipedia and measuring
how smaller text corpora will still improve the spelling correction results. In
<cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib3a" title="Building and using existing Hunspell dictionaries and TeX hyphenators as finite-state automata" class="ltx_ref"><span class="ltx_ERROR undefined">\al@pirinen2010building,pirinen2010creating</span></a>; <a href="#bib.bib5a" title="Creating and weighting Hunspell dictionaries as finite-state automata" class="ltx_ref"><span class="ltx_ERROR undefined">\al@pirinen2010building,pirinen2010creating</span></a>)</cite> I have studied the
compilation and reuse of existing non-finite/state language descriptions as
finite/state automata-based language models of a finite-state
spell/checking system. In <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib5a" title="Creating and weighting Hunspell dictionaries as finite-state automata" class="ltx_ref">V</a>)</cite> I attempt to
reimplement much of the Hunspell spell/checking system, including their error
correction methods, in finite/state form. In
<cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib6" title="Modularisation of Finnish finite-state language descriptionâtowards wide collaboration in open source development of morphological analyser" class="ltx_ref">VI</a>)</cite> I research the topic of how to maintain
finite/state <span class="ltx_ERROR undefined">\glspl</span>language model, and try to show that finite-state
language models are feasible for long-term maintenance for a vast array of
applications using just one language description instead of one description per
application. I extend this theme in <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib7" title="Compiling Apertium morphological dictionaries with HFST and using them in HFST applications" class="ltx_ref">VII</a>)</cite> by
showing how to convert language models outside of morphological analysers and
spell-checkers into a suitable finite/state automaton. In
<cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib9a" title="Improving finite-state spell-checker suggestions with part of speech n-grams" class="ltx_ref">VIII</a>)</cite> I tackle the problematic issue of
context-aware spell-checkers for morphologically complex and resource-poor
languages, showing some practical limitations and what can be done to get some
benefits from such an implementation. In
<cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib8a" title="Effect of language and error models on efficiency of finite-state spell-checking and correction" class="ltx_ref"><span class="ltx_ERROR undefined">\al@pirinen2012effects,pirinen2013quality</span></a>; <a href="#bib.bib10" title="Quality and speed trade-offs in weighted finite-state spell-checking" class="ltx_ref"><span class="ltx_ERROR undefined">\al@pirinen2012effects,pirinen2013quality</span></a>)</cite> I have already reached the
point of full-fledged, workable systems for spell/checking, and test the full
power of those systems, first for speed and efficiency in
<cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib8a" title="Effect of language and error models on efficiency of finite-state spell-checking and correction" class="ltx_ref">IX</a>)</cite>, and then in a larger scale
survey <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib10" title="Quality and speed trade-offs in weighted finite-state spell-checking" class="ltx_ref">X</a>)</cite>, the quality of spelling suggestions, as
well as extensions to some of the speed measurements. The chronological order
is also visible in the schematic diagramme of
Figure <a href="#S4.F1" title="Figure 1 ‣ 4.5 Chapter Structure ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The finite/state language models (in the
leftmost column) were conceptualised after the statistical weighting schemes
of morphologically complex language (in the middle column) and the resulting
combinations were evaluated on a larger scale (in the rightmost column).</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Division of Labour</h4>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">division of labour</span> within these articles also follows quite
naturally from the chronological development.
In <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib1" title="Weighted finite-state morphological analysis of Finnish compounds with HFST-LEXC" class="ltx_ref"><span class="ltx_ERROR undefined">\al@pirinen2009weighted,pirinen2009weighting</span></a>; <a href="#bib.bib2a" title="Weighting finite-state morphological analyzers using HFST tools" class="ltx_ref"><span class="ltx_ERROR undefined">\al@pirinen2009weighted,pirinen2009weighting</span></a>)</cite>, as I was the author
of the Finnish finite/state implementation of the morphological analyser we
were extending with weight structures, I did much of the groundwork in
implementing the practical software for weighting the analyser. The result of
this work is the weight support in such parts of our finite/state tool chain
as <span class="ltx_text ltx_font_typewriter">hfst-lexc</span> and <span class="ltx_text ltx_font_typewriter">hfst/strings2fst</span> – that is, in practice
the compilers for morphemes and other string sets. In these early articles, the
evaluation scheme and major parts of the written article are the work of
Krister Lindén. In <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib4a" title="Finite-state spell-checking with weighted language and error models" class="ltx_ref">III</a>)</cite> I already take into
account that previous experiments were performed using commercial corpora
unavailable for scientific, free and open-source work, which led me to devise a
statistical training as well as an evaluation scheme based on freely-available
Wikipedia data. The evaluation and statistical training setup that was built in
this article has then evolved throughout the thesis, in the form of collected
<span class="ltx_text ltx_font_typewriter">make</span> recipes, <span class="ltx_text ltx_font_typewriter">awk</span> scripts, <span class="ltx_text ltx_font_typewriter">python</span> scripts and other
pieces of <span class="ltx_text ltx_font_typewriter">bash</span> command-line programming. The engineering work on
Hunspell language models
in <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib3a" title="Building and using existing Hunspell dictionaries and TeX hyphenators as finite-state automata" class="ltx_ref"><span class="ltx_ERROR undefined">\al@pirinen2010building,pirinen2010creating</span></a>; <a href="#bib.bib5a" title="Creating and weighting Hunspell dictionaries as finite-state automata" class="ltx_ref"><span class="ltx_ERROR undefined">\al@pirinen2010building,pirinen2010creating</span></a>)</cite> originated from the
compilation formulas for morphologies, documented e.g.
in <cite class="ltx_cite ltx_citemacro_cite">LindÃ©n<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib46" title="HFST tools for morphology—an efficient open-source package for construction of morphological analyzers" class="ltx_ref">2009</a>)</cite>, and were constructed by Krister Lindén and myself.
The same technique was applied to the building of Apertium system’s language
models in <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib7" title="Compiling Apertium morphological dictionaries with HFST and using them in HFST applications" class="ltx_ref">VII</a>)</cite> with technical help from one of the
Apertium system’s main contributors, Francis Tyers. The context-based weighted
finite/state methods of <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib9a" title="Improving finite-state spell-checker suggestions with part of speech n-grams" class="ltx_ref">VIII</a>)</cite> originated
from <cite class="ltx_cite ltx_citemacro_citet">Silfverberg and LindÃ©n (<a href="#bib.bib109" title="Part-of-speech tagging using parallel weighted finite-state transducers" class="ltx_ref">2010</a>)</cite>, with whom I also co-operated in the
porting of the system to finite/state spelling checker use. The evaluation
scheme was slightly modified from an earlier one used in
<cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib3a" title="Building and using existing Hunspell dictionaries and TeX hyphenators as finite-state automata" class="ltx_ref">IV</a>)</cite>. The writing of the article
<cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib9a" title="Improving finite-state spell-checker suggestions with part of speech n-grams" class="ltx_ref">VIII</a>)</cite> was for the main part my own work, whereas
the mathematical formulas were built in co-operation with the article’s other
authors. In <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib8a" title="Effect of language and error models on efficiency of finite-state spell-checking and correction" class="ltx_ref">IX</a>)</cite>, the evaluation setup and article
was written by myself, with the major part of the technical implementations
done by Sam Hardwick, who is also the main author behind the finite/state
spell/checking component of our current software, as is further detailed
in <cite class="ltx_cite ltx_citemacro_citet">LindÃ©n<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib47" title="Hfstâframework for compiling and applying morphologies" class="ltx_ref">2011</a>)</cite>. In the
articles <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib6" title="Modularisation of Finnish finite-state language descriptionâtowards wide collaboration in open source development of morphological analyser" class="ltx_ref"><span class="ltx_ERROR undefined">\al@pirinen2011modularisation,pirinen2013quality</span></a>; <a href="#bib.bib10" title="Quality and speed trade-offs in weighted finite-state spell-checking" class="ltx_ref"><span class="ltx_ERROR undefined">\al@pirinen2011modularisation,pirinen2013quality</span></a>)</cite> the majority
of the test setup, article structure, engineering and other tasks were
performed by myself, which is reflected in the fact that they are published
under my name only. It is, however, true that all of the work within this
thesis was carried out as a member in the <span class="ltx_ERROR undefined">\gls</span>hfst research group, and there
is no doubt that there is an abundance of minor ideas regarding engineering of
the software, evaluation setup and article writeup that are the creation of
collective brainstorming as well.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>My Contributions</h4>

<div id="S4.SS3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">My contributions</span> to the co-authored articles can be broken down as
follow: In article <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib1" title="Weighted finite-state morphological analysis of Finnish compounds with HFST-LEXC" class="ltx_ref">I</a>)</cite>, I implemented a
proof-of-concept version of the finite-state lexicon compiler with weighting
support and extended the original Finnish description by adding weights of word
forms and their boundaries whereas the first author designed the scheme and
evaluation system. In <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib2a" title="Weighting finite-state morphological analyzers using HFST tools" class="ltx_ref">II</a>)</cite> we extended this scheme
with a similar division of labour, i.e., I finalised the software
implementation and worked with language descriptions and the first author
designed and evaluated; the writing was shared with the first author mainly
doing the overview and conclusion sections. In
<cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib4a" title="Finite-state spell-checking with weighted language and error models" class="ltx_ref">III</a>)</cite>, I made the training corpora and extended
the previous evaluation suites to the task; for the spell-checking automata and
application procedure both authors contributed as well as the research team.
The initial article writing was done by myself including the introductory part
and conclusion, and the final version written jointly by both authors. Articles
<cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib3a" title="Building and using existing Hunspell dictionaries and TeX hyphenators as finite-state automata" class="ltx_ref">IV</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib5a" title="Creating and weighting Hunspell dictionaries as finite-state automata" class="ltx_ref">V</a>)</cite> were
based on finite-state formulations of <span class="ltx_text ltx_font_typewriter">hfst-lexc</span> by the second author
and refined for this purpose by both authors with the assistance of the
research group. Most of the evaluation scheme was carried over from earlier
projects, and was extended and rewritten by myself. The article was structured
with the help of the second author, while writing was mainly done by myself,
including the new formulations of the algorithms. In
article <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib7" title="Compiling Apertium morphological dictionaries with HFST and using them in HFST applications" class="ltx_ref">VII</a>)</cite>, the original idea was discussed
jointly with the authors with myself doing the implementation based on previous
work on different compilation algorithms in other tools in earlier articles.
The evaluation scheme was taken from previous articles with minor modifications
by myself. In article <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib9a" title="Improving finite-state spell-checker suggestions with part of speech n-grams" class="ltx_ref">VIII</a>)</cite>, we used the second
author’s algorithms from earlier work on POS tagging combined with my earlier
evaluation scheme for spell-checking and the automata used for it. The second
author wrote most of the parts concerning mathematics and the description of
finite-state <math id="S4.SS3.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram models, and I wrote the evaluation, introduction and
conclusion sections. In article <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib8a" title="Effect of language and error models on efficiency of finite-state spell-checking and correction" class="ltx_ref">IX</a>)</cite>, on the effects
of weighted finite-state language and error models on speed and efficiency of
finite-state spell-checking, I wrote most of the evaluation based on previous
work, and the second author implemented the software including the optimisation
schemes. The optimisation schemes were constructed jointly with both authors
plus the help of the research group in a few project meeting sessions. In
practice all articles have been discussed in project group meetings ,and the
project has contributed ideas, software pieces, and linguistic descriptions as
further detailed in the articles themselves. For exact changelogs one can also
read SVN
log<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">19</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">19</sup><a href="http://sourceforge.net/p/hfst/code/HEAD/tree/trunk/articles/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://sourceforge.net/p/hfst/code/HEAD/tree/trunk/articles/</a></span></span></span>
and for newer articles also git
commitlog.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">20</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">20</sup><a href="https://github.com/flammie/purplemonkeydishwasher" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/flammie/purplemonkeydishwasher</a></span></span></span></p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Research Questions</h4>

<div id="S4.SS4.p1" class="ltx_para">
<p class="ltx_p">Another way to conceptualise the thesis is under the following <span class="ltx_text ltx_font_bold">research
questions</span>: How to implement reasonable statistical language models for
finite/state spell/checking, how to implement a finite-state equivalent of
state-of-the-art non-finite-state spell/checking, how to develop and maintain
a language model for finite/state spell/checking, what is the quality of
finite/state spell/checking compared with a string-algorithm approach, what
is the speed of the finite/state spell/checking compared with another
software based-approach, what are the limitations of the finite/state-based
approaches compared with string-algorithm spell/checking – and doing all of
this with morphologically complex languages that are lesser-resourced.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Chapter Structure</h4>

<div id="S4.SS5.p1" class="ltx_para">
<p class="ltx_p">The rest of the thesis is <span class="ltx_text ltx_font_bold">organised</span> by topic <span class="ltx_text ltx_font_bold">into chapters</span> as
follows: In Chapter <a href="#thechapter-at-IDc" title="Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">\thechapter</span></a> I summarise prior research on
spell/checking and correction, and describe the development of actual in-use
spell-checkers. I then survey the previous research specifically on
finite/state approaches to spell/checking, and describe the theory of
finite/state spell/checking in terms of our specific implementation.
Chapters <a href="#thechapter-at-IDd" title="Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">\thechapter</span></a>—<a href="#thechapter-at-IDg" title="Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">\thechapter</span></a> contain the
research papers of the thesis sorted under four headings that match the
original goals of this thesis. In Chapter <a href="#thechapter-at-IDd" title="Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">\thechapter</span></a>, I go
through the existing spell/checking and other language models and their use
as part of the spell/checking system, and introduce the finite/state point
of view to the language models that were initially non-finite-state. In
Chapter <a href="#thechapter-at-IDe" title="Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">\thechapter</span></a>, I describe some weighted finite/state
methods to introduce statistics into the language and error models. In
Chapter <a href="#thechapter-at-IDf" title="Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">\thechapter</span></a> I study the finite/state formulations of
error modelling and contrast my implementations with others that have been
used. In Chapter <a href="#thechapter-at-IDg" title="Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">\thechapter</span></a>, I fully evaluate these systems in
terms of both speed and quality to verify their usability in practical and
real-world applications. Most of the articles I have written fall neatly under
one of these headings, but I do revisit a few of them in more than one of the
chapters. In Chapter <a href="#thechapter-at-IDh" title="Chapter \thechapter Conclusion ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">\thechapter</span></a>, I summarise the thesis and lay out
possible future work in the field.</p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="" id="S4.F1.g1" class="ltx_graphics" alt="">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Articles and build order
</figcaption>
</figure>
</section>
</section>
</section>
<section id="thechapter-at-IDc" class="ltx_chapter">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter <span class="ltx_ERROR undefined">\thechapter</span> </span>Background</h2>

<div id="thechapter-at-IDc.p1" class="ltx_para">
<p class="ltx_p">Spell-checking and correction by computer is a topic that is already over half
a century old, and one that has been researched periodically throughout that
time. In this chapter, I attempt to walk through the long history of
spell/checking with an eye on both the scientific study of spell/checking,
as well as the practical end-user systems. While I attempt to cover as much as
possible of the topic, there are bound to be many omissions and for a fuller
picture I recommend reading some of the previous surveys on spell/checking
and correction, such as <cite class="ltx_cite ltx_citemacro_citet">Kukich (<a href="#bib.bib62" title="Spelling correction for the telecommunications network for the deaf" class="ltx_ref">1992</a>)</cite> and
<cite class="ltx_cite ltx_citemacro_citet">Mitton (<a href="#bib.bib76" title="Ordering the suggestions of a spellchecker without using context" class="ltx_ref">2009</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Kukich (<a href="#bib.bib62" title="Spelling correction for the telecommunications network for the deaf" class="ltx_ref">1992</a>)</cite> presents the history of
spell/checking, both scientific improvements and some real-world applications
are very well summed up until the publication time of the early 1990s.
<cite class="ltx_cite ltx_citemacro_citet">Mitton (<a href="#bib.bib76" title="Ordering the suggestions of a spellchecker without using context" class="ltx_ref">2009</a>)</cite> presents some more recent advances on non-word
spelling correction. <cite class="ltx_cite ltx_citemacro_citet">Kukich (<a href="#bib.bib62" title="Spelling correction for the telecommunications network for the deaf" class="ltx_ref">1992</a>)</cite> also refers to
spell/checking as a <em class="ltx_emph">perennial topic</em> in computational linguistics. I
find this characterisation quite accurate as it has been a prevalent and
recurring theme throughout the history of computational linguistics and earlier
in computer science.</p>
</div>
<div id="thechapter-at-IDc.p2" class="ltx_para">
<p class="ltx_p">This background chapter is organised as follows: In Section <a href="#S5" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>,
I go through the history of spell/checking and correction, especially the
scientific contributions that relate to my research, and the practical software
systems I have used as a baseline for my target functionality. After dealing
with the history of spell/checking and correction I go through the closely
related scientific fields in Section <a href="#S6" title="6 Related Subfields and Research Results ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, whose results are
relevant to my research. Finally, I go through the current situation in
Section <a href="#S7" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, and specifically the finite/state
approaches to spell/checking and correction, and describe my particular
system. I also detail much of the notation and terminology in these chapters,
and contrast them with other similar approaches.</p>
</div>
<section id="S5" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Brief History and Prior Work</h3>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">The history of spell/checking and correction by computer is usually said to
have begun somewhere around the 1960s, with inventions like Levenshtein’s and
Damerau’s measures of distances between strings
<cite class="ltx_cite ltx_citemacro_citep">(Levenshtein, <a href="#bib.bib65" title="Binary codes capable of correcting deletions, insertions, and reversals" class="ltx_ref">1966</a>; Damerau, <a href="#bib.bib26" title="A technique for computer detection and correction of spelling errors" class="ltx_ref">1964</a>)</cite>, or the
Damerau/Levenshtein <em class="ltx_emph">edit distance</em>, which even today is the core of
practically all spelling correction algorithms. This measure defines the
distance between two strings of characters, in terms of editing operations
performed on the string to match it to the other string. The editing operations
defined were the following:</p>
</div>
<div id="S5.p2" class="ltx_para">
<ul id="I2" class="ltx_itemize">
<li id="I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_itemize">•</span>
<div id="I2.i1.p1" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph">deletion</em> of a letter</p>
</div>
</li>
<li id="I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_itemize">•</span>
<div id="I2.i2.p1" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph">addition</em> of a letter</p>
</div>
</li>
<li id="I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_itemize">•</span>
<div id="I2.i3.p1" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph">changing</em> a letter to another, and</p>
</div>
</li>
<li id="I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_itemize">•</span>
<div id="I2.i4.p1" class="ltx_para">
<p class="ltx_p"><em class="ltx_emph">swapping</em> transposing adjacent letters (missing in
Levenshtein’s formulation, central to spelling correction for
e.g., keyboards)</p>
</div>
</li>
</ul>
</div>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p">For example, the distance between <em class="ltx_emph">cat</em> and <span class="ltx_ERROR undefined">\markoverwith</span><span class="ltx_ERROR undefined">\textcolor</span>red<span class="ltx_text" style="position:relative; bottom:-3.5pt;"><span class="ltx_ERROR undefined">\sixly</span>:</span><span class="ltx_ERROR undefined">\ULon</span>ca is 1 (deletion
of t), the distance between <em class="ltx_emph">cat</em> and <span class="ltx_ERROR undefined">\markoverwith</span><span class="ltx_ERROR undefined">\textcolor</span>red<span class="ltx_text" style="position:relative; bottom:-3.5pt;"><span class="ltx_ERROR undefined">\sixly</span>:</span><span class="ltx_ERROR undefined">\ULon</span>catt is 1 (addition of
t), and so forth. Formally, these operations create a metric between two
strings, and for spell-checking applications it is typical to find the smallest
distance between strings, or the least amount of editing operations when
correcting. It is also possible to use values differing from 1 for the
operations to obtain better suitable distance metrics for specific spelling
correction tasks.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p class="ltx_p">It is easy to see from the definitions of the operations how useful they are
for spelling correction; these operation provide the model for detecting and
correcting the basic typos or slips of the finger on a keyboard.
<cite class="ltx_cite ltx_citemacro_citet">Damerau (<a href="#bib.bib26" title="A technique for computer detection and correction of spelling errors" class="ltx_ref">1964</a>)</cite> is often cited as pointing out that 95% of the
errors are covered by distance 1, i.e. one application of this edit algorithm
would fix 95% of misspellings in a text. While this is indeed the claim made
in Damerau’s article, citing it in the context of a modern spelling-checker may
neglect the difference between input mechanisms (and data transmission, and
storage) of computers of the time. It is likely that spelling errors from
cursive hand writing, punch cards, and keyboards are different, though this
variation is dealt with in the article <cite class="ltx_cite ltx_citemacro_citep">(Damerau, <a href="#bib.bib26" title="A technique for computer detection and correction of spelling errors" class="ltx_ref">1964</a>)</cite>. Further
studies on different forms of computerised texts <cite class="ltx_cite ltx_citemacro_citep">(Kukich, <a href="#bib.bib63" title="Techniques for automatically correcting words in text" class="ltx_ref">1992</a>)</cite>
have shown that the range of single edit errors is around 70%–95% in various
setups (e.g. OCR, dictating and typing, normal running text).</p>
</div>
<div id="S5.p5" class="ltx_para">
<p class="ltx_p">Much of the early research on error detection
concentrated on efficiently looking up words from finite wordlists and
building data structures to work as <span class="ltx_ERROR undefined">\glspl</span>language model. During this time,
the first statistical approaches, as in <cite class="ltx_cite ltx_citemacro_citet">Raviv (<a href="#bib.bib98" title="Decision making in Markov chains applied to the problem of pattern recognition" class="ltx_ref">1967</a>)</cite>, drawing
from basic foundations of mathematical theory of information from as early as
<cite class="ltx_cite ltx_citemacro_citet">Shannon (<a href="#bib.bib106" title="A mathematical theory of communications, I and II" class="ltx_ref">1948</a>)</cite>,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">21</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">21</sup>Referred to
in <cite class="ltx_cite ltx_citemacro_citet">Liberman (<a href="#bib.bib67" title="Noisily channeling Claude Shannon" class="ltx_ref">2012</a>)</cite></span></span></span> were devised. In
Raviv’s research statistical theories are applied to characters in English
legal texts, recognising also names and other terms. The input mode of these
early applications seem to be more towards OCR than keyboard input. Their
approach took the letter <math id="S5.p5.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams and assumed that words containing unlikely
letter combinations were not spelled correctly.</p>
</div>
<div id="S5.p6" class="ltx_para">
<p class="ltx_p">For the very first spell/checking software in common use – there are a few
disputes and claims to it – but it is commonly attributed to SPELL
program <cite class="ltx_cite ltx_citemacro_citep">(Gorin, <a href="#bib.bib34" title="SPELL: a spelling checking and correction program" class="ltx_ref">1971</a>)</cite>. Some of the earlier work has been used mainly
by one research group for their own purposes
only.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">22</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">22</sup><a href="http://www.stanford.edu/~learnest/legacies.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.stanford.edu/~learnest/legacies.pdf</a></span></span></span> The first
predecessors of SPELL according to Les Earnest were systems for recognising
hand-written cursive text as auxiliary parts of larger software. Following this
SPELL was possibly among the first stand-alone software for spell/checking.</p>
</div>
<div id="S5.p7" class="ltx_para">
<p class="ltx_p">Much of the following research on error detection consisted of more elaborate
data structures for fast lookup and efficient encoding of large dictionaries.
While interesting as a computer science and data structures topic, it is not
particularly relevant to this thesis, so I will merely summarise that it
consisted of hashing, suffix-trees and binary search trees, partitioning of
dictionary by frequency <cite class="ltx_cite ltx_citemacro_citep">(Knuth, <a href="#bib.bib57" title="The art of computer programming" class="ltx_ref">1973</a>)</cite> and most importantly,
finite/state automata <cite class="ltx_cite ltx_citemacro_citep">(Aho and Corasick, <a href="#bib.bib3" title="Efficient string matching: an aid to bibliographic search" class="ltx_ref">1975</a>)</cite>. Although Aho and Corasick’s
article is about constructing more specific finite/state automata for
efficient keyword matching it constitutes an early implementation of
finite/state spell/checking.</p>
</div>
<div id="S5.p8" class="ltx_para">
<p class="ltx_p">SPELL program’s direct descendant is the current international
ispell <cite class="ltx_cite ltx_citemacro_citep">(Gorin, <a href="#bib.bib34" title="SPELL: a spelling checking and correction program" class="ltx_ref">1971</a>)</cite>, where the additional i originates from the name
<span class="ltx_text ltx_font_typewriter">ITS SPELL</span>. It is still commonly in use in many Unix-based systems.
According to its documentation, the feature of suffix stripping based on
classification was added by Bill Ackerman in 1978, e.g. it would only attempt
to strip the plural suffix <em class="ltx_emph">-es</em> for words that were identified as having
this plural suffix. The concept of affix flags is still used in all of
ispell’s successors as well.</p>
</div>
<div id="S5.p9" class="ltx_para">
<p class="ltx_p">At the turn of the 1990s there was a lot of new research on improving error
correction capabilities, possibly partially due to rapid growth in the
popularity of home computers. Most popularly the use of statistics from large
text corpora and large datasets of real errors that could be processed to learn
probabilities of word forms and error types was applied and extensively
tested <cite class="ltx_cite ltx_citemacro_citep">(Kernighan<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib56" title="A spelling correction program based on a noisy channel model" class="ltx_ref">1990</a>; Church and Gale, <a href="#bib.bib22" title="Probability scoring for spelling correction" class="ltx_ref">1991</a>)</cite>. These simple and
popular methods are still considered to be useful for modern spell-checkers,
which can be seen as common revisions of techniques, such as
in <cite class="ltx_cite ltx_citemacro_citet">Brill and Moore (<a href="#bib.bib17" title="An improved error model for noisy channel spelling correction" class="ltx_ref">2000</a>)</cite>.</p>
</div>
<div id="S5.p10" class="ltx_para">
<p class="ltx_p">One of the digressions was to improve <span class="ltx_ERROR undefined">\glspl</span>error model for English
competence errors. The initial work for this is the often cited Soundex
algorithm, originally meant for cataloguing names in a manner that allowed
similarly pronounced names to be easily found <cite class="ltx_cite ltx_citemacro_citep">(Russell and Odell, <a href="#bib.bib102" title="Soundex" class="ltx_ref">1918</a>)</cite>. It
can also be used to match common words, since it is a <em class="ltx_emph">similarity key</em>
that maps multiple words into one code and back, for example <span class="ltx_ERROR undefined">\markoverwith</span><span class="ltx_ERROR undefined">\textcolor</span>red<span class="ltx_text" style="position:relative; bottom:-3.5pt;"><span class="ltx_ERROR undefined">\sixly</span>:</span><span class="ltx_ERROR undefined">\ULon</span>squer
and <em class="ltx_emph">square</em> have the same code <span class="ltx_text ltx_font_typewriter">S140</span> and can be matched. What
soundex similarity basically does is save the first letter and assign the
remaining non-adjacent non-vowels a number.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">23</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">23</sup>For details, see e.g.
<a href="http://en.wikipedia.org/wiki/Soundex" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://en.wikipedia.org/wiki/Soundex</a> or the finite-state formulation in
the thesis article <cite class="ltx_cite ltx_citemacro_citealias"><a href="#bib.bib10" title="Quality and speed trade-offs in weighted finite-state spell-checking" class="ltx_ref">X</a></cite></span></span></span> There have been some
schemes to elaborate and make this work for foreign names and more words, most
notably Metaphones
by <cite class="ltx_cite ltx_citemacro_citet">Philips (<a href="#bib.bib92" title="Hanging on the metaphone" class="ltx_ref">1990</a>, <a href="#bib.bib91" title="The double metaphone search algorithm" class="ltx_ref">2000</a>)</cite>.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">24</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">24</sup>The third is a
commercial product without publicly available documentation:
<a href="http://amorphics.com/buy_metaphone3.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://amorphics.com/buy_metaphone3.html</a></span></span></span></p>
</div>
<div id="S5.p11" class="ltx_para">
<p class="ltx_p">As computational power increased it became increasingly practical to look again
at the problem of real-word spelling error detection. <cite class="ltx_cite ltx_citemacro_citet">Mays<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib73" title="Context based spelling correction" class="ltx_ref">1991</a>)</cite>
suggested that with English context-based approaches it is possible to detect
76% of the spelling errors, and are able to correct 73%. Context-aware
models are a requirement for the discovery of real-word spelling errors, but
independently of that, they can also be used to improve error correction.</p>
</div>
<div id="S5.p12" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Al-Mubaid and Truemper (<a href="#bib.bib5" title="Learning to find context based spelling errors" class="ltx_ref">2006</a>)</cite> show that it is possible to uncover common real-word
errors directly from the text using correctly spelled reference texts to
calculate context factors for the words, and seeking words that deviate
sufficiently enough from the factors in the other texts.</p>
</div>
<div id="S5.p13" class="ltx_para">
<p class="ltx_p">In the world of context-aware models, simple word-form <math id="S5.p13.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams are not the
only form of context that has been used – especially in the context of
languages other than English, it has often been noted that using sequences of
morphological analyses instead of the surface word forms is a more important
factor in detecting errors and improving the results <cite class="ltx_cite ltx_citemacro_citep">(Otero<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib86" title="Contextual spelling correction" class="ltx_ref">2007</a>, for
Spanish)</cite>. I have performed an experiment with this
approach in <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib9a" title="Improving finite-state spell-checker suggestions with part of speech n-grams" class="ltx_ref">VIII</a>)</cite> for Finnish.</p>
</div>
<div id="S5.p14" class="ltx_para">
<p class="ltx_p">The problems of implementing Hungarian with ispell, aspell and the like lead to
Hunspell, with multiple affix stripping and compounding added. Similarly for
Turkish, various computational methods have been used. Among those,
<cite class="ltx_cite ltx_citemacro_citet">Oflazer (<a href="#bib.bib84" title="Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction" class="ltx_ref">1996</a>)</cite> demonstrates one of the first finite/state
spell/checking with a full-fledged finite-state language model. There is an
earlier finite-state spelling correction system described by
<cite class="ltx_cite ltx_citemacro_citet">Aho and Corasick (<a href="#bib.bib3" title="Efficient string matching: an aid to bibliographic search" class="ltx_ref">1975</a>)</cite>, where both the dictionary and the texts are limited
to keyword search from indexes. This method used a specialised search
algorithm for error tolerance when traversing the finite/state network.
<cite class="ltx_cite ltx_citemacro_citet">Savary (<a href="#bib.bib1a" title="Typographical nearest-neighbor search in a finite-state lexicon and its application to spelling correction" class="ltx_ref">2002</a>)</cite> extends these finite/state methods to cover
the error-model as well. Their work showed a practical implementation of
finite/state edit distance, and finally in
<cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib4a" title="Finite-state spell-checking with weighted language and error models" class="ltx_ref">III</a>)</cite>, I have shown that an approach by an
extension to weighted finite/state automata for both language and error
models for spell/checking and correction is plausible for
morphologically complex languages.</p>
</div>
<div id="S5.p15" class="ltx_para">
<p class="ltx_p">In Table <a href="#S5.T1" title="Table 1 ‣ 5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> I have first summarised the practical
end-user applications of spell/checking. In the first column I give the
related software and academic reference if available, in the second column is
the year of the publication of the software or research finding. In the third
column is a short note about the error models the system uses, the abbreviation
ED is used for edit distance algorithm and FSA for finite-state automata’s
expressive power. In the fourth column is a characterisation of the dictionary
or language model format, e.g., the wordlist means finite list of words and
affixes which generally refer to algorithms that can remove add specific
suffixes to some words of the wordlist (or remove them, depending on how you
view the implementation). In the first set of entries I give the history from
the original SPELL software up until its contemporary descendant and <em class="ltx_emph">de
facto</em> standard of <span class="ltx_ERROR undefined">\gls</span>floss: Hunspell. The dictionaries have not changed
much in this branch, practically all use word-lists with some affix
capabilities based on classifying the words. In error correction, all use at
least some form of edit distance algorithm, with varying weights or orderings
of the edit operations, including simple soundslike weighting in GNU
aspell<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">25</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">25</sup><a href="http://aspell.net/man-html/The-Simple-Soundslike.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://aspell.net/man-html/The-Simple-Soundslike.html</a></span></span></span>
and Metaphone variants for English in kspell and its successors. More details
on error models are provided in Chapter <a href="#thechapter-at-IDf" title="Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">\thechapter</span></a>. In the
second part of Table, I summarise some of the main academic research on
specific practical and statistical advances as originally presented
by <cite class="ltx_cite ltx_citemacro_citet">Al-Mubaid and Truemper (<a href="#bib.bib5" title="Learning to find context based spelling errors" class="ltx_ref">2006</a>)</cite> that I have used during my research. For example, the
prominent <span class="ltx_text ltx_font_typewriter">correct</span> spell-checker re-ranker
by <cite class="ltx_cite ltx_citemacro_citet">Church and Gale (<a href="#bib.bib22" title="Probability scoring for spelling correction" class="ltx_ref">1991</a>)</cite>, which basically re-orders the results of the
above-mentioned spell programs by probabilities, has influenced some of my
spell-checker designs so that the re-ranking approach has been designed into
the original system. The further statistical approaches that are relevant are
the Bayesian approach <cite class="ltx_cite ltx_citemacro_citep">(Golding, <a href="#bib.bib35" title="A Bayesian hybrid method for context-sensitive spelling correction" class="ltx_ref">1995</a>)</cite> and
Winnow <cite class="ltx_cite ltx_citemacro_citep">(Golding and Roth, <a href="#bib.bib37" title="A winnow-based approach to context-sensitive spelling correction" class="ltx_ref">1999</a>)</cite>. The main differences between these approaches
lie in statistical formulations. In the final section of Table, I show the
main history of contemporary finite-state spell-checking, starting
from <cite class="ltx_cite ltx_citemacro_citet">Oflazer (<a href="#bib.bib84" title="Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction" class="ltx_ref">1996</a>)</cite>. For this section the noteworthy
differences are whether the error correction uses basic edit distance measures
in the finite-state graph, a more advanced search, or finite-state algebra with
the full expressive power of regular grammars. Other differences are in the
implementation and efficiency of the spelling correction or error-tolerant
search.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>History of spell-checker applications. Abbreviation ED stands for
edit distance and (W)FSA for weighted finite-state automata
</figcaption>
<table class="ltx_tabular">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Name</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Year</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Error Models</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Language Models</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r">(Authors)</th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_r"></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt ltx_border_t" colspan="4"><span class="ltx_text ltx_font_bold">SPELL – Unix – FLOSS branch (*spell)</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">SPELL,</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">1971</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ED1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Word-list,</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_citep">(Gorin, <a href="#bib.bib34" title="SPELL: a spelling checking and correction program" class="ltx_ref">1971</a>)</cite></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left ltx_border_r">(for partial</td>
<td class="ltx_td ltx_align_left ltx_border_r">English affixes</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_l ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left ltx_border_r">wordlist)</td>
<td class="ltx_td ltx_border_r"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r">ITS SPELL,</td>
<td class="ltx_td ltx_align_right ltx_border_r">1978</td>
<td class="ltx_td ltx_align_left ltx_border_r">ED1</td>
<td class="ltx_td ltx_align_left ltx_border_r">Affix rules</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r">(Bill Ackerman)</td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left ltx_border_r">unrestricted</td>
<td class="ltx_td ltx_border_r"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r">international ispell</td>
<td class="ltx_td ltx_align_right ltx_border_r">1988</td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left ltx_border_r">Non-English</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r">(Geoff Kuenning)</td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left ltx_border_r">support</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">kspell,</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">1998</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Metaphone 1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Affix rules</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r">GNU aspell</td>
<td class="ltx_td ltx_align_right ltx_border_r">2002</td>
<td class="ltx_td ltx_align_left ltx_border_r">Rule-weighted ED</td>
<td class="ltx_td ltx_align_left ltx_border_r">Compounding</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r">(Kevin Atkins)</td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left ltx_border_r">soundslike</td>
<td class="ltx_td ltx_align_left ltx_border_r">(dropped)</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">myspell</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">2000</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">weighted ED</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2 affixes</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Hunspell</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">2005</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">weighted ED</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">2 affixes</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_l ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left ltx_border_r">Confusables</td>
<td class="ltx_td ltx_align_left ltx_border_r">Compounds</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="4"><span class="ltx_text ltx_font_bold">Academic projects etc.</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">correct</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">1991</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Probabilistic</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Probalistic</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_citep">(Church and Gale, <a href="#bib.bib22" title="Probability scoring for spelling correction" class="ltx_ref">1991</a>)</cite></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left ltx_border_r">ED</td>
<td class="ltx_td ltx_align_left ltx_border_r">wordlist</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">bayspell</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">1995</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Bayesian</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Bayesian</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_citep">(Golding, <a href="#bib.bib35" title="A Bayesian hybrid method for context-sensitive spelling correction" class="ltx_ref">1995</a>)</cite></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">WinSpell</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">1999</td>
<td class="ltx_td ltx_border_r ltx_border_t"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Winnow</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_citep">(Golding and Roth, <a href="#bib.bib37" title="A winnow-based approach to context-sensitive spelling correction" class="ltx_ref">1999</a>)</cite></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="4"><span class="ltx_text ltx_font_bold">Finite-State</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_citep">(Oflazer, <a href="#bib.bib84" title="Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction" class="ltx_ref">1996</a>)</cite></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">1996</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">ED using</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">FSA</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_l ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left ltx_border_r">error-tolerant</td>
<td class="ltx_td ltx_border_r"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_l ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left ltx_border_r">search</td>
<td class="ltx_td ltx_border_r"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_citep">(Savary, <a href="#bib.bib1a" title="Typographical nearest-neighbor search in a finite-state lexicon and its application to spelling correction" class="ltx_ref">2002</a>)</cite></td>
<td class="ltx_td ltx_align_right ltx_border_r">2002</td>
<td class="ltx_td ltx_align_left ltx_border_r">ED using</td>
<td class="ltx_td ltx_align_left ltx_border_r">FSA</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_l ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left ltx_border_r">FSA algebra</td>
<td class="ltx_td ltx_border_r"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_citep">(Schulz and Mihov, <a href="#bib.bib105" title="Fast string correction with Levenshtein-automata" class="ltx_ref">2002</a>)</cite></td>
<td class="ltx_td ltx_align_right ltx_border_r">2002</td>
<td class="ltx_td ltx_align_left ltx_border_r">ED using</td>
<td class="ltx_td ltx_align_left ltx_border_r">FSA</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_l ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left ltx_border_r">pre-composed FSA</td>
<td class="ltx_td ltx_border_r"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_citep">(Mohri, <a href="#bib.bib79" title="Edit-distance of weighted automata: general definitions and algorithms" class="ltx_ref">2003</a>)</cite></td>
<td class="ltx_td ltx_align_right ltx_border_r">2003</td>
<td class="ltx_td ltx_align_left ltx_border_r">ED using</td>
<td class="ltx_td ltx_align_left ltx_border_r">WFSA</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_l ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left ltx_border_r">WFSA algebra</td>
<td class="ltx_td ltx_border_r"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_citep">(Otero<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib86" title="Contextual spelling correction" class="ltx_ref">2007</a>)</cite></td>
<td class="ltx_td ltx_align_right ltx_border_r">2007</td>
<td class="ltx_td ltx_align_left ltx_border_r">FSA</td>
<td class="ltx_td ltx_align_left ltx_border_r">FSA</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_citep">(HuldÃ©n, <a href="#bib.bib49" title="Fast approximate string matching with finite automata" class="ltx_ref">2009</a>)</cite></td>
<td class="ltx_td ltx_align_right ltx_border_r">2009</td>
<td class="ltx_td ltx_align_left ltx_border_r">FSA using</td>
<td class="ltx_td ltx_align_left ltx_border_r">FSA</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_l ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_left ltx_border_r">search algorithm</td>
<td class="ltx_td ltx_border_r"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r"><cite class="ltx_cite ltx_citemacro_citep">(Pirinen and LindÃ©n, <a href="#bib.bib4a" title="Finite-state spell-checking with weighted language and error models" class="ltx_ref">2010c</a>)</cite></td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r">2010</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r">WFSA</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r">WFSA</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S6" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Subfields and Research Results</h3>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">There are a number of other research questions in the field of computational
linguistics that use the same methodology and face the same problems. A large
set of research problems within computational linguistics can be formulated in
some frames that are directly relevant to the issues of spell/checking and
correction, as most of them require a basic component from a language model and
very often another one from an error model. The language model of
spell/checking predicts how correct a word form is, and in a linguistic
analyser it predicts the analysis and its likelihood. The error model of a
spelling corrector predicts what is meant based on what is written assuming an
error somewhere in the process of typing. Some of the error sources, such as
cognitive errors, overlap in speech recognition and information extraction
alike. Some of the error models for other applications like diacritic
restoration for information retrieval and noise cancellation for speech
recognition may solve different problems but the approaches used are still
applicable. Realising all this, at one point of the research, made me
persistently require a modular design from our spelling correction
systems – including a strict separation of the spelling detection task and the
correction task – to be able to mix and match the approaches found in various
sources to the spelling correction. The rest of this subsection I use to
briefly introduce the specific approaches and algorithms from outside the
spelling detection and correction domain that I have re-used or tried to
re-use in my experiments on spell/checking and correction.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p class="ltx_p">The relevant research on the finite/state approaches to <em class="ltx_emph">predictive text
entry</em> is referred to e.g. in <cite class="ltx_cite ltx_citemacro_citet">Silfverberg and LindÃ©n (<a href="#bib.bib109" title="Part-of-speech tagging using parallel weighted finite-state transducers" class="ltx_ref">2010</a>)</cite>, and the
results of these applications have been applied without modifications to our
language models where applicable.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p class="ltx_p">In many practical systems, a spelling corrector or its weak equivalent is used
for pre-processing. This phase of processing is often called e.g.
<em class="ltx_emph">normalisation</em>, <em class="ltx_emph">diacritic restoration</em>, and so forth. Normalisation
and diacritic restoration are both basically spell-checking tasks with very
limited alphabets and error models, e.g. in diacritic restorations only allowed
corrections are diacritical additions to a base character (e.g. changing
<em class="ltx_emph">a</em> to <em class="ltx_emph">à</em>, <em class="ltx_emph">á</em>, <em class="ltx_emph">ã</em>, <em class="ltx_emph">ą</em>, etc.), similarly
normalisation restricts correction to e.g., case changes and typographic
variants. Many normalisation approaches are directly relevant to spelling
correction, and parts of the spell/checking system I have developed have been
inspired by the development of systems such as mobile text message
normalisation to standard written language <cite class="ltx_cite ltx_citemacro_citep">(Kobus<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib59" title="Normalizing SMS: are two metaphors better than one" class="ltx_ref">2008</a>)</cite>.
Especially recent approaches in the context of social networking message
normalisation are nearly identical to the finite-state spell-checking presented
in this dissertation, e.g. <cite class="ltx_cite ltx_citemacro_citet">HuldÃ©n (<a href="#bib.bib50" title="Weighted and unweighted transducers for tweet normalization" class="ltx_ref">2013</a>)</cite>.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p class="ltx_p">The field of <em class="ltx_emph">historical linguistics</em> also uses finite-state and
edit-distance techniques, e.g., for mapping related words and
word forms <cite class="ltx_cite ltx_citemacro_citep">(Porta<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib90" title="Edit transducers for spelling variation in Old Spanish" class="ltx_ref">2013</a>)</cite>. Furthermore, the statistical approaches used
in recent works in the field, such as <cite class="ltx_cite ltx_citemacro_citet">Petterson<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib89" title="An SMT approach to automatic annotation of historical text" class="ltx_ref">2013</a>)</cite>, could well be
used in the context of spelling correction.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Theory of Finite-State Models</h3>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">Finite-state models for spell/checking and especially correction are
relatively new, beginning from <cite class="ltx_cite ltx_citemacro_citep">(Oflazer, <a href="#bib.bib84" title="Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction" class="ltx_ref">1996</a>)</cite>, and
definitions and interpretations have not been standardised yet, so in this
chapter, I will go through the various definitions and explain my solution and
how I ended up with it. To begin with, I will use a few paragraphs to recap the
formal definitions of finite/state systems and my selected notations. For
further information on finite-state theory, see e.g.
<cite class="ltx_cite ltx_citemacro_citet">Aho<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib4" title="Compilers: principles, techniques, and tools" class="ltx_ref">2007</a>); Mohri (<a href="#bib.bib77" title="Finite-state transducers in language and speech processing" class="ltx_ref">1997</a>)</cite>.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p class="ltx_p">The <span class="ltx_ERROR undefined">\glspl</span>fsa are conventionally marked in computer science and mathematics
as systems, such as n-tuple <math id="S7.p2.m1" class="ltx_Math" alttext="(Q,\Sigma,\delta,Q_{i},Q_{f},\rho)" display="inline"><mrow><mo stretchy="false">(</mo><mi>Q</mi><mo>,</mo><mi mathvariant="normal">Σ</mi><mo>,</mo><mi>δ</mi><mo>,</mo><msub><mi>Q</mi><mi>i</mi></msub><mo>,</mo><msub><mi>Q</mi><mi>f</mi></msub><mo>,</mo><mi>ρ</mi><mo stretchy="false">)</mo></mrow></math>, where <math id="S7.p2.m2" class="ltx_Math" alttext="Q" display="inline"><mi>Q</mi></math> is
the set of the states in the automaton, <math id="S7.p2.m3" class="ltx_Math" alttext="\Sigma" display="inline"><mi mathvariant="normal">Σ</mi></math> is the alphabet in
transitions, <math id="S7.p2.m4" class="ltx_Math" alttext="\delta:Q\times\Sigma\times W\rightarrow Q" display="inline"><mrow><mi>δ</mi><mo>:</mo><mrow><mrow><mi>Q</mi><mo>×</mo><mi mathvariant="normal">Σ</mi><mo>×</mo><mi>W</mi></mrow><mo>→</mo><mi>Q</mi></mrow></mrow></math> is a
deterministic transition mapping from a state to a state with an alphabet and a
weight, and <math id="S7.p2.m5" class="ltx_Math" alttext="Q_{i}\subset Q,Q_{f}\subset Q" display="inline"><mrow><mrow><msub><mi>Q</mi><mi>i</mi></msub><mo>⊂</mo><mi>Q</mi></mrow><mo>,</mo><mrow><msub><mi>Q</mi><mi>f</mi></msub><mo>⊂</mo><mi>Q</mi></mrow></mrow></math> the subsets of states for initial
states and final states, <math id="S7.p2.m6" class="ltx_Math" alttext="\rho:Q_{f}\rightarrow W" display="inline"><mrow><mi>ρ</mi><mo>:</mo><mrow><msub><mi>Q</mi><mi>f</mi></msub><mo>→</mo><mi>W</mi></mrow></mrow></math> the final weight mapping and
<math id="S7.p2.m7" class="ltx_Math" alttext="W" display="inline"><mi>W</mi></math> is the structure of weights. The <math id="S7.p2.m8" class="ltx_Math" alttext="\Sigma" display="inline"><mi mathvariant="normal">Σ</mi></math> set in the automata of a
spell/checking system is almost always just some – usually language-specific
for optimisation reasons – subset of the Unicode set of
symbols <cite class="ltx_cite ltx_citemacro_citep">(Unicode Consortium, <a href="#bib.bib113" title="The Unicode standard" class="ltx_ref">2013</a>)</cite> for writing natural languages, with the addition of the
following special symbols, which have specific meaning in automata theory: the
empty symbol epsilon <math id="S7.p2.m9" class="ltx_Math" alttext="\varepsilon" display="inline"><mi>ε</mi></math> that matches zero-length strings on application
of the automata, and the wild-card symbol <math id="S7.p2.m10" class="ltx_Math" alttext="?" display="inline"><mi mathvariant="normal">?</mi></math> that matches any one symbol of
<math id="S7.p2.m11" class="ltx_Math" alttext="\Sigma" display="inline"><mi mathvariant="normal">Σ</mi></math> during any application of the automaton. When talking of transducers,
it merely means that the alphabet is of the form <math id="S7.p2.m12" class="ltx_Math" alttext="\Sigma^{2}" display="inline"><msup><mi mathvariant="normal">Σ</mi><mn>2</mn></msup></math>. Practically this
can be thought of so that the <span class="ltx_ERROR undefined">\gls</span>finite-state acceptor accepts e.g., correct
wordforms and the <span class="ltx_ERROR undefined">\gls</span>fst, e.g., rewrites misspelled word forms into one or
more corrected forms. The weighted <span class="ltx_ERROR undefined">\glspl</span>fsa discussed throughout the thesis
are using the <span class="ltx_ERROR undefined">\gls</span>tropical semi-ring <cite class="ltx_cite ltx_citemacro_citep">(Mohri, <a href="#bib.bib77" title="Finite-state transducers in language and speech processing" class="ltx_ref">1997</a>)</cite> weight
structure <math id="S7.p2.m13" class="ltx_Math" alttext="(\mathbb{R}_{+}\cup\infty,\min,+)" display="inline"><mrow><mo stretchy="false">(</mo><mrow><msub><mi>ℝ</mi><mo>+</mo></msub><mo>∪</mo><mi mathvariant="normal">∞</mi></mrow><mo>,</mo><mi>min</mi><mo>,</mo><mo>+</mo><mo stretchy="false">)</mo></mrow></math>; this is the so-called penalty
weight structure, which practically means that on application and weight
combination the smallest one is used, on combination of the weights they are
added together. The set of final states is extended with a final weight
function <math id="S7.p2.m14" class="ltx_Math" alttext="\rho:W\rightarrow Q" display="inline"><mrow><mi>ρ</mi><mo>:</mo><mrow><mi>W</mi><mo>→</mo><mi>Q</mi></mrow></mrow></math> that specifies an additional weight for the
paths in an automaton in each end-state. For the rest of the finite/state
algebra I use the standard notations which to my knowledge do not have any
variation that requires documenting in this introduction (e.g. <math id="S7.p2.m15" class="ltx_Math" alttext="\cup" display="inline"><mo>∪</mo></math> for
union, <math id="S7.p2.m16" class="ltx_Math" alttext="\cap" display="inline"><mo>∩</mo></math> for intersection and so forth). I will furthermore use notation
<math id="S7.p2.m17" class="ltx_Math" alttext="\gls{machine}" display="inline"><mrow><merror class="ltx_ERROR undefined undefined"><mtext>\gls</mtext></merror><mo>⁢</mo><mi>m</mi><mo>⁢</mo><mi>a</mi><mo>⁢</mo><mi>c</mi><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>i</mi><mo>⁢</mo><mi>n</mi><mo>⁢</mo><mi>e</mi></mrow></math>, often with a subscript: <math id="S7.p2.m18" class="ltx_Math" alttext="\mathcal{M}_{\mathrm{name}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>name</mi></msub></math> for
finite-state machine, where the <math id="S7.p2.m19" class="ltx_Math" alttext="\mathrm{name}" display="inline"><mi>name</mi></math> is a descriptive name for an
automaton or an abbreviation.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p class="ltx_p">An <span class="ltx_ERROR undefined">\gls</span>fsa, with a <math id="S7.p3.m1" class="ltx_Math" alttext="\Sigma" display="inline"><mi mathvariant="normal">Σ</mi></math> set drawn from the set of natural language
alphabets, such as letters A through Z, digits 0 through 9 and the punctuation
marks hyphen and apostrophe, can accurately encode most words of the English
language in the accepting paths of the automaton – excepting such edge cases
as <em class="ltx_emph">naïve</em>. This kind of acceptors which recognise the words of a language
are used both in the process of detecting spelling errors of non-word type in a
running text, and matching the misspelt word forms to the correct word forms.
The use of such automata as language models is documented very extensively in
natural language processing. In the context of morphologically complex
languages more relevant to the topics of this thesis see, e.g., Finite-State
Morphology <cite class="ltx_cite ltx_citemacro_citep">(Beesley and Karttunen, <a href="#bib.bib8" title="Finite state morphology" class="ltx_ref">2003</a>; Beesley, <a href="#bib.bib11" title="Morphological analysis and generation: a first step in natural language processing" class="ltx_ref">2004</a>)</cite>.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p class="ltx_p">I use <span class="ltx_ERROR undefined">\glspl</span>fsa also in error correction. In this case, <span class="ltx_ERROR undefined">\glspl</span>fst encode
the relations from misspellings to corrections in their accepting paths. There
are few influential works in the field of finite/state methods for error
modelling. The initial work was laid out by <cite class="ltx_cite ltx_citemacro_citet">Oflazer (<a href="#bib.bib84" title="Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction" class="ltx_ref">1996</a>)</cite>,
who uses an algorithmic approach on language model traversal as a limited form
of error modelling; this has been extended and improved by among others
 <cite class="ltx_cite ltx_citemacro_citet">HuldÃ©n (<a href="#bib.bib49" title="Fast approximate string matching with finite automata" class="ltx_ref">2009</a>)</cite>. An approach to weighted finite/state systems was
described by <cite class="ltx_cite ltx_citemacro_citet">Mohri (<a href="#bib.bib79" title="Edit-distance of weighted automata: general definitions and algorithms" class="ltx_ref">2003</a>)</cite>, which includes a good mathematical basis
for finite-state error-modelling using weighted edit-distance automata and
weighted automata algorithms.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Savary (<a href="#bib.bib1a" title="Typographical nearest-neighbor search in a finite-state lexicon and its application to spelling correction" class="ltx_ref">2002</a>)</cite> describes the finite/state error model as an
automaton. In that paper, the research concentrates on the concept of expanding
the language-model automata by a Levenshtein rule automaton, creating a
language model automaton that can recognise word forms containing a given
number of Levenshtein type errors, i.e. all word forms at a given
Levenshtein-Damerau distance. My definition diverges here by considering the
error model as a separate, arbitrary transducer; this allows operating on
either the language model or the misspelt string with the error producing or
removing the functionality of the model, and provides an opportunity to test
which variation is the most effective.</p>
</div>
<div id="S7.p6" class="ltx_para">
<p class="ltx_p">Now we can also make the generalisation of considering the strings of the
language that we are correcting as a single-path acceptor consisting of just
one word, so we can formally define a finite/state spelling correction system
as the composition <math id="S7.p6.m1" class="ltx_Math" alttext="(\mathcal{M}_{\mathrm{word}}\circ\mathcal{M}_{\mathrm{E}}\circ\mathcal{M}_{%
\mathrm{L}})_{2}" display="inline"><msub><mrow><mo stretchy="false">(</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>word</mi></msub><mo>∘</mo><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi mathvariant="normal">E</mi></msub><mo>∘</mo><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi mathvariant="normal">L</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msub></math>, where <math id="S7.p6.m2" class="ltx_Math" alttext="\mathcal{M}_{\mathrm{word}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>word</mi></msub></math> is the word
to correct, <math id="S7.p6.m3" class="ltx_Math" alttext="\mathcal{M}_{\mathrm{E}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi mathvariant="normal">E</mi></msub></math> the automaton encoding the error model,
and <math id="S7.p6.m4" class="ltx_Math" alttext="\mathcal{M}_{\mathrm{L}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi mathvariant="normal">L</mi></msub></math> the automaton encoding the language model, and
<math id="S7.p6.m5" class="ltx_Math" alttext="{}_{2}" display="inline"><msub><mi></mi><mn>2</mn></msub></math> is the projection selecting the results from the second tape of the final
composition, i.e., the one on the language model side.
</p>
</div>
<div id="S7.p7" class="ltx_para">
<p class="ltx_p">The weights in weighted finite/state automata are used to encode the
preference in spelling correction, and sometimes also acceptability in the
spell/checking function. A path that has a larger collected weight gets
demoted in the suggestion list and those with smaller weights are promoted.
The weight can be amended by the error model, in which the weight expresses the
preference on errors corrected when mapping the incorrect string to correct.
One of my contributions throughout the thesis is a theory and methodology for
creating, acquiring and combining these weights in a way that is optimal for
speed, and that is usable for morphologically complex languages with limited
resources.</p>
</div>
<div id="S7.p8" class="ltx_para">
<p class="ltx_p">The baseline for acquisition of the weights for language and error models is
simply calculating and encoding probabilities –this is what most of the
comparable products do in spell/checking and correction, and what has been
researched in statistical language models. The key formula for conceiving any
statistical process in the tropical semiring giving the probabilistic
distribution <math id="S7.p8.m1" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math> as a part of a finite/state automaton is <math id="S7.p8.m2" class="ltx_Math" alttext="-\log P" display="inline"><mrow><mo>-</mo><mrow><mi>log</mi><mo>⁡</mo><mi>P</mi></mrow></mrow></math>, i.e. the
smaller the probability the bigger the weight. The probability here is not
straight-forward, but relatively simple. For most things we calculate</p>
</div>
<div id="S7.p9" class="ltx_para">
<table id="S7.E1" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S7.E1.m1" class="ltx_Math" alttext="P(x)=\frac{f(x)}{\sum_{z\in\mathcal{D}}f(z)}," display="block"><mrow><mrow><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><msub><mo largeop="true" symmetric="true">∑</mo><mrow><mi>z</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi></mrow></msub><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mfrac></mrow><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr>
</table>
</div>
<div id="S7.p10" class="ltx_para ltx_noindent">
<p class="ltx_p">where <math id="S7.p10.m1" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> is the event, <math id="S7.p10.m2" class="ltx_Math" alttext="f()" display="inline"><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mo stretchy="false">)</mo></mrow></mrow></math> is the frequency of the event, and
<span class="ltx_ERROR undefined">\gls</span>collection the collection of all events, so the probability of <math id="S7.p10.m3" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> is
counted as the proportion of the event <math id="S7.p10.m4" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> in all events <math id="S7.p10.m5" class="ltx_Math" alttext="z" display="inline"><mi>z</mi></math> of
<span class="ltx_ERROR undefined">\gls</span>collection. For example, for a simple language model, <math id="S7.p10.m6" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> could be a word
form, and <math id="S7.p10.m7" class="ltx_Math" alttext="\mathcal{D}_{\mathrm{wordforms}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>wordforms</mi></msub></math> a corpus of running text turned
into word forms, then we would expect that <math id="S7.p10.m8" class="ltx_Math" alttext="P(^{\prime}\mathrm{is}^{\prime})&gt;P(^{\prime}\mathrm{quantitatively}^{\prime})" display="inline"><mrow><mi>P</mi><mrow><msup><mo stretchy="false">(</mo><mo>′</mo></msup><msup><mi>is</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow><mo>&gt;</mo><mi>P</mi><mrow><msup><mo stretchy="false">(</mo><mo>′</mo></msup><msup><mi>quantitatively</mi><mo>′</mo></msup><mo stretchy="false">)</mo></mrow></mrow></math>, for any reasonable corpus of the English
language. Similarly for error modelling, <math id="S7.p10.m9" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> might be a typo of ‘a’ for ‘s’
denoted <math id="S7.p10.m10" class="ltx_Math" alttext="\mathrm{a}:\mathrm{s}" display="inline"><mrow><mi mathvariant="normal">a</mi><mo>:</mo><mi mathvariant="normal">s</mi></mrow></math> and <math id="S7.p10.m11" class="ltx_Math" alttext="\mathcal{D}_{\mathrm{errors}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>errors</mi></msub></math> an error
corpus of typos written with a qwerty keyboard layout, then we would expect
<math id="S7.p10.m12" class="ltx_Math" alttext="P(\mathrm{a}:\mathrm{s})&gt;P(\mathrm{a}:\mathrm{l})" display="inline"><mrow><mi>P</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">a</mi><mo>:</mo><mi mathvariant="normal">s</mi><mo stretchy="false">)</mo></mrow><mo>&gt;</mo><mi>P</mi><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">a</mi><mo>:</mo><mi mathvariant="normal">l</mi><mo stretchy="false">)</mo></mrow></mrow></math>.</p>
</div>
<div id="S7.p11" class="ltx_para">
<p class="ltx_p">An important factor that <span class="ltx_ERROR undefined">\glslink</span>morphological
complexitymorphologically-complex languages bring to the concept of
statistical language models is that the amount of different plausible word
forms is greater, the data is more sparse, and essentially language models turn
from finite word-form lists to infinite language models. There is a good
mathematical-lexicographical description of this problem in
<cite class="ltx_cite ltx_citemacro_citet">Kornai (<a href="#bib.bib60" title="How many words are there" class="ltx_ref">2002</a>)</cite>. This means that for simple statistical training models,
the number of unseen words rises, and unseen words in naive models mean a
probability of <math id="S7.p11.m1" class="ltx_Math" alttext="0" display="inline"><mn>0</mn></math>, which would pose problems for simple language models, e.g.
given the above weight formula <math id="S7.p11.m2" class="ltx_Math" alttext="-\log(0)=\infty" display="inline"><mrow><mrow><mo>-</mo><mrow><mi>log</mi><mo>⁡</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>=</mo><mi mathvariant="normal">∞</mi></mrow></math> for any given non-zero-size
corpus. A practical finite/state implementation will regard infinite weight
as a non-accepting path even if it were otherwise a valid path in the
automaton. The traditional approach to dealing with this is well known;
assuming a probability distribution we can estimate the likelihoods of the
tokens that were not seen, discount some of the probability mass of the seen
tokens, or otherwise increase the probability mass and distribute it among the
parts of the language model that would have been unseen. With language models
generating infinitely many word forms, the models made using arbitrary weights
may not be strict probability distributions at all. In practical applications
this does not necessarily matter as the preference relation still works. Some
of this background on not following strict statistical distributions in NLP
context has been given by Google in their statistical machine translation work,
e.g. by <cite class="ltx_cite ltx_citemacro_citet">Brants<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib16" title="Large language models in machine translation" class="ltx_ref">2007</a>)</cite>.</p>
</div>
<div id="S7.p12" class="ltx_para">
<p class="ltx_p">The basic forms of estimating and distributing the probability mass to account
for unseen events has been extensively researched. The basic logic that I have
used in many of my research papers is the simplest known additive discounting:
here the estimated probability for an event <math id="S7.p12.m1" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math> is seen as</p>
</div>
<div id="S7.p13" class="ltx_para">
<table id="S7.E2" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S7.E2.m1" class="ltx_Math" alttext="P(\hat{x})=\frac{f(x)+\alpha}{\sum_{z\in\mathcal{D}}(f(z)+\alpha)}" display="block"><mrow><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mover accent="true"><mi>x</mi><mo stretchy="false">^</mo></mover><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>α</mi></mrow><mrow><msub><mo largeop="true" symmetric="true">∑</mo><mrow><mi>z</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒟</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>f</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mi>α</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr>
</table>
</div>
<div id="S7.p14" class="ltx_para ltx_noindent">
<p class="ltx_p">that is, each frequency is incremented by <math id="S7.p14.m1" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> and this mass
of increments is added to the divisor to keep the distribution in probabilistic
bounds – this has the effect that all unseen tokens are considered to have been
seen <math id="S7.p14.m2" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> times and all others <math id="S7.p14.m3" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> more times than they have been seen
in the training data. For values of <math id="S7.p14.m4" class="ltx_Math" alttext="\alpha" display="inline"><mi>α</mi></math> in my practical applications, I
have used the range 0.5–1, based on empirical testing and values found in the
literature, such as <cite class="ltx_cite ltx_citemacro_citet">Manning and Schütze (<a href="#bib.bib69" title="Foundations of statistical natural language processing" class="ltx_ref">1999</a>)</cite>. There are other, more
elegant methods for discounting as well <cite class="ltx_cite ltx_citemacro_citep">(Chen and Goodman, <a href="#bib.bib21" title="An empirical study of smoothing techniques for language modeling" class="ltx_ref">1999</a>)</cite>. However,
the additive discounting being a few lines of code is reasonably easy to
implement and understand without errors.</p>
</div>
</section>
</section>
<section id="thechapter-at-IDd" class="ltx_chapter">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter <span class="ltx_ERROR undefined">\thechapter</span> </span>Language Models</h2>

<div id="thechapter-at-IDd.p1" class="ltx_para">
<p class="ltx_p">In this chapter, I will go through the various kinds of
<span class="ltx_ERROR undefined">\glspl</span>language model<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">26</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">26</sup>As a terminological note, in <span class="ltx_ERROR undefined">\gls</span>natural language
engineering the term language model is used narrowly, referring only
to systems that are purely statistical <math id="thechapter-at-IDd.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram models. In
computational linguistics a language model is any method capable of
telling whether a given word form is in a language, and can potentially
provide further data about it.</span></span></span> that are
used for the task of spell checking and correction. Depending on the system,
these two tasks can either share a common language model, or use separate and
different language models, or even different approaches to apply the language
models. This separation is an important one and I will try to make it clear
whenever I refer to language models in various systems and aspects of
finite/state spell/checking. The primary purpose of a language model in
both of these tasks is similar: to tell whether a word form is suitable, and,
ideally, how suitable it is.
</p>
</div>
<div id="thechapter-at-IDd.p2" class="ltx_para">
<p class="ltx_p">The purpose of this chapter is to present the development through traditional
simple word-list spell/checking models to <span class="ltx_ERROR undefined">\glslink</span>morphological
complexitycomplex morphological analysers with compounding and derivation,
and their finite/state formulations. This follows from my initial goal to
not only present new language models and spell-checkers in my thesis, but to
use finite/state technology to repeat the existing results of
non-finite/state spell-checkers as a baseline. In the later chapters, I will
show that the finite/state formulations are also as good as, or better than,
their non-finite-state counterparts, and the statistical methods we devise to
support morphologically complex languages can be applied to these
finite/state automata.</p>
</div>
<div id="thechapter-at-IDd.p3" class="ltx_para">
<p class="ltx_p">The rest of the chapter is organised as follows: first in
Section <a href="#S8" title="8 Sketch of Generic Finite-State Formula for Morphology ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, I introduce some generic formulas for finite/state
morphologies we have recognised when developing compilers for quite a few
dictionary formats. Based on this, I show in Section <a href="#S9" title="9 Compiling Hunspell Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> the
finite/state compilation of Hunspell dictionaries, the <em class="ltx_emph">de facto</em>
standard of open-source spell/checking. Then I show the conversion of the
rule-based machine translation dictionaries as spell-checkers in
Section <a href="#S10" title="10 Using Rule-Based Machine Translation Dictionaries ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. In section <a href="#S11" title="11 Other Possibilities for Generic Morphological Formula ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, I describe other
language models that can be used in finite/state spell/checking. Finally,
in Section <a href="#S12" title="12 Maintenance of the Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>, I introduce a brief general treatment on the
management of linguistic data so that it applies to a multitude of projects in
an article where I try to tie together the different approaches to compiling
and using finite/state dictionaries and computational linguistic models
at large.</p>
</div>
<section id="S8" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Sketch of Generic Finite-State Formula for Morphology</h3>

<div id="S8.p1" class="ltx_para">
<p class="ltx_p">Before delving further into the intricacies of compiling existing and new
<span class="ltx_ERROR undefined">\glspl</span>formalism into finite/state automata, I would like to draw the
readers’ attention to one underlying formula of computational morphology that
is central to all these approaches. This formula is the conceptualisation that
all systems are regular combinations of sets of morphemes, combined with rules
of <span class="ltx_ERROR undefined">\gls</span>morphotactics<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">27</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">27</sup>Morphotactics is used to refer to the rules
governing legal combinations of morphs within word forms of a language. For
example, one such rule would be defining that the English plural suffix form
‘-s’ follows a certain subset of noun stems.</span></span></span> and possibly morphophonology.
The basic starting point is an arbitrary combination of morphs as an automaton
<math id="S8.p1.m1" class="ltx_Math" alttext="\mathcal{M}_{\mathrm{morphs}}=(\bigcup_{\mathrm{m\in{\mathcal{D}_{\mathrm{%
morphs}}}}}({\mathrm{m}}))^{\star}" display="inline"><mrow><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>morphs</mi></msub><mo>=</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mo largeop="true" mathsize="160%" stretchy="false" symmetric="true">⋃</mo><mrow><mi mathvariant="normal">m</mi><mo>∈</mo><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>morphs</mi></msub></mrow></msub><mrow><mo stretchy="false">(</mo><mi mathvariant="normal">m</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>⋆</mo></msup></mrow></math>, where
<math id="S8.p1.m2" class="ltx_Math" alttext="\mathcal{D}_{\mathrm{morphs}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>morphs</mi></msub></math> is a collection of morphs of the language where,
that is, a disjunction of strings that are the morphs of a language. This kind
of a <em class="ltx_emph">bag of morphs</em> approach creates an automaton consisting of arbitrary
combinations of the morphs of a language, e.g. for English it would have
combinations like ‘cat’, ‘cats’, ‘dog’, ‘dogs’, …but also ‘catdog’,
‘sdog’, or ‘sssdogsdogsscatcat’. To restrict these, we usually define
classifications of these morphs, say, ‘cat’ and ‘dog’ are <span class="ltx_text ltx_font_typewriter">[nouns]</span>, and
‘s’ is a <span class="ltx_text ltx_font_typewriter">[plural]</span>, English morphotax would define that a plural
appears after nouns. To realise morphotactic rules over the classifications of
morphs, we can usually create a set of helper symbols, e.g. <span class="ltx_text ltx_font_typewriter">[nouns]</span>,
<span class="ltx_text ltx_font_typewriter">[plural]</span> <math id="S8.p1.m3" class="ltx_Math" alttext="\in\Gamma,\Gamma\cap\Sigma=\emptyset" display="inline"><mrow><mrow><mi></mi><mo>∈</mo><mi mathvariant="normal">Γ</mi></mrow><mo>,</mo><mrow><mrow><mi mathvariant="normal">Γ</mi><mo>∩</mo><mi mathvariant="normal">Σ</mi></mrow><mo>=</mo><mi mathvariant="normal">∅</mi></mrow></mrow></math> and then have
morphotax as an automaton construed over <math id="S8.p1.m4" class="ltx_Math" alttext="(\Gamma\cup\Sigma)^{\star}" display="inline"><msup><mrow><mo stretchy="false">(</mo><mrow><mi mathvariant="normal">Γ</mi><mo>∪</mo><mi mathvariant="normal">Σ</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⋆</mo></msup></math>. In this
example we might define something like <span class="ltx_text ltx_font_typewriter">[nouns]</span> <math id="S8.p1.m5" class="ltx_Math" alttext="\Sigma^{\star}" display="inline"><msup><mi mathvariant="normal">Σ</mi><mo>⋆</mo></msup></math>
<span class="ltx_text ltx_font_typewriter">[plural]</span> <math id="S8.p1.m6" class="ltx_Math" alttext="\Sigma^{\star}" display="inline"><msup><mi mathvariant="normal">Σ</mi><mo>⋆</mo></msup></math>, if those special symbols are prefixed to the
relevant morphs.</p>
</div>
</section>
<section id="S9" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Compiling Hunspell Language Models</h3>

<div id="S9.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Main Article</span>: <em class="ltx_emph">Building and Using Existing Hunspell Dictionaries
and TeX Hyphenators as Finite-State Automata</em>, by Tommi A Pirinen and Krister
Lindén. In this article we present finite/state formulations of existing
spell/checking language and error models.</p>
</div>
<section id="S9.SS1" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1 </span>Motivation</h4>

<div id="S9.SS1.p1" class="ltx_para">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">motivation</span> for this piece of engineering is far-reaching both on
the technical and the theoretical level. We set out to prove that the existing
methods for spell/checking are truly a subset of finite/state methods. In
practice, we also tried to show that typically, if not always, the
finite/state version should be faster than the non-finite-state version, e.g.
for affix stripping, but also for error modelling. The practical motivation for
the work is that the chances for any new spell/checking system surviving are
rather limited, unless it is capable of benefiting from the existing Hunspell
dictionaries without major difficulties.</p>
</div>
</section>
<section id="S9.SS2" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.2 </span>Related Works</h4>

<div id="S9.SS2.p1" class="ltx_para">
<p class="ltx_p">In <span class="ltx_text ltx_font_bold">related works</span>, there has, to my knowledge, only been, one attempt
to use Hunspell data as automata, namely a master’s thesis project partially
guided by Hunspell maintainers <cite class="ltx_cite ltx_citemacro_citep">(Greenfield and Judd, <a href="#bib.bib38" title="Open source natural language processing" class="ltx_ref">2010</a>)</cite>.
The
original Hunspell has been dealt with in an academic context
by <cite class="ltx_cite ltx_citemacro_citet">TrÃ³n<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib112" title="Hunmorph: open source word analysis" class="ltx_ref">2005</a>)</cite>.</p>
</div>
</section>
<section id="S9.SS3" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.3 </span>Results</h4>

<div id="S9.SS3.p1" class="ltx_para">
<p class="ltx_p">There is a number of meaningful <span class="ltx_text ltx_font_bold">results</span> in our article. It is a
central part of the thesis in that it provides the empirical proof that
finite/state spell/checking is a proper superset of Hunspell’s algorithmic
approach in terms of expressive power, and the empirical results also suggest
an improvement in speed across the board. The results on speed are detailed in
my later articles and also in Chapter <a href="#thechapter-at-IDg" title="Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">\thechapter</span></a> of this thesis, so
here I will first concentrate on enumerating the parts of the first
result – the expressiveness and faithfulness of the finite/state formulation
of the Hunspell systems.</p>
</div>
<div id="S9.SS3.p2" class="ltx_para">
<p class="ltx_p">Hunspell’s language model writing formalism clearly shows that it is based on
the main branch of word-list-based ispell spell-checkers. Words are assigned
flags that combine them with affixes, where the concept of affixes is extended
with context restrictions and deletions, although with the same affix logic as
its predecessors. The additional features brought to Hunspell to set it apart
from previous versions were the possibility of having more than one affix per
root – two for most versions, and then a number of various extra features based
on the same flags as used for affixation, such as compounding, offensive
suggestion pruning,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">28</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">28</sup>E.g. English Hunspell dictionary will recognise,
but not suggest such word forms as <em class="ltx_emph">asshole, bugger, shitty, etc.</em></span></span></span> and
limited circumfixation. A majority of these features come from the basic
formula we use for a bag of morphs and filters style finite/state morphology
as described by <cite class="ltx_cite ltx_citemacro_citet">LindÃ©n<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib46" title="HFST tools for morphology—an efficient open-source package for construction of morphological analyzers" class="ltx_ref">2009</a>)</cite>. The practical side of the main result
shows that all of the language models are around the same size as
finite/state dictionaries and morphologies made with other methods, as shown
in Table <a href="#S9.T2" title="Table 2 ‣ 9.3 Results ‣ 9 Compiling Hunspell Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reproduced
from <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib3a" title="Building and using existing Hunspell dictionaries and TeX hyphenators as finite-state automata" class="ltx_ref">IV</a>)</cite>.</p>
</div>
<figure id="S9.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Compiled Hunspell automata sizes in HFST. Reproduced
from <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib3a" title="Building and using existing Hunspell dictionaries and TeX hyphenators as finite-state automata" class="ltx_ref">IV</a>)</cite>
</figcaption>
<table class="ltx_tabular">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Language</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Dictionary</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Roots</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span class="ltx_text ltx_font_bold">Affixes</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Portugese (Brazil)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">14 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">307,199</td>
<td class="ltx_td ltx_align_center ltx_border_tt">25,434</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Polish</td>
<td class="ltx_td ltx_align_center ltx_border_r">14 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">277,964</td>
<td class="ltx_td ltx_align_center">6,909</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Czech</td>
<td class="ltx_td ltx_align_center ltx_border_r">12 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">302,542</td>
<td class="ltx_td ltx_align_center">2,492</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Hungarian</td>
<td class="ltx_td ltx_align_center ltx_border_r">9.7 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">86,230</td>
<td class="ltx_td ltx_align_center">22,991</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Northern Sámi</td>
<td class="ltx_td ltx_align_center ltx_border_r">8.1 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">527,474</td>
<td class="ltx_td ltx_align_center">370,982</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Slovak</td>
<td class="ltx_td ltx_align_center ltx_border_r">7.1 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">175,465</td>
<td class="ltx_td ltx_align_center">2,223</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Dutch</td>
<td class="ltx_td ltx_align_center ltx_border_r">6.7 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">158,874</td>
<td class="ltx_td ltx_align_center">90</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Gascon</td>
<td class="ltx_td ltx_align_center ltx_border_r">5.1 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">2,098,768</td>
<td class="ltx_td ltx_align_center">110</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Afrikaans</td>
<td class="ltx_td ltx_align_center ltx_border_r">5.0 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">125,473</td>
<td class="ltx_td ltx_align_center">48</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Icelandic</td>
<td class="ltx_td ltx_align_center ltx_border_r">5.0 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">222087</td>
<td class="ltx_td ltx_align_center">0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Greek</td>
<td class="ltx_td ltx_align_center ltx_border_r">4.3 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">574,961</td>
<td class="ltx_td ltx_align_center">126</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Italian</td>
<td class="ltx_td ltx_align_center ltx_border_r">3.8 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">95,194</td>
<td class="ltx_td ltx_align_center">2,687</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Gujarati</td>
<td class="ltx_td ltx_align_center ltx_border_r">3.7 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">168,956</td>
<td class="ltx_td ltx_align_center">0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Lithuanian</td>
<td class="ltx_td ltx_align_center ltx_border_r">3.6 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">95,944</td>
<td class="ltx_td ltx_align_center">4,024</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">English (Great Britain)</td>
<td class="ltx_td ltx_align_center ltx_border_r">3.5 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">46,304</td>
<td class="ltx_td ltx_align_center">1,011</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">German</td>
<td class="ltx_td ltx_align_center ltx_border_r">3.3 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">70,862</td>
<td class="ltx_td ltx_align_center">348</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Croatian</td>
<td class="ltx_td ltx_align_center ltx_border_r">3.3 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">215,917</td>
<td class="ltx_td ltx_align_center">64</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Spanish</td>
<td class="ltx_td ltx_align_center ltx_border_r">3.2 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">76,441</td>
<td class="ltx_td ltx_align_center">6,773</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Catalan</td>
<td class="ltx_td ltx_align_center ltx_border_r">3.2 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">94,868</td>
<td class="ltx_td ltx_align_center">996</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Slovenian</td>
<td class="ltx_td ltx_align_center ltx_border_r">2.9 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">246,857</td>
<td class="ltx_td ltx_align_center">484</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Faroese</td>
<td class="ltx_td ltx_align_center ltx_border_r">2.8 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">108,632</td>
<td class="ltx_td ltx_align_center">0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">French</td>
<td class="ltx_td ltx_align_center ltx_border_r">2.8 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">91,582</td>
<td class="ltx_td ltx_align_center">507</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Swedish</td>
<td class="ltx_td ltx_align_center ltx_border_r">2.5 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">64,475</td>
<td class="ltx_td ltx_align_center">330</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">English (U.S.)</td>
<td class="ltx_td ltx_align_center ltx_border_r">2.5 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">62,135</td>
<td class="ltx_td ltx_align_center">41</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Estonian</td>
<td class="ltx_td ltx_align_center ltx_border_r">2.4 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">282,174</td>
<td class="ltx_td ltx_align_center">9,242</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Portuguese (Portugal)</td>
<td class="ltx_td ltx_align_center ltx_border_r">2 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">40.811</td>
<td class="ltx_td ltx_align_center">913</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Irish</td>
<td class="ltx_td ltx_align_center ltx_border_r">1.8 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">91,106</td>
<td class="ltx_td ltx_align_center">240</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Friulian</td>
<td class="ltx_td ltx_align_center ltx_border_r">1.7 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">36,321</td>
<td class="ltx_td ltx_align_center">664</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Nepalese</td>
<td class="ltx_td ltx_align_center ltx_border_r">1.7 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">39,925</td>
<td class="ltx_td ltx_align_center">502</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Thai</td>
<td class="ltx_td ltx_align_center ltx_border_r">1.7 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">38,870</td>
<td class="ltx_td ltx_align_center">0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Esperanto</td>
<td class="ltx_td ltx_align_center ltx_border_r">1.5 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">19,343</td>
<td class="ltx_td ltx_align_center">2,338</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Hebrew</td>
<td class="ltx_td ltx_align_center ltx_border_r">1.4 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">329237</td>
<td class="ltx_td ltx_align_center">0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Bengali</td>
<td class="ltx_td ltx_align_center ltx_border_r">1.3 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">110,751</td>
<td class="ltx_td ltx_align_center">0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Frisian</td>
<td class="ltx_td ltx_align_center ltx_border_r">1.2 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">24,973</td>
<td class="ltx_td ltx_align_center">73</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Interlingua</td>
<td class="ltx_td ltx_align_center ltx_border_r">1.1 MiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">26850</td>
<td class="ltx_td ltx_align_center">54</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Persian</td>
<td class="ltx_td ltx_align_center ltx_border_r">791 KiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">332,555</td>
<td class="ltx_td ltx_align_center">0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Indonesian</td>
<td class="ltx_td ltx_align_center ltx_border_r">765 KiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">23,419</td>
<td class="ltx_td ltx_align_center">17</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Azerbaijani</td>
<td class="ltx_td ltx_align_center ltx_border_r">489 KiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">19,132</td>
<td class="ltx_td ltx_align_center">0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Hindi</td>
<td class="ltx_td ltx_align_center ltx_border_r">484 KiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">15,991</td>
<td class="ltx_td ltx_align_center">0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Amharic</td>
<td class="ltx_td ltx_align_center ltx_border_r">333 KiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">13,741</td>
<td class="ltx_td ltx_align_center">4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_r">Chichewa</td>
<td class="ltx_td ltx_align_center ltx_border_r">209 KiB</td>
<td class="ltx_td ltx_align_center ltx_border_r">5,779</td>
<td class="ltx_td ltx_align_center">0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Kashubian</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r">191 KiB</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r">5,111</td>
<td class="ltx_td ltx_align_center ltx_border_b">0</td>
</tr>
</tbody>
</table>
</figure>
<div id="S9.SS3.p3" class="ltx_para">
<p class="ltx_p">When talking about Hunspell it is impossible to avoid the motivation behind
the development of a new system for spell/checking – as my thesis is also
about a new system for spell/checking. Hunspell was made on the basis that
the existing spell/checking dictionary formalisms are insufficient to write a
Hungarian dictionary efficiently. Similar systems were written for many of the
more morphologically complex languages.
</p>
</div>
<div id="S9.SS3.p4" class="ltx_para">
<p class="ltx_p">As the implementation framework for context restrictions and deletions for the
Hunspell affix morphotactics, we used <em class="ltx_emph">twol</em> (two-level)
rules <cite class="ltx_cite ltx_citemacro_citep">(Karttunen<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib42" title="Two-level morphology with composition" class="ltx_ref">1992</a>)</cite>. Twol rules are a generic way of defining valid
contexts of character pairs in a transducer. Hunspell on the other hand,
defines deletions, and combinations of morphs. In practice this means that when
Hunspell formulates that a suffix follows a word root in a certain class if it
ends in a given regular expression, our twol representation was made to allow
the morph boundary symbol in that regular expression context while also
deleting the context as defined by another expression. With the context
restrictions and its combinatorics twol rules are well within reason, as
restriction is the central rule type in the design of twol, whereas the
deletions are not very easy to implement in that formalism. As an afterthought,
a better and more efficient representation might have been to combine
morphotactic filters with replace style rules <cite class="ltx_cite ltx_citemacro_citep">(Karttunen, <a href="#bib.bib43" title="The replace operator" class="ltx_ref">1995</a>)</cite>,
although neither the compilation time nor the clarity of the algorithms are a
key issue for one-shot format conversions like this.</p>
</div>
<div id="S9.SS3.p5" class="ltx_para">
<p class="ltx_p">One of the main stumbling blocks for some of the finite/state systems with
full language models is that – while the running of finite/state automata is
known to be fast – the building of finite/state automata can be lengthy. This
did not appear to be an issue for the practical systems we tried.</p>
</div>
</section>
</section>
<section id="S10" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Using Rule-Based Machine Translation Dictionaries</h3>

<div id="S10.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Main Article:</span> <em class="ltx_emph">Compiling Apertium morphological dictionaries with
HFST and using them in HFST applications</em> by Tommi A Pirinen and Francis M.
Tyers. In this engineering-oriented article, we experiment with the existing
language models used for machine translation as part of the finite/state
spell/checking system.</p>
</div>
<section id="S10.SS1" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.1 </span>Motivation</h4>

<div id="S10.SS1.p1" class="ltx_para">
<p class="ltx_p">The main <span class="ltx_text ltx_font_bold">motivation</span> for doing this experiment was to see how well we
can reuse the vast numbers of existing dictionaries from other projects as
finite/state automata in completely unrelated language technology systems.
This is especially interesting from my computer science background, since
the reusability of code and data is something that I feel is lacking in
computational linguistics even now in the 2010s.</p>
</div>
</section>
<section id="S10.SS2" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.2 </span>Related Works</h4>

<div id="S10.SS2.p1" class="ltx_para">
<p class="ltx_p">As this experiment was built on an existing, partially finite/state-based
system, the <span class="ltx_text ltx_font_bold">related works</span> already consist of a specific algorithm for
building the automata from XML dictionaries <cite class="ltx_cite ltx_citemacro_citep">(OrtÃ­z-Rojas<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib100" title="ConstrucciÃ³n y minimizaciÃ³n eficiente de transductores de letras a partir de diccionarios con paradigmas" class="ltx_ref">2005</a>)</cite>
for fast and accurate tokenising and analysing. To that end, our
experiment does not provide any significant improvements, the final automata
are nearly the same and the tokenisation algorithm used to evaluate the result
closely imitates the one described in the article.</p>
</div>
<div id="S10.SS2.p2" class="ltx_para">
<p class="ltx_p">The compilation formula presented in the article was merely a simplification of
the ones used for Hunspell and lexc, making it relatively straightforward and
short as a technical contribution. This straightforward approach is notable
however, as the computational side is based on a solid standardised XML
format for dictionary representation and interchange, and this makes it optimal
for future experiments.</p>
</div>
</section>
<section id="S10.SS3" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.3 </span>Results</h4>

<div id="S10.SS3.p1" class="ltx_para">
<p class="ltx_p">The main <span class="ltx_text ltx_font_bold">results</span> shown in the paper are the improvement on text
processing time as shown in Table <a href="#S10.T3" title="Table 3 ‣ 10.3 Results ‣ 10 Using Rule-Based Machine Translation Dictionaries ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. This is in line
with previous results <cite class="ltx_cite ltx_citemacro_citep">(Silfverberg and LindÃ©n, <a href="#bib.bib108" title="HFST runtime format—a compacted transducer format allowing for fast lookup" class="ltx_ref">2009</a>)</cite>. More importantly to the
topic of this thesis, results showing the reasonably fast processing time of
the finite/state spelling checkers built in this manner; this is not totally
unexpected as the <span class="ltx_ERROR undefined">\gls</span>formalism does not support compounding or recurring
derivation, and thus the final automata are bound to be acyclic, except
potential regular expression recognisers in language models, which we chose to
exclude from the spell-checking models.</p>
</div>
<div id="S10.SS3.p2" class="ltx_para">
<p class="ltx_p">The main result that we tried to highlight in the paper was that we could
already provide a rudimentary spell-checker based on lexical data built solely
for the purpose of limited range machine translation, and we could do that for
a range of languages, including some that at the time of writing lacked
spell-checkers altogether. A list of the languages tested here is provided in
Table <a href="#S10.T3" title="Table 3 ‣ 10.3 Results ‣ 10 Using Rule-Based Machine Translation Dictionaries ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, reproduced
from <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib7" title="Compiling Apertium morphological dictionaries with HFST and using them in HFST applications" class="ltx_ref">VII</a>)</cite>. The task of writing the compiler for
this existing format based on other morphology <span class="ltx_ERROR undefined">\glspl</span>formalism, that we had
made, and turning the compiled language model into a baseline spell-checker,
was not more than a few days’ work. This point concerning the generic
usefulness of finite/state systems is often easily overlooked with assumption
that all new systems require months of development time.</p>
</div>
<figure id="S10.T3" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Language</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:90%;">HFST Rate</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:90%;">Apertium lookup</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold" style="font-size:90%;">HFST lookup</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">Basque</span></th>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">7,900</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">35.7 s</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><span class="ltx_text" style="font-size:90%;">160.0 s</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r"><span class="ltx_text" style="font-size:90%;">Norwegian</span></th>
<td class="ltx_td ltx_align_right ltx_border_r"><span class="ltx_text" style="font-size:90%;">9,200</span></td>
<td class="ltx_td ltx_align_right ltx_border_r"><span class="ltx_text" style="font-size:90%;">6.6 s</span></td>
<td class="ltx_td ltx_align_right ltx_border_r"><span class="ltx_text" style="font-size:90%;">200.2 s</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r"><span class="ltx_text" style="font-size:90%;">Manx</span></th>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r"><span class="ltx_text" style="font-size:90%;">4,700</span></td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r"><span class="ltx_text" style="font-size:90%;">0.8 s</span></td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r"><span class="ltx_text" style="font-size:90%;">11.2 s</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Efficiency of spelling correction in an artificial test setup,
average over three runs. Rate given in words per second. Speed of
HFST-based system against original in compilation and speed of HFST-based
system against original in corpus analysis (as seconds in user time).
Reformatted from <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib7" title="Compiling Apertium morphological dictionaries with HFST and using them in HFST applications" class="ltx_ref">VII</a>)</cite>.
</figcaption>
</figure>
<div id="S10.SS3.p3" class="ltx_para">
<p class="ltx_p">A side result, which I partially failed to communicate in this paper, is the
development towards a generalised compilation formula for finite/state
dictionaries. The compilation formula in this article is a variation of the
earlier formulas I have presented <cite class="ltx_cite ltx_citemacro_citep">(LindÃ©n<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib46" title="HFST tools for morphology—an efficient open-source package for construction of morphological analyzers" class="ltx_ref">2009</a>)</cite> and
<cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib3a" title="Building and using existing Hunspell dictionaries and TeX hyphenators as finite-state automata" class="ltx_ref">IV</a>)</cite> – I propose that this has two implications:
for the engineering side, we can use the same algorithms to compile all the
different dictionaries, which means that the automatic improvements and
optimisations that can be applied to the structure of the automata will improve
all applications. More importantly, it means that the different application
formalisms for writing dictionaries use the same linguistic models and the same
kind of abstraction i, or lack of abstraction, is present. This suggests that
there is a potential for generalisations such that different applications could
converge using a single, linguistically motivated dictionary format.</p>
</div>
</section>
</section>
<section id="S11" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">11 </span>Other Possibilities for Generic Morphological Formula</h3>

<div id="S11.p1" class="ltx_para">
<p class="ltx_p">There is an abundance of formalisms for writing dictionaries, analysers and
language technology tools available, and I have only covered the most prominent
for use in finite/state spell/checking. For example, I have given a
compilation formula for the Hunspell formalism. The aspell and ispell
formalisms are simplifications of this, and the formula can easily be
simplified for them by removing the multiple suffix consideration and
compounding loop in the bag-of-morphs construction: <math id="S11.p1.m1" class="ltx_Math" alttext="\mathcal{M}_{L}=\mathcal{M}_{\mathrm{prefix}}\mathcal{M}_{\mathrm{roots}}%
\mathcal{M}_{\mathrm{suffix}}" display="inline"><mrow><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>L</mi></msub><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>prefix</mi></msub><mo>⁢</mo><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>roots</mi></msub><mo>⁢</mo><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>suffix</mi></msub></mrow></mrow></math>, where <math id="S11.p1.m2" class="ltx_Math" alttext="\mathcal{M}_{\mathrm{prefix}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>prefix</mi></msub></math>,
<math id="S11.p1.m3" class="ltx_Math" alttext="\mathcal{M}_{\mathrm{roots}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>roots</mi></msub></math>, and <math id="S11.p1.m4" class="ltx_Math" alttext="\mathcal{M}_{\mathrm{suffix}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>suffix</mi></msub></math> are subsets of
<math id="S11.p1.m5" class="ltx_Math" alttext="\mathcal{M}_{\mathrm{morphs}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>morphs</mi></msub></math>, that is, disjunctions of strings that make up
the morphs of a language.</p>
</div>
<div id="S11.p2" class="ltx_para">
<p class="ltx_p">It should also be noted that the common baseline approaches using word-form
lists or such spell/checking corpora, such as <cite class="ltx_cite ltx_citemacro_citet">Norvig (<a href="#bib.bib83" title="How to write a spelling corrector" class="ltx_ref">2010</a>)</cite>, are
even easier to formulate as the whole language model is just a disjunction of
the surface word forms.</p>
</div>
</section>
<section id="S12" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">12 </span>Maintenance of the Language Models</h3>

<div id="S12.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Main Article:</span> <em class="ltx_emph">Modularisation of Finnish finite/state language
description—towards wide collaboration in open-source development of
morphological analyser</em> by Tommi A Pirinen. This short paper is mainly an
opinion piece article, but it is included here as it ties together the
aspect of maintainable language models for spell/checking applications.</p>
</div>
<section id="S12.SS1" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">12.1 </span>Motivation</h4>

<div id="S12.SS1.p1" class="ltx_para">
<p class="ltx_p">One of the main <span class="ltx_text ltx_font_bold">motivations</span> for writing this article was the
realisation after some years of work on the finite/state language model
compilation and harvesting different computational dictionaries and
descriptions of morphologies that there is a common pattern in all the
data – as one generalised finite/state algebraic compilation algorithm works
for all of them. An opposite but equally strong motivation was that for
lesser-resourced languages a single computational language model is a very
precious resource that needs to be reused for all possible applications, as
there are no human resources to rewrite these descriptions separately for each
application. This should motivate computational linguists in general to work
towards reusable, well-written resources, with proper abstraction levels and
basic knowledge representation.</p>
</div>
<div id="S12.SS1.p2" class="ltx_para">
<p class="ltx_p">Another motivation is that the language descriptions, whether for
finite/state morphological analysis or Hunspell spell/checking are
typically written by one linguist, and ignored when no one can read them any
longer. This is a not a useful approach in the crowd-sourcing world of today,
and it could surely be alleviated by simple improvements in formalisms and
practices. In this branch of my research I argue for building more
crowd-sourceable systems, in the form of linguistically motivated data that is
understandable for anyone fluent in language. While producing easy access to
achievable goals in dictionary building, the research of crowd-sourcing
lexicography is still a new topic in science and very much a work in progress.</p>
</div>
</section>
<section id="S12.SS2" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">12.2 </span>Related Works</h4>

<div id="S12.SS2.p1" class="ltx_para">
<p class="ltx_p">Looking into the <span class="ltx_text ltx_font_bold">related works</span>, I soon realised that I was not
totally, though admittedly mostly, alone in my feeling that finite/state and
other computational language descriptions need a some thought to be
maintainable and re-usable. The three notable computational linguists writing
on the same topic that I immediately came across when researching the topic
were <cite class="ltx_cite ltx_citemacro_citet">Maxwell and David (<a href="#bib.bib71" title="Joint grammar development by linguists and computer scientists" class="ltx_ref">2008</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Wintner (<a href="#bib.bib119" title="Strengths and weaknesses of finite-state technology: a case study in morphological grammar development" class="ltx_ref">2008</a>)</cite>. Similar
concerns have been addressed in non-finite-state computational linguistics in
the Grammatical Framework project, of which, e.g., the concept of smart
paradigms <cite class="ltx_cite ltx_citemacro_citep">(Ranta, <a href="#bib.bib97" title="How predictable is Finnish morphology? An experiment on lexicon construction" class="ltx_ref">2008</a>)</cite> was in part an inspiration for me to
start thinking of better systems for managing finite-state language
descriptions. The common features of all our concerns include the lack of
abstraction, and therefore very application-specific hacks – such as encoding
optimisations of graph structure into the dictionary data as
by <cite class="ltx_cite ltx_citemacro_citet">Karttunen (<a href="#bib.bib44" title="Numbers and Finnish numerals" class="ltx_ref">2006</a>)</cite> – in language descriptions that could ideally
be very generic. This also leads me to suggest that some related works that are
very under-used in the field of <em class="ltx_emph">computational</em> linguistics are the basic
schoolbook materials on computer science and software engineering- Knuth’s
discussion of literate programming <cite class="ltx_cite ltx_citemacro_citep">(Knuth, <a href="#bib.bib58" title="Literate programming" class="ltx_ref">1984</a>)</cite> and the creation
of and documented programs is useful for that is what computational language
models really are. They are not merely data. Indeed, the whole object-oriented
programming movement, involving the encapsulation, abstraction and management
of data for re-usability and other principled purposes, is valuable.</p>
</div>
</section>
<section id="S12.SS3" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">12.3 </span>Results</h4>

<div id="S12.SS3.p1" class="ltx_para">
<p class="ltx_p">The main <span class="ltx_text ltx_font_bold">results</span> of this paper are seen in the continuity of the
Finnish language model and its relatively good amount of repurposing over the
years. Beyond that, the common movement towards maintainable and reusable
language models in computational linguistics, and morphology specifically, is
still very much work in progress.</p>
</div>
</section>
</section>
<section id="S13" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">13 </span>Conclusions</h3>

<div id="S13.p1" class="ltx_para">
<p class="ltx_p">In this chapter I have shown that the vast majority of the different
contemporary approaches to spell/checking can be formulated as finite/state
automata, and they can even be compiled into one using one simple generic
formula. These results, with the addition of all those existing finite/state
language models that no one has been able to turn into e.g. Hunspell
dictionaries should provide a basis for evidence that finite/state
spell/checking is a plausible alternative for traditional spell/checking
approaches at least as far as the modelling of correctly spelled language is
concerned. Finally, I’ve described a way forward with finite-state morphologies
as a maintainable form of electronic dictionaries for use with a number of
applications including spell-checking.</p>
</div>
</section>
</section>
<section id="thechapter-at-IDe" class="ltx_chapter">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter <span class="ltx_ERROR undefined">\thechapter</span> </span>Statistical Language Models</h2>

<div id="thechapter-at-IDe.p1" class="ltx_para">
<p class="ltx_p">One of the goals of my thesis was to bring the statistical <span class="ltx_ERROR undefined">\glspl</span>language
model closer to usability for <span class="ltx_ERROR undefined">\glslink</span>morphological
complexitymorphologically complex languages with fewer resources. For this
purpose I tried out the most typical traditional statistical approaches turning
them into a finite/state form using well-studied algorithms of weighted
finite/state automata. With the more morphologically complex languages, the
improvement gained by bringing statistical data to the language models in the
system is less encouraging than it is with English. I then experimented with
various ways of improving the results by more efficient use of linguistic data
and simply fine-tuning the parameters.</p>
</div>
<div id="thechapter-at-IDe.p2" class="ltx_para">
<p class="ltx_p">The problem that arises from the combination of morphologically complex
languages and the lesser-resourced languages is challenging, since the amount
and quality of the training data is smaller and the requirements for it are
greater. I go through two separate approaches to tackle this. The first
approach is to gather more linguistically motivated pieces of information from
the data as the primary training feature rather than the plain old surface word
forms. This is not free of problems, as many good pieces of information can
only be extracted from analysed, disambiguated language resources. This
requires manual labour, whereas surface word forms come without extra effort.
So instead of requiring manually annotated data I have studied how to salvage
what can be easily used from running texts without the need of analysis – for
Finnish this meant word forms and roots for compound parts. The other approach
I took was to try and to see if even a limited resource of lesser quality –
that happens to be available under a free and open licence – can be used
combined with well-formed, rule-based material to greatly improve the results
in error correction.</p>
</div>
<div id="thechapter-at-IDe.p3" class="ltx_para">
<p class="ltx_p">In this chapter I will describe the statistical methods I needed to introduce
to get good statistical language models for morphologically complex languages
such as Finnish, though most approaches could be applied to other languages
with a few simple modifications. In Section <a href="#S14" title="14 Theory of Statistical Finite-State Models ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>, I will
introduce the general format of all statistical finite/state models. In
Section <a href="#S15" title="15 Language Models for Languages with Compounding ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>, I discuss the basic findings of the compound
boundary identification problem, and in Section <a href="#S16" title="16 The Statistical Training of Compounding Language Models ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>, I
extend this approach towards generic statistical training of language models
with compounding features. In Section <a href="#S17" title="17 Weighting Hunspell as Finite-State Automata ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>, I use the
same statistical models with the Hunspell dictionaries turned into
finite/state automata. Finally in Section <a href="#S18" title="18 Lesser-Resourced Languages in Statistical Spelling Correction ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18</span></a>, I study
the common problem with the combination of fewer resources and a more complex
morphology which actually requires more resources, and propose some possible
solutions.</p>
</div>
<section id="S14" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">14 </span>Theory of Statistical Finite-State Models</h3>

<div id="S14.p1" class="ltx_para">
<p class="ltx_p">A basic statistical language model is a simple structure that can be
<em class="ltx_emph">trained</em> using a corpus of texts that contains word forms. There is ample
existing research for statistical and trained language models for various
purposes, and I strongly recommend basic works
like <cite class="ltx_cite ltx_citemacro_citet">Manning and Schütze (<a href="#bib.bib69" title="Foundations of statistical natural language processing" class="ltx_ref">1999</a>)</cite> as a good reference for the methods and
mathematical background in this chapter. For spelling correction, a simple
logic of suggesting a word form that is more common before one that is less, is
usually the best approach in terms of precision, and in the majority of cases
this leads to a correct suggestion. This is formalised in weighted
finite/state form by a simple probability formula: <math id="S14.p1.m1" class="ltx_Math" alttext="w(x)=-\log P(x)" display="inline"><mrow><mrow><mi>w</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mo>-</mo><mrow><mrow><mi>log</mi><mo>⁡</mo><mi>P</mi></mrow><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></math>, where
<math id="S14.p1.m2" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> is a weight in the tropical semi-ring structure and <math id="S14.p1.m3" class="ltx_Math" alttext="P(x)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mrow></math> is the probability
of word form <math id="S14.p1.m4" class="ltx_Math" alttext="x" display="inline"><mi>x</mi></math>. Morphologically complex languages’ models are thought to be
more difficult to learn because the number of the different word forms is much
larger than for morphologically simple languages, which leads to decreased
discriminative capabilities for the probability calculations, as the number of
word forms with exactly the same number of appearances increases. In my
research, I have studied some approaches to deal with this situation. What
makes this situation even more difficult is the fact that morphologically
complex languages often have fewer resources. Many of the contributions I have
provided here concentrate on using the scarce resources in a way that will give
nearly the same advantage from the statistical training as one would expect for
morphologically poor languages with more resources.</p>
</div>
<div id="S14.p2" class="ltx_para">
<p class="ltx_p">The more complex statistical language models, such as those that use
morphological, syntactic or other analyses, require a large amount of manually
verified data that is not in general available for the majority of the world’s
languages. In my research, I present some methods to make use of the data that
is available, even if it is unverified, to improve the language models when
possible. In general, I try to show that the significance of analysed data is
not essential to typical spell/checking systems, but more significant for the
fine-tuning of the final product and the building of the grammar-checking
systems from spell-checkers.</p>
</div>
<div id="S14.p3" class="ltx_para">
<p class="ltx_p">Another significant piece of information in statistical models is
<em class="ltx_emph">context</em>. The basic statistical probabilities I described above work on
simple frequencies of word forms in isolation. This kind of model is called
e.g. a <em class="ltx_emph">unigram</em> or context-agnostic language model. The common language
models for applications like part-of-speech guessing, automatic translation and
even spell/checking for real word errors typically use the probabilities of
e.g. word forms in context, taking the likelihood of the word following its
preceding word forms as the probability value. These are called
<em class="ltx_emph"><math id="S14.p3.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram</em> language models. The requirement for large amounts of
training data is even steeper in models like these than it is with
context-ignorant models, so I will only briefly study the use of such models in
conjunction with finite/state spelling correction. One has to be wary though,
when reading the terms in this chapter: I will commonly apply methods
of <math id="S14.p3.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram language models to make unigram language models of complex
languages work better, practically treating morphs as
components of a unigram model the way one would treat word forms for
<math id="S14.p3.m3" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram word models of English. I will commonly refer to the resulting
models as unigram while the underlying models are morph <math id="S14.p3.m4" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram models.</p>
</div>
<div id="S14.p4" class="ltx_para">
<p class="ltx_p">The main theme to follow for the rest of the chapter is the concept of workable
statistical models for morphologically complex languages with only small
amounts of the training data available. The improvements are provided as ways
to cleverly reuse the existing scarce data as much as possible while otherwise
sticking to the well-established algorithms of the field otherwise.</p>
</div>
</section>
<section id="S15" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">15 </span>Language Models for Languages with Compounding</h3>

<div id="S15.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Main article</span>: <em class="ltx_emph">Weighted Finite-State Morphological Analysis of
Finnish Inflection and Compounding</em> <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib1" title="Weighted finite-state morphological analysis of Finnish compounds with HFST-LEXC" class="ltx_ref">I</a>)</cite> by Krister
Lindén and Tommi Pirinen. This article introduced the statistical models of a
morphologically complex language with an infinite lexicon.</p>
</div>
<section id="S15.SS1" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">15.1 </span>Motivation</h4>

<div id="S15.SS1.p1" class="ltx_para">
<p class="ltx_p">The original <span class="ltx_text ltx_font_bold">motivation</span> for this research was to implement the
compound segmentation for Finnish given the ambiguous compounding of the
previous implementation <cite class="ltx_cite ltx_citemacro_citep">(Pirinen, <a href="#bib.bib93" title="Suomen kielen Ã¤Ã¤rellistilainen automaattinen morfologinen analyysi avoimen lÃ¤hdekoodin menetelmin" class="ltx_ref">2008</a>)</cite>. In the article, we
established that Finnish as a morphologically complex language requires special
statistical handling for its language models to be considered similar to
simpler statistical models for morphologically simpler languages.</p>
</div>
</section>
<section id="S15.SS2" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">15.2 </span>Related Works</h4>

<div id="S15.SS2.p1" class="ltx_para">
<p class="ltx_p">In <span class="ltx_text ltx_font_bold">related works</span>, the first approaches to the compound segmentation
problem that have been used are non-lexical, grapheme <math id="S15.SS2.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram
approaches <cite class="ltx_cite ltx_citemacro_citep">(Kokkinakis, <a href="#bib.bib45" title="A semantically annotated swedish medical corpus" class="ltx_ref">28-30</a>, referred to in)</cite> to Swedish. Later
solutions include systematically selecting the word forms with fewest compound
boundaries <cite class="ltx_cite ltx_citemacro_citep">(Karlsson, <a href="#bib.bib53" title="SWETWOL: a comprehensive morphological analyser for Swedish" class="ltx_ref">1992</a>)</cite>, and using statistics about
words for German <cite class="ltx_cite ltx_citemacro_citep">(Schiller, <a href="#bib.bib103" title="German compound analysis with wfsc" class="ltx_ref">2006</a>)</cite>. In the article we showed a combination with
both approaches, where the statistics are learned from the surface word forms
in running text, instead of a preanalysed and disambiguated corpus and a
manual selection routine. Showing that this method worked added two important
contributions to the language models of morphologically complex languages:
firstly, information can be derived from running unanalysed texts, which is
crucial, because the lack of good materials for most of the lesser-resourced
languages, and secondly, the application of basic statistical formulas to
morphologically complex languages, was an important generalisation.</p>
</div>
</section>
<section id="S15.SS3" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">15.3 </span>Results</h4>

<div id="S15.SS3.p1" class="ltx_para">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">results</span>, as stated in the motivation, are that the experiment
presented in the article was built on disambiguation of the compound
boundaries, rather than building language models for general applications. The
weighting mechanism however was applied to the language model of Finnish for
all the future research in a wider set of topics.</p>
</div>
<div id="S15.SS3.p2" class="ltx_para">
<p class="ltx_p">In the article, I study the precision and recall of the compound segmentation
of Finnish compounds using the statistical language modelling scheme. The
results shown in the article are rather promising; nearly all compound
segmentations are correctly found even with the baseline approaches of giving
rule-based weights to morphological complexity. There is a slight improvement
when using the statistically formed system that gives a higher granularity in a
few cases. There is one caveat in the material used in the paper, since
Finnish lacks a gold standard material for any analysed text corpora, we used a
commercial corpus made by another combination of automatic
analysers.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">29</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">29</sup><a href="http://www.csc.fi/english/research/software/ftc" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.csc.fi/english/research/software/ftc</a></span></span></span> This
means that the precision and recall values we obtained were gained by measuring
how closely we imitated the referent black box system, rather than real world
compound segmentations.</p>
</div>
</section>
</section>
<section id="S16" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">16 </span>The Statistical Training of Compounding Language Models</h3>

<div id="S16.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Main Article</span>: <em class="ltx_emph">Weighting Finite-State Morphological Analyzers
using HFST tools</em> by Krister Lindén and Tommi Pirinen. In this article we set out
to provide a generic formulation of statistical training of finite/state
language models regardless of complexity.</p>
</div>
<section id="S16.SS1" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">16.1 </span>Motivation</h4>

<div id="S16.SS1.p1" class="ltx_para">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">motivation</span> of this piece of research was to streamline and
generalise the process of creating the statistical finite/state language
models, as the experiment in the previous article
<cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib1" title="Weighted finite-state morphological analysis of Finnish compounds with HFST-LEXC" class="ltx_ref">I</a>)</cite> was constructed in a very <em class="ltx_emph">ad hoc</em>
manner. The intention of the new method for training language models is to get
all the existing language models trainable regardless of how they are
constructed; this makes the process of training a language model very similar
to what it has been in all non-finite/state software to date.</p>
</div>
</section>
<section id="S16.SS2" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">16.2 </span>Related Works</h4>

<div id="S16.SS2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Related works</span> constitute a whole field of literature of statistical
language models. The concept of having a good language model and using data
from corpora to train it provides the basis of any form of statistical natural
language engineering. For common established software in the field, see e.g.
SRILM <cite class="ltx_cite ltx_citemacro_citep">(Stolcke, <a href="#bib.bib111" title="SRILM-an extensible language modeling toolkit" class="ltx_ref">2002</a>)</cite>, and IRSTLM <cite class="ltx_cite ltx_citemacro_citep">(Federico<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib31" title="IRSTLM: an open source toolkit for handling large scale language models." class="ltx_ref">2008</a>)</cite> and so
forth. The existing work on training infinite dictionaries is more rare, as it
is not needed for English. For German, this is dealt with
by <cite class="ltx_cite ltx_citemacro_citet">Schiller (<a href="#bib.bib103" title="German compound analysis with wfsc" class="ltx_ref">2006</a>)</cite>, who also uses finite/state technology in its
implementation.</p>
</div>
</section>
<section id="S16.SS3" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">16.3 </span>Results</h4>

<div id="S16.SS3.p1" class="ltx_para">
<p class="ltx_p">As an initial <span class="ltx_text ltx_font_bold">result</span> in the article, I replicated the earlier compound
segmentation task from <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib1" title="Weighted finite-state morphological analysis of Finnish compounds with HFST-LEXC" class="ltx_ref">I</a>)</cite> to show that the generalised formula works.
Furthermore, I experimented with the traditional POS tagging task to see how
compound ambiguity actually affects the quality of POS tagging. The results
show an improvement which proves that statistical word-based compound
modelling is a useful part of a statistically trained morphological
analyser. This has the practical implication that it is indeed better to
use a compound-based model as a baseline statistical trained analyser
of a language, rather than a simple word-form-based statistical model.</p>
</div>
<div id="S16.SS3.p2" class="ltx_para">
<p class="ltx_p">In general, the unigram language model with the addition of word-form-based
training for compounds shows good promise for training
morphologically complex languages from the morphologically relevant parts
instead of from plain word forms. Based on these results I moved towards an
understanding that the statistical method of weighting morphologically complex
language models based on the word constituents of compound forms is a way
forward for statistically training morphologically complex language models.</p>
</div>
</section>
<section id="S16.SS4" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">16.4 </span>Future Research</h4>

<div id="S16.SS4.p1" class="ltx_para">
<p class="ltx_p">As conceivable <span class="ltx_text ltx_font_bold">future research</span>, there is a generalisation to this
method I have not yet experimented with. The constituents that I used for
Finnish were word forms, and this is based on the fact that Finnish compounds
are of a relatively simple form. The initial parts are the same as surface
forms of regular singular nominatives or genitives of any plurality and in some
cases any form. This is similar to some other languages, such as German and
Swedish. There are some compound word forms with a <em class="ltx_emph">compositive</em> marker
that is not available in isolated surface forms, and there are a few of those
in Finnish as well, e.g. words ending in a ‘-nen’ final derivation will
compound with the ‘-s’ form which is not seen outside compounds. In addition,
some archaic compositives exist for a few words, and some typically
non-compounding cases are attested as well. This is, however, not a productive
phenomenon and therefore it is handled in the lexicon by making those rare
forms as new entries in the dictionary. In languages like Greenlandic,
compounding does not heavily contribute to the complexity, though recurring
inflection and derivation does. The generalisation that might be needed is to
train the language model based on <em class="ltx_emph">morphs</em> instead of the uncompounded
word forms. I believe this could make statistics on languages of varying
morphological complexity more uniform, since the variation in the number of
morphs between languages is far more limited than the number of word forms.
There is a previous study on such language models based not on morphs, but on
letter <math id="S16.SS4.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-graphs, or statistically likely affixes,
by <cite class="ltx_cite ltx_citemacro_citet">Creutz<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib24" title="Morfessor and Hutmegs: unsupervised morpheme segmentation for highly-inflecting and compounding languages" class="ltx_ref">2005</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S17" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">17 </span>Weighting Hunspell as Finite-State Automata</h3>

<div id="S17.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Main Article</span>: <em class="ltx_emph">Creating and Weighting Hunspell Dictionaries as
Finite-State Automata</em> by Tommi A. Pirinen and Krister Lindén. In this article
we present weighted versions of finite/state Hunspell automata. This is also
the main article introducing for the first time the weighted finite/state
spell/checking on a larger scale.</p>
</div>
<section id="S17.SS1" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">17.1 </span>Motivation</h4>

<div id="S17.SS1.p1" class="ltx_para">
<p class="ltx_p">The main <span class="ltx_text ltx_font_bold">motivation</span> for this experiment was to work on extending the
previously verified results for weighting finite/state language models to
newly created formulations of Hunspell language models as finite/state
automata. The functionality of this approach could be counted as further
motivation for moving from a Hunspell’s non-finite-state approach to weighted
finite/state automata with the added expressive power and efficiency.</p>
</div>
</section>
<section id="S17.SS2" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">17.2 </span>Related Works</h4>

<div id="S17.SS2.p1" class="ltx_para">
<p class="ltx_p">In <span class="ltx_text ltx_font_bold">related works</span> describing probabilistic or otherwise preferential
language models for spell/checking, the main source for the current version
of context-insensitive spelling correction using statistical models comes
from <cite class="ltx_cite ltx_citemacro_citet">Church and Gale (<a href="#bib.bib22" title="Probability scoring for spelling correction" class="ltx_ref">1991</a>)</cite>.</p>
</div>
</section>
<section id="S17.SS3" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">17.3 </span>Results</h4>

<div id="S17.SS3.p1" class="ltx_para">
<p class="ltx_p">The main <span class="ltx_text ltx_font_bold">results</span> in the article were measured from a large range of
Hunspell languages for precision, showing improvement in quality across the
board with a simple unigram training method and the Hunspell automata. A
methodological detail about the evaluation should be noted; namely, that the
errors were constructed using automatic methods in the vein
of <cite class="ltx_cite ltx_citemacro_citep">(Bigert<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib15" title="AutoEval and Missplel: two generic tools for automatic evaluation" class="ltx_ref">2003</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citep">(Bigert, <a href="#bib.bib14" title="Automatic and unsupervised methods in natural language processing" class="ltx_ref">2005</a>)</cite>. In this setup,
a statistical approach to language models of spell/checking improves the
overall quality.</p>
</div>
<div id="S17.SS3.p2" class="ltx_para">
<p class="ltx_p">The implication of these results for the larger picture of this research is to
show that the finite/state formulation of Hunspell language models is a
reliable way forward for Hunspell spell-checkers, providing the additional
benefit of statistical language modelling to Hunspell’s well-engineered
rule-based models. This is important for practical development of the next
generation of spell/checking systems, since the creation of dictionaries and
language models requires some expert knowledge, which is not widely available
for most of the lesser-resourced languages.</p>
</div>
</section>
</section>
<section id="S18" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">18 </span>Lesser-Resourced Languages in Statistical Spelling Correction</h3>

<div id="S18.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Main Article</span>: <em class="ltx_emph">Finite-State Spell-Checking with Weighted Language
and Error Models</em> by Tommi A. Pirinen and Krister Lindén. In this article we
first researched the problem from the point of view of less-resourced
languages. Even though Finnish does not have significantly more freely usable
resources this is the first explicit mention of the problem in my research.
Furthermore, I used North Saami for the first time to show the limitedness of
freely-available open-source language resources for spell-checking for a real
lesser-resourced non-national language.</p>
</div>
<section id="S18.SS1" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">18.1 </span>Motivation</h4>

<div id="S18.SS1.p1" class="ltx_para">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">motivation</span> for this article was to write a publication that
explicitly explores the difficulties that morphologically complex and
lesser-resourced languages face when trying to pursue statistical language
models and corpus-based training. The article fits well into contemporary
research on natural language engineering, such research often considers
corpus-training as a kind of solution for all problems and ignores both the
scarcity of resources and the much higher requirement of unannotated resources
for morphologically complex languages.</p>
</div>
</section>
<section id="S18.SS2" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">18.2 </span>Related Works</h4>

<div id="S18.SS2.p1" class="ltx_para">
<p class="ltx_p">In <span class="ltx_text ltx_font_bold">related works</span>, the concept of lesser-resourced languages in
computational linguistics has come into focus in recent years. In particular, I
followed Anssi Yli-Jyrä’s recent work on African
languages <cite class="ltx_cite ltx_citemacro_citep">(Yli-Jyrä, <a href="#bib.bib121" title="Toward a widely usable finite-state morphology workbench for less studied languages, 1: desiderata" class="ltx_ref">2005</a>)</cite> and the work presented in the previous years at
the Speech and Language Technology for Minority Languages workshop, where I
delivered this paper. The concept of using Wikipedia as a basis for a
statistical language model was likewise a new common theme in the LREC
conferences.</p>
</div>
</section>
<section id="S18.SS3" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">18.3 </span>Results</h4>

<div id="S18.SS3.p1" class="ltx_para">
<p class="ltx_p">The most central <span class="ltx_text ltx_font_bold">results</span> of this work show that the Wikipedias for
small language communities will give some gains in spelling correction, when
properly coupled with a rule-based language model. This is despite the fact
that Wikipedias of lesser-resourced languages are comparably small and of not
very high quality, especially when related to the quality of correct spelling.
I have hypothesised that the underlying reason for this is that in events where
simple spell/checking like edit distance or confusion sets produce ambiguous
results, in the majority of the cases it is beneficial to simply opt for the
most common word form from these results.</p>
</div>
<div id="S18.SS3.p2" class="ltx_para">
<p class="ltx_p">As an additional result in this article, I show how to use language models to
automatically generate the baseline error models. This finding is expanded in
Section <a href="#thechapter-at-IDg" title="Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">\thechapter</span></a>.</p>
</div>
<div id="S18.SS3.p3" class="ltx_para">
<p class="ltx_p">In general, the concept of freely available and open/source, and
crowd/sourced, data should be a common focus in the research of computational
linguistics in lesser-resourced languages. This is one of the main sources of
large amounts of free data that most communities are willing to produce, and it
does not require advanced algorithms or an expert proofreader to make the data
usable for many of the applications of computational linguistics.</p>
</div>
</section>
</section>
<section id="S19" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">19 </span>Conclusions</h3>

<div id="S19.p1" class="ltx_para">
<p class="ltx_p">In this chapter, I have discussed different approaches to creating
statistical language models for finite/state spell/checking. I have
shown in this chapter that it is possible to take arbitrary language models for
morphologically complex languages and train them with the same approaches
regardless of the method and theory according to which the original model was
built. I have extended the basic word-form unigram training with methods from
<math id="S19.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram training to be able to cope with the infinite compounding of some
morphologically-complex languages and have thus laid out a path forward for all
morphologically-complex languages to be trained with limited corpora.</p>
</div>
</section>
</section>
<section id="thechapter-at-IDf" class="ltx_chapter">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter <span class="ltx_ERROR undefined">\thechapter</span> </span>Error Models</h2>

<div id="thechapter-at-IDf.p1" class="ltx_para">
<p class="ltx_p">The error modelling for spelling correction is an important part of a
spell/checking system and deserves to be treated as a separate topic.
The weighted finite/state formulation of <span class="ltx_ERROR undefined">\glspl</span>error model in a
finite/state spell/checking system in particular is a relatively novel
idea. There are some mentions of finite-state error modelling in the
literature <cite class="ltx_cite ltx_citemacro_citep">(Agirre<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib2" title="XUXEN: a spelling checker/corrector for Basque based on two-level morphology" class="ltx_ref">1992</a>; Van Noord and Gerdemann, <a href="#bib.bib114" title="An extendible regular expression compiler for finite-state approaches in natural language processing" class="ltx_ref">2001</a>; Savary, <a href="#bib.bib1a" title="Typographical nearest-neighbor search in a finite-state lexicon and its application to spelling correction" class="ltx_ref">2002</a>; Mohri, <a href="#bib.bib79" title="Edit-distance of weighted automata: general definitions and algorithms" class="ltx_ref">2003</a>)</cite>.
Error modelling, however, has been a central topic for non-finite-state
solutions to spell/checking,
cf. <cite class="ltx_cite ltx_citemacro_citet">Kukich (<a href="#bib.bib62" title="Spelling correction for the telecommunications network for the deaf" class="ltx_ref">1992</a>); Mitton (<a href="#bib.bib76" title="Ordering the suggestions of a spellchecker without using context" class="ltx_ref">2009</a>); Deorowicz and Ciura (<a href="#bib.bib27" title="Correcting spelling errors by modelling their causes" class="ltx_ref">2005</a>)</cite>, and
I believe that a pure and well-formed finite/state solution is called for.</p>
</div>
<div id="thechapter-at-IDf.p2" class="ltx_para">
<p class="ltx_p">One of the typical arguments for not using finite/state error models is that
specific non-finite-state algorithms are faster and more efficient than
building and especially applying finite/state models. In the articles,
where I construct finite/state models for spell/checking, I have run some
evaluations on the resulting system to prove that the finite/state system is
at usable speed and memory consumption levels. A thorough evaluation of this
is given in Chapter <a href="#thechapter-at-IDg" title="Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">\thechapter</span></a>.</p>
</div>
<div id="thechapter-at-IDf.p3" class="ltx_para">
<p class="ltx_p">The theoretical and practical motivation for suggesting finite/state models
for error correction is that the expressive power of a <span class="ltx_ERROR undefined">\gls</span>wfst is an
ideal format for specifying the combinations of the spelling errors that can be
made and corrected, and it is possible to build more complex statistical models
consisting of different types of errors by combining them from building blocks.
The fact that traditional non-finite-state spelling correctors use a number of
different algorithms starting from all the variants and modifications of the
edit distance to arbitrary context-based string rewriting to phonemic folding
schemes provides a good motivation to have such arbitrarily combinable systems
to create these error models outside the application development environment,
i.e. within end users’ systems.</p>
</div>
<div id="thechapter-at-IDf.p4" class="ltx_para">
<p class="ltx_p">The rest of this chapter is laid out as follows: In
Section <a href="#S20" title="20 The Application of Finite-State Error Models ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a>, I describe how the finite/state
application of error modelling is implemented in our spelling correction
system, and in Section <a href="#S21" title="21 Edit Distance Measures ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a>, I show how our variant of
finite/state edit distance style error models are constructed. In
Section <a href="#S22" title="22 Hunspell Error Correction ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22</span></a>, I go through the existing error models used in
the popular spelling software Hunspell, and show their finite/state
formulations. In Section <a href="#S23" title="23 Phonemic Key Corrections ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">23</span></a>, I show how to build and apply the
phonemic key type of error models in finite/state form. In
Section <a href="#S24" title="24 Context-Aware Spelling Correction ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24</span></a>, I discuss context-based language models in
finite/state spelling correction for morphologically complex languages with
limited resources that is, mostly the limitations and necessary modifications.
Finally in Section <a href="#S25" title="25 Other Finite-State Error Models ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">25</span></a>, I discuss some other error correcting
models and their finite/state formulations.</p>
</div>
<section id="S20" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">20 </span>The Application of Finite-State Error Models</h3>

<div id="S20.p1" class="ltx_para">
<p class="ltx_p">There are many approaches to spelling correction with finite/state automata.
It is possible to use various generic graph algorithms to work on
error-tolerant traversal of the finite/state graph, as is done
by <cite class="ltx_cite ltx_citemacro_citet">HuldÃ©n (<a href="#bib.bib49" title="Fast approximate string matching with finite automata" class="ltx_ref">2009</a>)</cite>. It is also possible to create a finite/state
network that already includes potential error mapping, as is shown
by <cite class="ltx_cite ltx_citemacro_citet">Schulz and Mihov (<a href="#bib.bib105" title="Fast string correction with Levenshtein-automata" class="ltx_ref">2002</a>)</cite>. The approach that I use is to apply basic
finite/state algebra with a transducer working as the error model. This is a
slight variation of a model originally presented by <cite class="ltx_cite ltx_citemacro_citet">Mohri (<a href="#bib.bib79" title="Edit-distance of weighted automata: general definitions and algorithms" class="ltx_ref">2003</a>)</cite>. We
remove the errors from misspelled strings by two compositions, performed
serially in a single operation: <math id="S20.p1.m1" class="ltx_Math" alttext="\mathcal{M}_{\mathrm{suggestions}}=(\mathcal{M}_{\mathrm{wordform}}\circ%
\mathcal{M}_{\mathrm{error}}\circ\mathcal{M}_{\mathrm{language}})_{2}" display="inline"><mrow><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>suggestions</mi></msub><mo>=</mo><msub><mrow><mo stretchy="false">(</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>wordform</mi></msub><mo>∘</mo><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>error</mi></msub><mo>∘</mo><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>language</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msub></mrow></math>, where <math id="S20.p1.m2" class="ltx_Math" alttext="\mathcal{M}_{\mathrm{suggestions}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>suggestions</mi></msub></math>
is automaton encoding suggestions, <math id="S20.p1.m3" class="ltx_Math" alttext="(\mathcal{M}_{\mathrm{wordform}}" display="inline"><mrow><mo stretchy="false">(</mo><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>wordform</mi></msub></mrow></math> is
automaton containing misspelt word form, <math id="S20.p1.m4" class="ltx_Math" alttext="\mathcal{M}_{\mathrm{E}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi mathvariant="normal">E</mi></msub></math> is error
model, and <math id="S20.p1.m5" class="ltx_Math" alttext="\mathcal{M}_{\mathrm{L}})" display="inline"><mrow><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi mathvariant="normal">L</mi></msub><mo stretchy="false">)</mo></mrow></math> is language model. This specific form of
finite/state error correction was introduced
in <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib4a" title="Finite-state spell-checking with weighted language and error models" class="ltx_ref">III</a>)</cite>, but the underlying technology and
optimisations were more widely documented by <cite class="ltx_cite ltx_citemacro_citet">LindÃ©n<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib47" title="Hfstâframework for compiling and applying morphologies" class="ltx_ref">2011</a>)</cite>.</p>
</div>
<div id="S20.p2" class="ltx_para">
<p class="ltx_p">The implications of using weighted finite/state algebra are two-fold. On the
one hand, we have the full expressive power of the weighted finite/state
systems. On the other hand, the specialised algorithms are known to be faster,
e.g. the finite/state traversal shown by <cite class="ltx_cite ltx_citemacro_citet">HuldÃ©n (<a href="#bib.bib49" title="Fast approximate string matching with finite automata" class="ltx_ref">2009</a>)</cite> is both fast
and has the option of having regular languages as contexts, but no true
weights. So there is a trade-off to consider when choosing between these
methods, but considering the popularity of statistical approaches in the field
of natural language engineering the time trade-off may be palatable for most
end-users developing language models.</p>
</div>
<div id="S20.p3" class="ltx_para">
<p class="ltx_p">The implication of the fully weighted finite/state framework for
spell/checking is that we can e.g. apply statistics to all parts of the
process, i.e., the probability of the input, the conditional probability of a
specific combination of errors and the probability of the resulting word form
in context.</p>
</div>
</section>
<section id="S21" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">21 </span>Edit Distance Measures</h3>

<div id="S21.p1" class="ltx_para">
<p class="ltx_p">The baseline error model for spelling correction since its inception in the
1960s has been the Damerau-Levenshtein edit
distance <cite class="ltx_cite ltx_citemacro_citep">(Damerau, <a href="#bib.bib26" title="A technique for computer detection and correction of spelling errors" class="ltx_ref">1964</a>; Levenshtein, <a href="#bib.bib65" title="Binary codes capable of correcting deletions, insertions, and reversals" class="ltx_ref">1966</a>)</cite>. There have been
multiple formulations of the finite/state edit distance, but in this section
we describe the one used by our systems. This formulation of the edit distance
as a transducer was first presented by <cite class="ltx_cite ltx_citemacro_citet">Schulz and Mihov (<a href="#bib.bib105" title="Fast string correction with Levenshtein-automata" class="ltx_ref">2002</a>)</cite>.</p>
</div>
<div id="S21.p2" class="ltx_para">
<p class="ltx_p">Finite-state edit distance is a rather simple concept; each of the error types
except swapping is represented by a single transition of the form <math id="S21.p2.m1" class="ltx_Math" alttext="\varepsilon:x" display="inline"><mrow><mi>ε</mi><mo>:</mo><mi>x</mi></mrow></math> (for
insertion), <math id="S21.p2.m2" class="ltx_Math" alttext="x:\varepsilon" display="inline"><mrow><mi>x</mi><mo>:</mo><mi>ε</mi></mrow></math> (for deletion) or <math id="S21.p2.m3" class="ltx_Math" alttext="x:y" display="inline"><mrow><mi>x</mi><mo>:</mo><mi>y</mi></mrow></math> (for change). Swaps
requires an additional state in the automaton as a memory for the symbols to be
swapped at the price of one edit that is a path <math id="S21.p2.m4" class="ltx_Math" alttext="\pi_{x:yy:x}" display="inline"><msub><mi>π</mi><mrow><mi>x</mi><mo>:</mo><mrow><mi>y</mi><mo>⁢</mo><mi>y</mi></mrow><mo>:</mo><mi>x</mi></mrow></msub></math>. This
unweighted edit distance 1 error model is shown in Figure <a href="#S21.F2" title="Figure 2 ‣ 21 Edit Distance Measures ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S21.F2" class="ltx_figure"><img src="" id="S21.F2.g1" class="ltx_graphics" alt="">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A non-deterministic unweighted transducer implementing edit
distance 1, where <math id="S21.F2.m6" class="ltx_Math" alttext="\Sigma={\mathrm{a},\mathrm{b},\varepsilon}" display="inline"><mrow><mi mathvariant="normal">Σ</mi><mo>=</mo><mrow><mi mathvariant="normal">a</mi><mo>,</mo><mi mathvariant="normal">b</mi><mo>,</mo><mi>ε</mi></mrow></mrow></math>. The
deletion, addition and change is shown in the edits from <math id="S21.F2.m7" class="ltx_Math" alttext="q_{0}" display="inline"><msub><mi>q</mi><mn>0</mn></msub></math> to
<math id="S21.F2.m8" class="ltx_Math" alttext="q_{1}" display="inline"><msub><mi>q</mi><mn>1</mn></msub></math>, and the states <math id="S21.F2.m9" class="ltx_Math" alttext="q_{2}" display="inline"><msub><mi>q</mi><mn>2</mn></msub></math> and <math id="S21.F2.m10" class="ltx_Math" alttext="q_{3}" display="inline"><msub><mi>q</mi><mn>3</mn></msub></math> are used as a memory for the
swaps of <em class="ltx_emph">ab</em> to <em class="ltx_emph">ba</em> and <em class="ltx_emph">ba</em> to <em class="ltx_emph">ab</em> respectively.
</figcaption>
</figure>
<div id="S21.p3" class="ltx_para">
<p class="ltx_p">There are numerous possibilities to formalise the edit-distance in weighted
finite/state automata. The simplest form is a cyclic automaton that is
capable of measuring arbitrary distances, using the weights as a distance
measure. This weighted automaton for measuring edit distances is shown in
Figure <a href="#S21.F3" title="Figure 3 ‣ 21 Edit Distance Measures ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. This form is created by drawing arcs of the
form <math id="S21.p3.m1" class="ltx_Math" alttext="\varepsilon:x::w" display="inline"><mrow><mi>ε</mi><mo>:</mo><mi>x</mi><mo>:</mo><mo>:</mo><mi>w</mi></mrow></math>, <math id="S21.p3.m2" class="ltx_Math" alttext="x:\varepsilon::w" display="inline"><mrow><mi>x</mi><mo>:</mo><mi>ε</mi><mo>:</mo><mo>:</mo><mi>w</mi></mrow></math> and <math id="S21.p3.m3" class="ltx_Math" alttext="x:y::w" display="inline"><mrow><mi>x</mi><mo>:</mo><mi>y</mi><mo>:</mo><mo>:</mo><mi>w</mi></mrow></math>, from the initial
state to itself, where <math id="S21.p3.m4" class="ltx_Math" alttext="w" display="inline"><mi>w</mi></math> is the weight of a given operation. If the measure
includes the swap of adjacent characters, the extra states and paths of the
form <math id="S21.p3.m5" class="ltx_Math" alttext="\pi_{x:yy:x::w}" display="inline"><msub><mi>π</mi><mrow><mrow><mi>x</mi><mo>:</mo><mrow><mi>y</mi><mo>⁢</mo><mi>y</mi></mrow><mo>:</mo><mi>x</mi></mrow><mo>⁣</mo><mo>:</mo><mo>⁣</mo><mrow><mi></mi><mo>:</mo><mi>w</mi></mrow></mrow></msub></math> need to be defined. It is possible to use this
automaton to help automatic harvesting or in training of an error model. The
practical weighted finite/state error models used in real applications are of
the same form as the unweighted ones defined earlier, with limited maximum
distance, as the application of an infinite edit measure is expectedly slow.</p>
</div>
<figure id="S21.F3" class="ltx_figure"><img src="" id="S21.F3.g1" class="ltx_graphics" alt="">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>A Weighted automaton for measuring arbitrary edit distance where
<math id="S21.F3.m6" class="ltx_Math" alttext="\Sigma={\mathrm{a},\mathrm{b},\varepsilon}" display="inline"><mrow><mi mathvariant="normal">Σ</mi><mo>=</mo><mrow><mi mathvariant="normal">a</mi><mo>,</mo><mi mathvariant="normal">b</mi><mo>,</mo><mi>ε</mi></mrow></mrow></math> and uniform weights of <math id="S21.F3.m7" class="ltx_Math" alttext="1" display="inline"><mn>1</mn></math> per
edit. The states <math id="S21.F3.m8" class="ltx_Math" alttext="q_{1}" display="inline"><msub><mi>q</mi><mn>1</mn></msub></math> and <math id="S21.F3.m9" class="ltx_Math" alttext="q_{2}" display="inline"><msub><mi>q</mi><mn>2</mn></msub></math> are used as a memory for
the swaps of <em class="ltx_emph">ab</em> to <em class="ltx_emph">ba</em> and <em class="ltx_emph">ba</em> to <em class="ltx_emph">ab</em> to
keep them at the weight of <math id="S21.F3.m10" class="ltx_Math" alttext="1" display="inline"><mn>1</mn></math>. </figcaption>
</figure>
<div id="S21.p4" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Mohri (<a href="#bib.bib79" title="Edit-distance of weighted automata: general definitions and algorithms" class="ltx_ref">2003</a>)</cite> describes the mathematical background of implementing a
weighted edit distance measure for finite/state automata. It differs slightly
from the formulation we use in that it describes edit distance of two weighted
automata exactly, whereas our definition is suitable for measuring the edit
distance of a string or path automaton and the target language model. The
practical differences between Mohri’s formulations and the automata I present
seem to be minimal, e.g. placing weights into the final state instead
of on the arcs.</p>
</div>
</section>
<section id="S22" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">22 </span>Hunspell Error Correction</h3>

<div id="S22.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Main Article</span>: <em class="ltx_emph">Building and Using Existing Hunspell Dictionaries
and TeX Hyphenators as Finite-State Automata</em> by Tommi A. Pirinen and Krister
Lindén. In this article we have formulated the Hunspell’s string-algorithm
algorithms for error correction and their specific optimisations in terms
of finite/state automata.</p>
</div>
<section id="S22.SS1" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">22.1 </span>Motivation</h4>

<div id="S22.SS1.p1" class="ltx_para">
<p class="ltx_p">The main <span class="ltx_text ltx_font_bold">motivation</span> for this research was to reimplement Hunspell’s
spell/checking in a finite/state form. In terms of error modelling it
basically consists of modifications of the edit distance for speed gains. One
modification is to take the keyboard layout into account when considering the
replacement type of errors. Another addition is to sort and limit the alphabet
used for other error types. Finally, Hunspell allows the writer of the
spell/checking dictionary to define specific string-to-string mutations for
common errors.</p>
</div>
</section>
<section id="S22.SS2" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">22.2 </span>Results</h4>

<div id="S22.SS2.p1" class="ltx_para">
<p class="ltx_p">The finite/state formulation of these errors is a combination of limitations
and modifications to the edit distance automaton, disjuncted with a simple
string-to-string mapping automaton. The key aspect of Hunspell’s modified edit
distance formulations are optimising the speed of the error lookup using
configurable, language specific, limitations of the edit distance algorithm’s
search space. The other correctional facilities are mainly meant to cover the
type of errors that cannot be reached with regular edit distance modifications.
The effects of these optimisations are further detailed in
Chapter <a href="#thechapter-at-IDg" title="Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">\thechapter</span></a>.</p>
</div>
<div id="S22.SS2.p2" class="ltx_para">
<p class="ltx_p">The settings that Hunspell gives to a user are the following: A <span class="ltx_text ltx_font_typewriter">KEY</span>
setting to limit characters that can be <em class="ltx_emph">replaced</em> in the edit distance
algorithm to optimise the replacements to adjacent keys on a typical native
language keyboard. A <span class="ltx_text ltx_font_typewriter">TRY</span> setting to limit the possible characters
that are used in the insertion and deletion part of the edit distance
algorithm. A <span class="ltx_text ltx_font_typewriter">REP</span> setting that can be used for arbitrary
string-to-string confusion sets, and a parallel setting called <span class="ltx_text ltx_font_typewriter">MAP</span>
used for encoding differences rather than linguistic confusions. Finally, for
the English language, there is a <span class="ltx_text ltx_font_typewriter">PHONE</span> setting that implements a
variation of the double metaphone algorithm <cite class="ltx_cite ltx_citemacro_citep">(Philips, <a href="#bib.bib91" title="The double metaphone search algorithm" class="ltx_ref">2000</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S23" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">23 </span>Phonemic Key Corrections</h3>

<div id="S23.p1" class="ltx_para">
<p class="ltx_p">Phonemic keys are commonly used for languages where orthography does not have a
very obvious mapping to pronunciation, such as English. In the phonemic keying
methods the strings of a language are intended to connect with a phonemic or
phonemically motivated description. This technique was originally used for
cataloguing family names in archives <cite class="ltx_cite ltx_citemacro_citep">(Russell and Odell, <a href="#bib.bib102" title="Soundex" class="ltx_ref">1918</a>)</cite>. These systems
have been successfully adapted to spell/checking and are in use for English,
for example in aspell as the double metaphone
algorithm <cite class="ltx_cite ltx_citemacro_citep">(Philips, <a href="#bib.bib91" title="The double metaphone search algorithm" class="ltx_ref">2000</a>)</cite>. Since these algorithms are a mapping from
strings to strings, it is trivially modifiable into a finite/state spelling
correction model. Considering a finite/state automaton <math id="S23.p1.m1" class="ltx_Math" alttext="\mathcal{M}_{phon}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mrow><mi>p</mi><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>n</mi></mrow></msub></math>
that maps all strings of a language into a single string called a phonetic key,
the spelling correction can be performed with mapping <math id="S23.p1.m2" class="ltx_Math" alttext="\mathcal{M}_{phon}\circ\mathcal{M}_{phon}^{-1}" display="inline"><mrow><msub><mi class="ltx_font_mathcaligraphic">ℳ</mi><mrow><mi>p</mi><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>n</mi></mrow></msub><mo>∘</mo><msubsup><mi class="ltx_font_mathcaligraphic">ℳ</mi><mrow><mi>p</mi><mo>⁢</mo><mi>h</mi><mo>⁢</mo><mi>o</mi><mo>⁢</mo><mi>n</mi></mrow><mrow><mo>-</mo><mn>1</mn></mrow></msubsup></mrow></math>. For example, a soundex automaton can map strings
like <span class="ltx_ERROR undefined">\markoverwith</span><span class="ltx_ERROR undefined">\textcolor</span>red<span class="ltx_text" style="position:relative; bottom:-3.5pt;"><span class="ltx_ERROR undefined">\sixly</span>:</span><span class="ltx_ERROR undefined">\ULon</span>squer and <em class="ltx_emph">square</em> into string <span class="ltx_text ltx_font_typewriter">S140</span>, and the
composition of S140 to the inverse soundex mapping would produce, among
others, <em class="ltx_emph">square</em>, <span class="ltx_ERROR undefined">\markoverwith</span><span class="ltx_ERROR undefined">\textcolor</span>red<span class="ltx_text" style="position:relative; bottom:-3.5pt;"><span class="ltx_ERROR undefined">\sixly</span>:</span><span class="ltx_ERROR undefined">\ULon</span>squer, <span class="ltx_ERROR undefined">\markoverwith</span><span class="ltx_ERROR undefined">\textcolor</span>red<span class="ltx_text" style="position:relative; bottom:-3.5pt;"><span class="ltx_ERROR undefined">\sixly</span>:</span><span class="ltx_ERROR undefined">\ULon</span>sqr, <span class="ltx_ERROR undefined">\markoverwith</span><span class="ltx_ERROR undefined">\textcolor</span>red<span class="ltx_text" style="position:relative; bottom:-3.5pt;"><span class="ltx_ERROR undefined">\sixly</span>:</span><span class="ltx_ERROR undefined">\ULon</span>sqrrr, and
so forth. And this set can be applied to the language model collecting only the
good suggestions, such as <em class="ltx_emph">square</em>.</p>
</div>
</section>
<section id="S24" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">24 </span>Context-Aware Spelling Correction</h3>

<div id="S24.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Main article:</span> <em class="ltx_emph">Improving finite/state spell-checker suggestions
with part of speech <math id="S24.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams</em> by Tommi A. Pirinen, Miikka Silfverberg, and
Krister Lindén. In this article I have attempted to cover spelling
correction of morphologically more complex languages with finite/state
technology.</p>
</div>
<section id="S24.SS1" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">24.1 </span>Motivation</h4>

<div id="S24.SS1.p1" class="ltx_para">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">motivation</span> for this article was to show that a finite/state
version of spell/checking is also usable for a context-aware form, and
secondarily to explore the limitations and required changes for the
context-based language models that are needed for more morphologically complex
languages to get similar results as are attained with simple statistical
context models for morphologically poor languages.</p>
</div>
</section>
<section id="S24.SS2" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">24.2 </span>Related Works</h4>

<div id="S24.SS2.p1" class="ltx_para">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">related works</span> include the well-known results on English for the
same task implemented in non-finite-state approaches. In general, the baseline
of the context-based spelling correction was probably established
by <cite class="ltx_cite ltx_citemacro_citet">Mays<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib73" title="Context based spelling correction" class="ltx_ref">1991</a>)</cite> and been revisited many times, e.g.
by <cite class="ltx_cite ltx_citemacro_citet">Wilcox-O’Hearn<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib118" title="Real-word spelling correction with trigrams: a reconsideration of the Mays, Damerau, and Mercer model" class="ltx_ref">2008</a>)</cite>, and has usually been found to be
beneficial for the languages that have been studied. The examples I have found
are all of the morphologically poor languages such as English or Spanish. In my
research I have tried to replicate these results for Finnish, but the results
have not been as successful as with English or Spanish . The basic word form
<math id="S24.SS2.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams were not seen as an efficient method for Spanish by
e.g. <cite class="ltx_cite ltx_citemacro_citet">Otero<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib86" title="Contextual spelling correction" class="ltx_ref">2007</a>)</cite>, instead the method was extended by studying
also the part-of-speech analyses to rank the suggestion lists for a
spell-checker. In my article <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib9a" title="Improving finite-state spell-checker suggestions with part of speech n-grams" class="ltx_ref">VIII</a>)</cite>, we
reimplemented a finite/state system very similar to the one used for Spanish,
originally unaware of these parallel results.</p>
</div>
</section>
<section id="S24.SS3" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">24.3 </span>Results</h4>

<div id="S24.SS3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Results</span> – given in Table <a href="#S24.T4" title="Table 4 ‣ 24.3 Results ‣ 24 Context-Aware Spelling Correction ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> reproduced
from <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib9a" title="Improving finite-state spell-checker suggestions with part of speech n-grams" class="ltx_ref">VIII</a>)</cite> – show that the regular <math id="S24.SS3.p1.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams,
which are used for English to successfully improve spelling correction, are not
so effective for Finnish. Furthermore, the improvement that is gained for
Spanish when applying the POS <math id="S24.SS3.p1.m2" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-grams in this task does not give equally
promising results for Finnish, but do improve the quality of suggestions, by
around 5% points.</p>
</div>
<figure id="S24.T4" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">Algorithm</td>
<td class="ltx_td ltx_align_right ltx_border_t">1</td>
<td class="ltx_td ltx_align_right ltx_border_t">2</td>
<td class="ltx_td ltx_align_right ltx_border_t">3</td>
<td class="ltx_td ltx_align_right ltx_border_t">4</td>
<td class="ltx_td ltx_align_right ltx_border_t">5</td>
<td class="ltx_td ltx_align_right ltx_border_t">1—10</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="7"><span class="ltx_text ltx_font_bold">English</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">Edit distance 2 (baseline)</td>
<td class="ltx_td ltx_align_right ltx_border_t">25.9%</td>
<td class="ltx_td ltx_align_right ltx_border_t">2.4%</td>
<td class="ltx_td ltx_align_right ltx_border_t">2.4%</td>
<td class="ltx_td ltx_align_right ltx_border_t">1.2%</td>
<td class="ltx_td ltx_align_right ltx_border_t">3.5%</td>
<td class="ltx_td ltx_align_right ltx_border_t">94.1%</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">Edit distance 2 with Unigrams</td>
<td class="ltx_td ltx_align_right ltx_border_t">28.2%</td>
<td class="ltx_td ltx_align_right ltx_border_t">5.9%</td>
<td class="ltx_td ltx_align_right ltx_border_t">29.4%</td>
<td class="ltx_td ltx_align_right ltx_border_t">3.5%</td>
<td class="ltx_td ltx_align_right ltx_border_t">28.2%</td>
<td class="ltx_td ltx_align_right ltx_border_t">97.6%</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">Edit distance 2 with Word N-grams</td>
<td class="ltx_td ltx_align_right ltx_border_t">29.4%</td>
<td class="ltx_td ltx_align_right ltx_border_t">10.6%</td>
<td class="ltx_td ltx_align_right ltx_border_t">34.1%</td>
<td class="ltx_td ltx_align_right ltx_border_t">5.9%</td>
<td class="ltx_td ltx_align_right ltx_border_t">14.1%</td>
<td class="ltx_td ltx_align_right ltx_border_t">97.7%</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">Edit distance 2 with POS N-grams</td>
<td class="ltx_td ltx_align_right ltx_border_t">68.2%</td>
<td class="ltx_td ltx_align_right ltx_border_t">18.8%</td>
<td class="ltx_td ltx_align_right ltx_border_t">3.5%</td>
<td class="ltx_td ltx_align_right ltx_border_t">2.4%</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.0%</td>
<td class="ltx_td ltx_align_right ltx_border_t">92.9%</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="7"><span class="ltx_text ltx_font_bold">Finnish</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">Edit distance 2 (baseline)</td>
<td class="ltx_td ltx_align_right ltx_border_t">66.5%</td>
<td class="ltx_td ltx_align_right ltx_border_t">8.7%</td>
<td class="ltx_td ltx_align_right ltx_border_t">4.0%</td>
<td class="ltx_td ltx_align_right ltx_border_t">4.7%</td>
<td class="ltx_td ltx_align_right ltx_border_t">1.9%</td>
<td class="ltx_td ltx_align_right ltx_border_t">89.8%</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">Edit distance 2 with Unigrams</td>
<td class="ltx_td ltx_align_right ltx_border_t">61.2%</td>
<td class="ltx_td ltx_align_right ltx_border_t">13.4%</td>
<td class="ltx_td ltx_align_right ltx_border_t">1.6%</td>
<td class="ltx_td ltx_align_right ltx_border_t">3.1%</td>
<td class="ltx_td ltx_align_right ltx_border_t">3.4%</td>
<td class="ltx_td ltx_align_right ltx_border_t">88.2%</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">Edit distance 2 with Word N-grams</td>
<td class="ltx_td ltx_align_right ltx_border_t">65.0%</td>
<td class="ltx_td ltx_align_right ltx_border_t">14.4%</td>
<td class="ltx_td ltx_align_right ltx_border_t">3.8%</td>
<td class="ltx_td ltx_align_right ltx_border_t">3.1%</td>
<td class="ltx_td ltx_align_right ltx_border_t">2.2%</td>
<td class="ltx_td ltx_align_right ltx_border_t">90.6%</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t">Edit distance 2 with POS N-grams</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_t">71.4%</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_t">9.3%</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_t">1.2%</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_t">3.4%</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_t">0.3%</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_t">85.7%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Precision of suggestion algorithms with real spelling errors.
Reproduced from <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib9a" title="Improving finite-state spell-checker suggestions with part of speech n-grams" class="ltx_ref">VIII</a>)</cite>
</figcaption>
</figure>
<div id="S24.SS3.p2" class="ltx_para">
<p class="ltx_p">One of the negative results of the paper is that the time and memory
consumption of the POS-based or word-form-based spelling correction is not yet
justifiable for general consumer products as a good tradeoff. There have been
many recent articles exploring the optimisation side of the context-based
spell/checking that should be seen as the future work for context-based
finite/state spelling correction. The optimisation of the <math id="S24.SS3.p2.m1" class="ltx_Math" alttext="n" display="inline"><mi>n</mi></math>-gram models
has been brought up many times in non-finite/state solutions too,
e.g. <cite class="ltx_cite ltx_citemacro_citet">Church<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib23" title="Compressing trigram language models with Golomb coding" class="ltx_ref">2007</a>)</cite>. While mainly an engineering problem, a lot
is left to be desired from this development before practical end-user
applications for contextual spell/checking can be considered.</p>
</div>
</section>
</section>
<section id="S25" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">25 </span>Other Finite-State Error Models</h3>

<div id="S25.p1" class="ltx_para">
<p class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Deorowicz and Ciura (<a href="#bib.bib27" title="Correcting spelling errors by modelling their causes" class="ltx_ref">2005</a>)</cite> present an extensive study on error
modelling for spelling correction. Some of the models have been
introduced earlier in this chapter.
</p>
</div>
<div id="S25.p2" class="ltx_para">
<p class="ltx_p">One of the optimisations used in some practical spell/checking systems,
but not in Hunspell, is to avoid modifying the first character of the word in
the error modelling step. The gained improvement in speed is notable. The
rationale for this is the assumption that it is rarer to make mistakes in the
first character of the word – or perhaps it is easier to notice and fix such
mistakes. There are some measurements on this phenomenon
by <cite class="ltx_cite ltx_citemacro_citet">Bhagat (<a href="#bib.bib13" title="Spelling error pattern analysis of Punjabi typed text" class="ltx_ref">2007</a>)</cite>, but the probabilities of having the first-letter
error in the studies they refer to vary from 1% up to 15%, so it is still
open to debate. In measurements I made for morphologically complex languages
in <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib9a" title="Improving finite-state spell-checker suggestions with part of speech n-grams" class="ltx_ref">VIII</a>)</cite>, I also found a slight tendency towards
the effect that the errors are more likely in non-initial parts of the words,
and these account for word-length effects.</p>
</div>
<div id="S25.p3" class="ltx_para">
<p class="ltx_p">The concept of creating an error model from a (possibly annotated) error corpus
is plausible for finite/state spelling-correction as well. The modification
of the training logic as presented by <cite class="ltx_cite ltx_citemacro_citet">Church and Gale (<a href="#bib.bib22" title="Probability scoring for spelling correction" class="ltx_ref">1991</a>)</cite> into
finite/state form is mostly trivial. E.g. if we consider the building of a
finite/state language model from a corpus of word forms, the corpus of errors
is the same set of string pairs with weights of</p>
</div>
<div id="S25.p4" class="ltx_para">
<table id="S25.E3" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S25.E3.m1" class="ltx_Math" alttext="w(x:y)=-\log\frac{f(x:y)+\alpha}{\mathcal{D}_{\mathrm{errors}}+\alpha}," display="block"><mrow><mi>w</mi><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>:</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo>=</mo><mo>-</mo><mi>log</mi><mfrac><mrow><mi>f</mi><mrow><mo stretchy="false">(</mo><mi>x</mi><mo>:</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mo>+</mo><mi>α</mi></mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>errors</mi></msub><mo>+</mo><mi>α</mi></mrow></mfrac><mo>,</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr>
</table>
</div>
<div id="S25.p5" class="ltx_para ltx_noindent">
<p class="ltx_p">where <math id="S25.p5.m1" class="ltx_Math" alttext="x,y\subset\Sigma^{\star}" display="inline"><mrow><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow><mo>⊂</mo><msup><mi mathvariant="normal">Σ</mi><mo>⋆</mo></msup></mrow></math> are error string pairs <math id="S25.p5.m2" class="ltx_Math" alttext="x:y" display="inline"><mrow><mi>x</mi><mo>:</mo><mi>y</mi></mrow></math> in a collection
of all errors <math id="S25.p5.m3" class="ltx_Math" alttext="\mathcal{D}_{\mathrm{errors}}" display="inline"><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>errors</mi></msub></math>.</p>
</div>
</section>
<section id="S26" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">26 </span>Conclusions</h3>

<div id="S26.p1" class="ltx_para">
<p class="ltx_p">In this chapter, I have enumerated some of the most important finite-state
formulations of common error models found in spell-checkers and shown that they
are implementable and combinable under basic finite-state algebra. I have
re-implemented a full set of error models used by the current <em class="ltx_emph">de facto</em>
standard spelling corrector Hunspell in a finite state form demonstrating that
it is possible to replace string-algorithm correction algorithms by a sound
finite-state error model.</p>
</div>
</section>
</section>
<section id="thechapter-at-IDg" class="ltx_chapter">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter <span class="ltx_ERROR undefined">\thechapter</span> </span>Efficiency of Finite-State Spell-Checking</h2>

<div id="thechapter-at-IDg.p1" class="ltx_para">
<p class="ltx_p">One of the motivations for finite/state technology is that the time
complexity of the application of finite/state automata is known to be linear
with regard to the size of the input as long as finite/state automata can be
optimised by operations like determinisation. In practice, however, the speed
and size requirements of the finite/state automata required for language
models, error models and their combinations are not easy to predict, and the
worst case scenarios of these specific combinatorics are still theoretically
exponential, because the error models and language models are kept separated
and composed during run time due to space restrictions. This is especially
important for error modelling in spell/checking, since it easily goes towards
the worst case scenario, i.e. the theoretically ideal naive error model that
can rewrite any string into any other string with some probabilities attached
is not easily computable.</p>
</div>
<div id="thechapter-at-IDg.p2" class="ltx_para">
<p class="ltx_p">Towards the final parts of my research on finite/state spell/checking, I
set out to write a few larger-scale evaluation articles to demonstrate the
efficiency and usability of finite/state spell/checking. For larger scale
testing, I also sought to show how the linguistically common but
technologically almost undealt with concept of <span class="ltx_ERROR undefined">\gls</span>morphological complexity
practically relates to the finite/state language models.</p>
</div>
<div id="thechapter-at-IDg.p3" class="ltx_para">
<p class="ltx_p">The evaluation of finite/state spelling correction needs to be contrasted
with the existing comparisons and evaluations of spell/checking systems.
Luckily, spell/checking has been a popular enough topic so that a number of
recent evaluations are available. Also the informative introduction to
spell/checking by <cite class="ltx_cite ltx_citemacro_citet">Norvig (<a href="#bib.bib83" title="How to write a spelling corrector" class="ltx_ref">2010</a>)</cite> has been a valuable source of
practical frameworks for a test setup of the evaluation of spelling checkers as
well as the ultimate baseline comparison for English language spell/checking.</p>
</div>
<div id="thechapter-at-IDg.p4" class="ltx_para">
<p class="ltx_p">The evaluation of finite/state spelling checkers in this chapter consists of
two different goals: when working with English, we merely aim to reproduce the
results that are published in the literature and are well known. When working
with more <span class="ltx_ERROR undefined">\glslink</span>morphological complexitymorphologically complex
languages, we aim to show that the more complex the languages we select, the
more work and optimisations are needed to attain reasonable performance for
baseline spell/checking and correction. Throughout the chapter, we have
tried to follow the linguistically motivated selection of languages, i.e.
selecting at least one from moderately rich and one from poly-synthetic
languages; suitable languages for these purposes with open-source, freely
available implementations have been acquired from the University of Tromsø’s
server.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">30</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">30</sup><a href="http://giellatekno.uit.no" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://giellatekno.uit.no</a></span></span></span> From there we have selected
North Saami as a moderately rich and Greenlandic as an example of a
poly-synthetic language.</p>
</div>
<div id="thechapter-at-IDg.p5" class="ltx_para">
<p class="ltx_p">The efficiency evaluation is divided into two sections. In
Section <a href="#S27" title="27 Speed of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">27</span></a>, I evaluate the speed efficiency and optimisation
schemes of finite/state spelling checkers together with Sam Hardwick who did
most of the groundwork on the finite/state spell/checking implementation in
the article by e.g., <cite class="ltx_cite ltx_citemacro_citet">LindÃ©n<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib47" title="Hfstâframework for compiling and applying morphologies" class="ltx_ref">2011</a>)</cite>. Then in
Section <a href="#S28" title="28 Precision of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">28</span></a>, I measure the effects these optimisations have on
the quality of spell/checking.</p>
</div>
<section id="S27" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">27 </span>Speed of Finite-State Spell-Checking</h3>

<div id="S27.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Main article:</span> <em class="ltx_emph">Effect of Language and Error Models on Efficiency
of Finite-State Spell-Checking and Correction</em> by Tommi A. Pirinen and Sam
Hardwick. In this article we set out to prove that finite/state
spell/checking is an efficient solution and document how builders of
spelling checkers can tune the parameters of their models.</p>
</div>
<section id="S27.SS1" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">27.1 </span>Motivation</h4>

<div id="S27.SS1.p1" class="ltx_para">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">motivation</span> for the article was practical, in discussions with
people building alternative spelling checkers, as well as with alpha testers of
our more complex spell-checkers it was commonly noted that the speed might be
an issue for finite/state spell/checking systems to gain ground from the
traditional solutions. It is noteworthy, that the formulation of the system was
from the beginning built so that the developers of the language and error
models can freely make these optimisations as needed.</p>
</div>
<div id="S27.SS1.p2" class="ltx_para">
<p class="ltx_p">When discussing such practical measurements as the speed of a spell-checker,
care must be taken in evaluation metrics. The most common use of a
spell-checker is still inter-active, where a misspelt word is underlined when
the user makes a mistake, and the system is capable of suggesting corrections
when a user requests one. This sets the limits that a system should be capable
of checking text as it is typed, and responding with suggestions as demanded.
It is well known that as acceptors and language recognisers, finite/state
language models can answer the question of whether the string is in a given
language in linear time. In practical measurements, this amounts to
spell/checking times up to 100,000 strings per second e.g.
by <cite class="ltx_cite ltx_citemacro_citet">Silfverberg and LindÃ©n (<a href="#bib.bib108" title="HFST runtime format—a compacted transducer format allowing for fast lookup" class="ltx_ref">2009</a>)</cite>. The central evaluation and optimisation was
therefore performed on the error correction part, which varies both with the
language model and the error model selected.
</p>
</div>
</section>
<section id="S27.SS2" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">27.2 </span>Related Works</h4>

<div id="S27.SS2.p1" class="ltx_para">
<p class="ltx_p">In the article, we study different <span class="ltx_text ltx_font_bold">related</span> optimisation schemes from
non-finite/state spell/checking and evaluate how different error models and
parameters affect the speed of finite/state spell/checking. We also ran the
tests on a scale of morphologically different languages to see if the concept
of morphological complexity is reflected in the speed of spell/checking.</p>
</div>
<div id="S27.SS2.p2" class="ltx_para">
<p class="ltx_p">The optimisation of speed in spell/checking software has been a central topic
in my research since the beginning. There are numerous solutions that we ported
into finite/state form to test their efficiency, and a few methods related to
problems with the finite/state formulation of the error models (i.e. the
generated duplicate correction paths in the edit distance models larger than 1
edits long, and the limitation of the result set size during traversal).</p>
</div>
</section>
<section id="S27.SS3" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">27.3 </span>Results</h4>

<div id="S27.SS3.p1" class="ltx_para">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">results</span> of the evaluation – reproduced in
Table <a href="#S27.T5" title="Table 5 ‣ 27.3 Results ‣ 27 Speed of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> from the
article <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib8a" title="Effect of language and error models on efficiency of finite-state spell-checking and correction" class="ltx_ref">IX</a>)</cite> – are enlightening. Basically the main
finding of the article is that disallowing the change of the initial letter
(<em class="ltx_emph">no firsts ed</em>) in the error model provides a significant boost of
performance.</p>
</div>
<figure id="S27.T5" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Error model</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t"><span class="ltx_text ltx_font_bold">English</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t"><span class="ltx_text ltx_font_bold">Finnish</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Greenlandic</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Edit distance 1</th>
<td class="ltx_td ltx_align_right ltx_border_t">0.26</td>
<td class="ltx_td ltx_align_right ltx_border_t">6.78</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">4.79</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Edit distance 2</th>
<td class="ltx_td ltx_align_right">7.55</td>
<td class="ltx_td ltx_align_right">220.42</td>
<td class="ltx_td ltx_align_right ltx_border_r">568.36</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">No firsts ed 1</th>
<td class="ltx_td ltx_align_right">0.44</td>
<td class="ltx_td ltx_align_right">3.19</td>
<td class="ltx_td ltx_align_right ltx_border_r">3.52</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">No firsts ed 2</th>
<td class="ltx_td ltx_align_right ltx_border_b">1.38</td>
<td class="ltx_td ltx_align_right ltx_border_b">61.88</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r">386.06</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Effect of language and error models on speed (time in seconds per
10,000 word forms). Reproduced from <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib8a" title="Effect of language and error models on efficiency of finite-state spell-checking and correction" class="ltx_ref">IX</a>)</cite>.
</figcaption>
</figure>
<div id="S27.SS3.p2" class="ltx_para">
<p class="ltx_p">Ignoring errors in the first-position of the word is known from many practical
systems, but is little documented. Some of the research, such
as <cite class="ltx_cite ltx_citemacro_citet">Bhagat (<a href="#bib.bib13" title="Spelling error pattern analysis of Punjabi typed text" class="ltx_ref">2007</a>)</cite>, provides some statistics on the plausibility of
a method: it is more likely to make a mistake in other parts of the words
than the very first letter, but does not offer hypotheses why this happens.
Practically, we replicated these results for our set of morphologically correct
languages as well, giving further evidence that it may be used as an
optimisation scheme with a reasonable quality trade-off.</p>
</div>
<div id="S27.SS3.p3" class="ltx_para">
<p class="ltx_p">It is likewise interesting that even though we tried to devise methods to
prevent the larger error models from doing unnecessary things like removing and
adding precisely the same characters in succession, this did not provide a
significant addition to the search speed of the corrections.</p>
</div>
<div id="S27.SS3.p4" class="ltx_para">
<p class="ltx_p">The comparative evaluation of the different language models is relatively
rarely found in literature. This could be due to the fact that scientific
writing in spell/checking is still more oriented towards algorithmics
research, where the language models of related systems do not have an influence
on the asymptotic speed of the process and are thus considered uninteresting.
Most of the studies are focused on measuring overall systems of single
languages, such as Arabic <cite class="ltx_cite ltx_citemacro_citep">(Attia<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib9" title="Improved spelling error detection and correction for Arabic" class="ltx_ref">2012</a>)</cite>, Indian
languages <cite class="ltx_cite ltx_citemacro_citep">(Chaudhuri, <a href="#bib.bib19" title="Towards Indian language spell-checker design" class="ltx_ref">2002</a>)</cite>,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">31</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">31</sup>The wording “Indian
language(s)” is from the article, and while not a linguistically coherent
group, is commonly used in the field of Indian natural language engineering
and e.g., in linguistic conferences such as COLING 2012, CICLING 2012 organised
in India, as the name of the topic focus. The specific examples in the article
seem to be about Bangla.</span></span></span> Hungarian <cite class="ltx_cite ltx_citemacro_citep">(TrÃ³n<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib112" title="Hunmorph: open source word analysis" class="ltx_ref">2005</a>)</cite> or
Spanish <cite class="ltx_cite ltx_citemacro_citep">(Otero<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib86" title="Contextual spelling correction" class="ltx_ref">2007</a>)</cite>, as well as all the research on
English <cite class="ltx_cite ltx_citemacro_citep">(Mitton, <a href="#bib.bib75" title="Spelling checkers, spelling correctors and the misspellings of poor spellers" class="ltx_ref">1987</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S28" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">28 </span>Precision of Finite-State Spell-Checking</h3>

<div id="S28.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Main article:</span> <em class="ltx_emph">Speed and Quality Trade-Offs in Weighted
Finite-State Spell-Checking</em>. In this article, I take a broad look at the
evaluation of all the language and error models and testing approaches we have
developed in the past few years of research and compare them with existing
results in real systems.</p>
</div>
<section id="S28.SS1" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">28.1 </span>Motivation</h4>

<div id="S28.SS1.p1" class="ltx_para">
<p class="ltx_p">The <span class="ltx_text ltx_font_bold">motivation</span> for this article was to perform a larger survey of all
the finite/state spelling methods that have been implemented in my thesis
project. In the paper I aim to show that the current state-of-the-art for
finite/state spell/checking is starting to be a viable option for many
common end-user applications. This article is a practical continuation of the
speed optimisation evaluation, where I studied the concept of speeding up
finite/state spell/checking systems by manipulating error and language
models. In order for these optimisations to be usable for end-users, I needed
to show that the quality degradation caused by the optimisation schemes that
remove a part of the search space is not disproportionately large for the error
models that are effective enough to be used in real-world systems.</p>
</div>
</section>
<section id="S28.SS2" class="ltx_subsection">
<h4 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">28.2 </span>Results</h4>

<div id="S28.SS2.p1" class="ltx_para">
<p class="ltx_p">When measuring the quality of spelling error correction, the basic finding is
that the simple edit-distance measure at length one will usually cover the
majority of errors in typical typing error situations. For English, the original
statistics were 95% <cite class="ltx_cite ltx_citemacro_citep">(Damerau, <a href="#bib.bib26" title="A technique for computer detection and correction of spelling errors" class="ltx_ref">1964</a>)</cite>, with various newer studies
giving similar findings between 79% and 95%, depending on the type of
corpora <cite class="ltx_cite ltx_citemacro_citep">(Kukich, <a href="#bib.bib62" title="Spelling correction for the telecommunications network for the deaf" class="ltx_ref">1992</a>)</cite>. The numbers for morphologically complex
languages are not in the same range. Rather the edit distance of one gains
around 50–70% coverage. One reason for this may be that the length of a word
is a factor in the typing error process. Thus it is more likely to type two or
more typos and leave them uncorrected with a language where the average word
length is 16 compared with one where it is 6. Intuitively this would make
sense, and the figures obtained from my tests in
article <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib10" title="Quality and speed trade-offs in weighted finite-state spell-checking" class="ltx_ref">X</a>)</cite> – reproduced in
Table <a href="#S28.T6" title="Table 6 ‣ 28.2 Results ‣ 28 Precision of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> – provide some evidence to that effect.</p>
</div>
<figure id="S28.T6" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Rank:</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><math id="S28.T6.m1" class="ltx_Math" alttext="1^{st}" display="inline"><msup><mn>1</mn><mrow><mi>s</mi><mo>⁢</mo><mi>t</mi></mrow></msup></math></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><math id="S28.T6.m2" class="ltx_Math" alttext="2^{nd}" display="inline"><msup><mn>2</mn><mrow><mi>n</mi><mo>⁢</mo><mi>d</mi></mrow></msup></math></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><math id="S28.T6.m3" class="ltx_Math" alttext="3^{rd}" display="inline"><msup><mn>3</mn><mrow><mi>r</mi><mo>⁢</mo><mi>d</mi></mrow></msup></math></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><math id="S28.T6.m4" class="ltx_Math" alttext="4^{th}" display="inline"><msup><mn>4</mn><mrow><mi>t</mi><mo>⁢</mo><mi>h</mi></mrow></msup></math></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t"><math id="S28.T6.m5" class="ltx_Math" alttext="5^{th}" display="inline"><msup><mn>5</mn><mrow><mi>t</mi><mo>⁢</mo><mi>h</mi></mrow></msup></math></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">rest</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">Language and error models</span></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_border_r"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">English Hunspell</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">59.3</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">5.8</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">3.5</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">2.3</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">English aspell</span></td>
<td class="ltx_td ltx_align_right ltx_border_r">55.7</td>
<td class="ltx_td ltx_align_right ltx_border_r">5.7</td>
<td class="ltx_td ltx_align_right ltx_border_r">8.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">2.2</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">English w/ 1 error</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">66.7</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">7.0</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">5.2</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">1.8</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">1.8</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">1.8</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">English w/ 1 non-first error</span></td>
<td class="ltx_td ltx_align_right ltx_border_r">66.7</td>
<td class="ltx_td ltx_align_right ltx_border_r">8.8</td>
<td class="ltx_td ltx_align_right ltx_border_r">7.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">1.8</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">English w/ 1 Hunspell error</span></td>
<td class="ltx_td ltx_align_right ltx_border_r">45.6</td>
<td class="ltx_td ltx_align_right ltx_border_r">8.7</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">English w/ 2 errors</span></td>
<td class="ltx_td ltx_align_right ltx_border_r">71.9</td>
<td class="ltx_td ltx_align_right ltx_border_r">14.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">3.5</td>
<td class="ltx_td ltx_align_right ltx_border_r">3.5</td>
<td class="ltx_td ltx_align_right ltx_border_r">3.5</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">English w/ 2 non-first errors</span></td>
<td class="ltx_td ltx_align_right ltx_border_r">71.3</td>
<td class="ltx_td ltx_align_right ltx_border_r">17.5</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">1.8</td>
<td class="ltx_td ltx_align_right ltx_border_r">3.5</td>
<td class="ltx_td ltx_align_right ltx_border_r">1.8</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">English w/ 2 Hunspell errors</span></td>
<td class="ltx_td ltx_align_right ltx_border_r">73.7</td>
<td class="ltx_td ltx_align_right ltx_border_r">12.3</td>
<td class="ltx_td ltx_align_right ltx_border_r">1.8</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">3.5</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">English w/ 3 errors</span></td>
<td class="ltx_td ltx_align_right ltx_border_r">73.7</td>
<td class="ltx_td ltx_align_right ltx_border_r">14.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">3.5</td>
<td class="ltx_td ltx_align_right ltx_border_r">3.5</td>
<td class="ltx_td ltx_align_right ltx_border_r">5.3</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">English w/ 3 non-first errors</span></td>
<td class="ltx_td ltx_align_right ltx_border_r">73.7</td>
<td class="ltx_td ltx_align_right ltx_border_r">17.5</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">1.8</td>
<td class="ltx_td ltx_align_right ltx_border_r">3.5</td>
<td class="ltx_td ltx_align_right ltx_border_r">3.5</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">English w/ 3 Hunspell errors</span></td>
<td class="ltx_td ltx_align_right ltx_border_r">73.7</td>
<td class="ltx_td ltx_align_right ltx_border_r">12.3</td>
<td class="ltx_td ltx_align_right ltx_border_r">1.8</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">8.8</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Finnish aspell</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">21.1</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">5.8</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">3.8</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">1.9</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">Finnish w/ 1 errors</span></td>
<td class="ltx_td ltx_align_right ltx_border_r">54.8</td>
<td class="ltx_td ltx_align_right ltx_border_r">19.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">7.1</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">Finnish w/ 2 errors</span></td>
<td class="ltx_td ltx_align_right ltx_border_r">54.8</td>
<td class="ltx_td ltx_align_right ltx_border_r">19.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">7.1</td>
<td class="ltx_td ltx_align_right ltx_border_r">2.4</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">7.1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">Finnish w/ 1 non-first error</span></td>
<td class="ltx_td ltx_align_right ltx_border_r">54.8</td>
<td class="ltx_td ltx_align_right ltx_border_r">21.4</td>
<td class="ltx_td ltx_align_right ltx_border_r">4.8</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">Finnish w/ 2 non-first errors</span></td>
<td class="ltx_td ltx_align_right ltx_border_r">54.8</td>
<td class="ltx_td ltx_align_right ltx_border_r">21.4</td>
<td class="ltx_td ltx_align_right ltx_border_r">4.8</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">7.1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Greenlandic w/ 1 error</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">13.3</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">2.2</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">6.7</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">2.2</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t">8.9</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">Greenlandic w/ 1 nonfirst error</span></td>
<td class="ltx_td ltx_align_right ltx_border_r">13.3</td>
<td class="ltx_td ltx_align_right ltx_border_r">2.2</td>
<td class="ltx_td ltx_align_right ltx_border_r">6.7</td>
<td class="ltx_td ltx_align_right ltx_border_r">2.2</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">8.9</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">Greenlandic w/ 2 errors</span></td>
<td class="ltx_td ltx_align_right ltx_border_r">13.3</td>
<td class="ltx_td ltx_align_right ltx_border_r">6.7</td>
<td class="ltx_td ltx_align_right ltx_border_r">4.4</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_r">35.6</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r"><span class="ltx_text ltx_font_bold">Greenlandic w/ 2 nonfirst errors</span></td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r">13.3</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r">6.7</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r">4.4</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r">0.0</td>
<td class="ltx_td ltx_align_right ltx_border_b ltx_border_r">35.6</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Effect of different language and error models on correction
quality (in%). Reproduced from <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib10" title="Quality and speed trade-offs in weighted finite-state spell-checking" class="ltx_ref">X</a>)</cite>.
</figcaption>
</figure>
</section>
</section>
<section id="S29" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">29 </span>Conclusions</h3>

<div id="S29.p1" class="ltx_para">
<p class="ltx_p">In this chapter, I have studied the aspects of finite/state spell/checking
that would ensure a plausible end-user spell-checker in a practical application. I
have shown that decrease in speed is negligible when moving from
optimised specific non-finite-state methods to correct strings, or traverse
a finite/state network to an actual finite/state algebra. The
speed of finite-state solutions for correcting morphologically complex
languages is faster than that of current software solutions such as Hunspell.</p>
</div>
<div id="S29.p2" class="ltx_para">
<p class="ltx_p">I have tested a set of morphologically different languages and large corpora to
show the usability of finite/state language and error-models for practical
spell/checking applications. I have also shown that it is possible to tune
with little effort the systems for interactive spell/checking using the
finite/state algebra.</p>
</div>
<div id="S29.p3" class="ltx_para">
<p class="ltx_p">The practical findings show the requirement of different speeds and quality
factors for different applications of spelling checkers. The research
presented here shows, that by varying both the automata that make up the
language model and the error model of the spelling checker, it is possible to
create suitable variants of spelling checkers for various tasks by simple
fine-tuning the parameters of the finite/state automaton.</p>
</div>
</section>
</section>
<section id="thechapter-at-IDh" class="ltx_chapter">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter <span class="ltx_ERROR undefined">\thechapter</span> </span>Conclusion</h2>

<div id="thechapter-at-IDh.p1" class="ltx_para">
<p class="ltx_p">In this thesis, I have researched the plausibility of making finite/state
language and error models for real-world, finite/state spell/checking and
correction applications, and presented approaches to formulate traditional
spell/checking solutions for finite/state use. The contributions of this
work are divided into two separate sections:
Section <a href="#S30" title="30 Scholarly Contributions ‣ Chapter \thechapter Conclusion ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">30</span></a> summarises the scientific
contributions of the work that is in my thesis, and
Section <a href="#S31" title="31 Practical Contributions ‣ Chapter \thechapter Conclusion ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">31</span></a> recounts the results of this thesis
in terms of real-world spelling checkers that can be used in end-user systems.
Finally, in Section <a href="#S32" title="32 Future Work ‣ Chapter \thechapter Conclusion ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">32</span></a>, I go through the related research
questions and practical ideas that the results of this thesis lead into.</p>
</div>
<section id="S30" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">30 </span>Scholarly Contributions</h3>

<div id="S30.p1" class="ltx_para">
<p class="ltx_p">One of the main contributions, I believe, is the improvement and verification
of the statistical language modelling methods in the context of weighted
finite/state automata. I believe that throughout the thesis, by continuously
using the initial finding of the word-form-based compound word training
approach introduced in <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib1" title="Weighted finite-state morphological analysis of Finnish compounds with HFST-LEXC" class="ltx_ref">I</a>)</cite>, I have shown that
<span class="ltx_ERROR undefined">\glslink</span>morphological complexitymorphologically complex languages can be
statistically trained as weighted finite/state automata and I maintain that
this approach should be carried out in the future finite/state language
models.</p>
</div>
<div id="S30.p2" class="ltx_para">
<p class="ltx_p">In the spelling correction mechanics, I have shown that weighted finite-state
transducers implement a feasible error model. According to my experiments, it
can be reasonably used in interactive spelling correction software in stead of
any traditional spelling correction solution.
</p>
</div>
<div id="S30.p3" class="ltx_para">
<p class="ltx_p">In my contribution to the practical software engineering side of language
modelling, I have shown that a generalised finite/state formula based on a
knowledge of morphology can be used to compile the most common existing
language models into automata.</p>
</div>
<div id="S30.p4" class="ltx_para">
<p class="ltx_p">Finally, to tie in the scientific evaluation of finite/state spell/checking
as a viable alternative, I have performed a large-scale testing on languages
with a wide range of morphological complexity, with different resources, and
shown that all are plausibly usable for typical spelling correction situations.</p>
</div>
</section>
<section id="S31" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">31 </span>Practical Contributions</h3>

<div id="S31.p1" class="ltx_para">
<p class="ltx_p">The spell/checking software that is the topic of this thesis is a very
practical real-world application that, despite this being an academic
dissertation, cannot be ignored when talking about the contributions of this
thesis. One of the important contributions, I believe, is the alpha testing
version of the Greenlandic spelling correction for OpenOffice, providing a kind
of a proof that finite-state spell-checking of Greenlandic is implementable.</p>
</div>
<div id="S31.p2" class="ltx_para">
<p class="ltx_p">Real-world software, consisting of components such as spell-checking libraries
libvoikko <span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">32</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">32</sup><a href="http://voikko.sf.net" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://voikko.sf.net</a></span></span></span> and
enchant,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">33</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">33</sup><a href="http://abiword.com/enchant" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://abiword.com/enchant</a></span></span></span> GNOME desktop
environment,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">34</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">34</sup><a href="http://gnome.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://gnome.org</a></span></span></span> office suites OpenOffice.org and
Libreoffice,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">35</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">35</sup><a href="http://libreoffice.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://libreoffice.org</a></span></span></span> and Firefox
browser,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">36</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">36</sup><a href="http://firefox.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://firefox.org</a></span></span></span> have been a good testing ground for
other morphologically complex languages, including Finnish and North Saami, but
also a range of forthcoming Uralic languages currently being implemented at
the University of
Helsinki,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">37</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">37</sup><a href="http://www.ling.helsinki.fi/~rueter/UrjSpell/testaus_fi.shtml" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.ling.helsinki.fi/~rueter/UrjSpell/testaus_fi.shtml</a>
(temporary URL for testing phase)</span></span></span> most of which have never had a
spell/checking system at all.</p>
</div>
<div id="S31.p3" class="ltx_para">
<p class="ltx_p">The statistical and weighted methods used to improve language models have
been directly ported into classical tool chains and work flows of the
finite/state morphology, and are now supported in the HFST tools including
<span class="ltx_text ltx_font_typewriter">hfst-lexc</span> for traditional finite/state morphologies.</p>
</div>
</section>
<section id="S32" class="ltx_section">
<h3 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">32 </span>Future Work</h3>

<div id="S32.p1" class="ltx_para">
<p class="ltx_p">In terms of error correcting, the systems are deeply rooted in the basic models
of typing errors, such as the Levenshtein-Damerau edit distance. It would be
interesting to try to implement more accurate models of errors based on
findings of cognitive-psychological studies on the kinds of errors that are
actually made – this research path I believe has barely scratched the surface
with optimisations like <em class="ltx_emph">avoid typing mistakes at the beginning of the
word</em>. Furthermore, it should be possible to make adaptive error models using a
simple feedback system for finite/state error-models.</p>
</div>
<div id="S32.p2" class="ltx_para">
<p class="ltx_p">During the time I spent writing this thesis, the world of spell/checking has
changed considerably with the mass market of a variety of input methods in the
new touch screen user interfaces. When I started my thesis work, by far the
most common methods to input text were regular 100+ key PC keyboards and key
pads on mobile phones with the T9 input
method.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">38</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">38</sup><a href="http://www.t9.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.t9.com</a></span></span></span> This is no longer the case, as input
methods like Swype, XT9, and so on, are gaining popularity. A revision of
the methods introduced in this thesis is required, especially since the
input methods of these systems are more heavily reliant on high-quality,
automatic spelling correction than before.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">39</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">39</sup>As a case in point, there
are collections of humorous automatic spelling corrections caused by these
systems, on the internet, see e.g., <a href="http://www.damnyouautocorrect.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.damnyouautocorrect.com/</a>.</span></span></span>
This point will slightly affect the considerations of the practical evaluations
of the speed of spell/checking; new spelling correctors coupled with
predictive entry methods need to be invoked after each input event instead
of only when correction is required.</p>
</div>
<div id="S32.p3" class="ltx_para">
<p class="ltx_p">On the engineering and software management side, one practical, even
foreseeable, development would be to see these methods being adopted into use
in real-world spell-checkers. The actual implementation has been done with the
help of spell/checking systems like voikko, and enchant. The morphologically
complex languages that are represented in my thesis lack basic language
technology support partially due to the dominance of too limited non-finite
state forms of language modelling.</p>
</div>
<div id="S32.p4" class="ltx_para">
<p class="ltx_p">The concept of context-aware spell/checking with finite/state systems was
merely touched upon by article <cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib9a" title="Improving finite-state spell-checker suggestions with part of speech n-grams" class="ltx_ref">VIII</a>)</cite>. However, the
results showed some promise and pinpointed some problems, and with some work it
may be possible to gain as sizable improvements for morphologically complex
languages as for morphologically simple ones. One possible solution to be drawn
from the actual results of the other articles in the thesis is to use
morph-based statistics for all languages to get more robust statistics
and confirmatory results. In order to get the speed and memory efficiency to a
level that is usable in everyday applications, I suspect there is room for much
improvement through basic engineering decisions and optimisations. This has
also been a topic in much of the recent spell/checking research for
non-finite/state systems, see e.g. <cite class="ltx_cite ltx_citemacro_citet">Carlson<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib18" title="Scaling up context-sensitive text correction" class="ltx_ref">2001</a>)</cite>.</p>
</div>
<div id="S32.p5" class="ltx_para">
<p class="ltx_p">The concept of context-based error-detection, real-word error detection, and
other forms of grammar correction has not been dealt with at a very large-scale
in this thesis. That has been done partly on purpose, but in fact the basic
context-aware real-word error detection task does not really differ in practice
from its non-finite/state versions, and the conversion is even more trivial
than in the case of the error correction part discussed in article
<cite class="ltx_cite ltx_citemacro_citepalias">(<a href="#bib.bib9a" title="Improving finite-state spell-checker suggestions with part of speech n-grams" class="ltx_ref">VIII</a>)</cite>. The main limiting factor for rapidly testing
such a system is the lack of error corpora with real-word spelling errors.</p>
</div>
<div id="S32.p6" class="ltx_para">
<p class="ltx_p">As a theoretically interesting development, it would be nice to have formal
proof that the finite/state spell/checking and correction models are a
proper superset of the Hunspell spell/checking methods, as my quantitative
evaluation and existing conversion algorithm for the current implementation
shows that this is plausible and even likely.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
<li id="bib.bib2" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">E. Agirre, I. Alegria, X. Arregi, X. Artola, A. D. de, M. Maritxalar, K. Sarasola and M. Urkia (1992)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">XUXEN: a spelling checker/corrector for Basque based on two-level morphology</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of The Third Conference on Applied Natural Language Processing</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 119–125</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#thechapter-at-IDf.p1" title="Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Chapter \thechapter</span></a>.
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">A.V. Aho and M.J. Corasick (1975)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Efficient string matching: an aid to bibliographic search</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Communications of the ACM</span> <span class="ltx_text ltx_bib_volume">18</span> (<span class="ltx_text ltx_bib_number">6</span>), <span class="ltx_text ltx_bib_pages"> pp. 333–340</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p14" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>,
<a href="#S5.p7" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem ltx_bib_book">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">A.V. Aho, M.S. Lam, R. Sethi and J.D. Ullman (2007)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Compilers: principles, techniques, and tools</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_edition">second edition</span>,  <span class="ltx_text ltx_bib_publisher">Pearson/Addison Wesley</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 0321486811</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S7.p1" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">H. Al-Mubaid and K. Truemper (2006)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning to find context based spelling errors</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Data Mining and Knowledge Discovery Approaches Based on Rule Induction Techniques</span>, <span class="ltx_text ltx_bib_pages"> pp. 597–627</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p12" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>,
<a href="#S5.p15" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">M. Attia, P. Pecina, Y. Samih, K. Shaalan and J.  (2012)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Improved spelling error detection and correction for Arabic</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 103–112</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/C12-2011" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S27.SS3.p4" title="27.3 Results ‣ 27 Speed of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">27.3</span></a>.
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K.R. Beesley (2004)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Morphological analysis and generation: a first step in natural language processing</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">First Steps in Language Documentation for Minority Languages: Computational Linguistic Tools for Morphology, Lexicon and Corpus Compilation, Proceedings of the SALTMIL Workshop at LREC</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1–8</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S7.p3" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem ltx_bib_book">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K. R. Beesley and L. Karttunen (2003)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Finite state morphology</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">CSLI publications</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1575864341</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p1" title="3 Finite-State Technology in Natural Language Processing ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>,
<a href="#S7.p3" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>,
<a href="#thechapter-at-IDa.p24" title="Original Papers ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_title">Original Papers</span></a>.
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem ltx_bib_thesis">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">M. Bhagat (2007)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Spelling error pattern analysis of Punjabi typed text</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Master’s Thesis</span>, <span class="ltx_text ltx_bib_publisher">Thapar University</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://hdl.handle.net/123456789/247" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S25.p2" title="25 Other Finite-State Error Models ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">25</span></a>,
<a href="#S27.SS3.p2" title="27.3 Results ‣ 27 Speed of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">27.3</span></a>.
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">J. Bigert, L. Ericson and A. Solis (2003)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">AutoEval and Missplel: two generic tools for automatic evaluation</span>.
</span>
<span class="ltx_bibblock">See <span class="ltx_text ltx_bib_crossref"><cite class="ltx_cite"><a href="#bib.bib124" title="NODALIDA 2003, the 14th nordic conference of computational linguistics, reykjavik, may 30-31, 2003, proceedings" class="ltx_ref">62</a></cite></span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Reykjavik, Iceland</span>, <span class="ltx_text ltx_bib_pages"> pp. 7</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S17.SS3.p1" title="17.3 Results ‣ 17 Weighting Hunspell as Finite-State Automata ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17.3</span></a>.
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem ltx_bib_thesis">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">J. Bigert (2005)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automatic and unsupervised methods in natural language processing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Ph.D. Thesis</span>, <span class="ltx_text ltx_bib_publisher">Royal Institute of Technology (KTH)</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S17.SS3.p1" title="17.3 Results ‣ 17 Weighting Hunspell as Finite-State Automata ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17.3</span></a>.
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">T. Brants, A. C. Popat, P. Xu, F. J. Och and J. Dean (2007)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Large language models in machine translation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Prague, Czech Republic</span>, <span class="ltx_text ltx_bib_pages"> pp. 858–867</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/D/D07/D07-1090" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S7.p11" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">E. Brill and R. C. Moore (2000)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An improved error model for noisy channel spelling correction</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ACL 2000: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Morristown, NJ, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 286–293</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/http://dx.doi.org/10.3115/1075218.1075255" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p9" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">A.J. Carlson, J. Rosen and D. Roth (2001)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Scaling up context-sensitive text correction</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Thirteenth Conference on Innovative Applications of Artificial Intelligence Conference</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 45–50</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S32.p4" title="32 Future Work ‣ Chapter \thechapter Conclusion ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">32</span></a>.
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">B.B. Chaudhuri (2002)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Towards Indian language spell-checker design</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Language Engineering Conference, 2002. Proceedings</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 139–146</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S27.SS3.p4" title="27.3 Results ‣ 27 Speed of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">27.3</span></a>.
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K. Chege, P. Wagacha, G. De Pauw, L. Muchemi and W. Ngâ (2010)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Developing an open source spell-checker for GÄ«kÅ«yÅ«</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">AfLaT 2010</span>, <span class="ltx_text ltx_bib_pages"> pp. 31–35</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p7" title="2 Morphological Complexity ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">S.F. Chen and J. Goodman (1999)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An empirical study of smoothing techniques for language modeling</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computer Speech &amp; Language</span> <span class="ltx_text ltx_bib_volume">13</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 359–393</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S7.p14" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K. Church, T. Hart and J. Gao (2007)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Compressing trigram language models with Golomb coding</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 199–207</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S24.SS3.p2" title="24.3 Results ‣ 24 Context-Aware Spelling Correction ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24.3</span></a>.
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K.W. Church and W.A. Gale (1991)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Probability scoring for spelling correction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Statistics and Computing</span> <span class="ltx_text ltx_bib_volume">1</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 93–103</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S17.SS2.p1" title="17.2 Related Works ‣ 17 Weighting Hunspell as Finite-State Automata ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17.2</span></a>,
<a href="#S25.p3" title="25 Other Finite-State Error Models ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">25</span></a>,
<a href="#S5.T1" title="Table 1 ‣ 5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a href="#S5.p15" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>,
<a href="#S5.p9" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">M. Creutz, K. Lagus, K. LindÃ©n and S. Virpioja (2005)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Morfessor and Hutmegs: unsupervised morpheme segmentation for highly-inflecting and compounding languages</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S16.SS4.p1" title="16.4 Future Research ‣ 16 The Statistical Training of Compounding Language Models ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16.4</span></a>.
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem ltx_bib_incollection">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">C. Culy (1987)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The complexity of the vocabulary of Bambara</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">The Formal Complexity of Natural Language</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 349–357</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p1" title="3 Finite-State Technology in Natural Language Processing ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">F. J. Damerau (1964)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A technique for computer detection and correction of spelling errors</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Commun. ACM</span> <span class="ltx_text ltx_bib_volume">7</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp. 171–176</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0001-0782</span>,
<a href="http://doi.acm.org/10.1145/363958.363994" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1145/363958.363994" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S21.p1" title="21 Edit Distance Measures ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a>,
<a href="#S28.SS2.p1" title="28.2 Results ‣ 28 Precision of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">28.2</span></a>,
<a href="#S5.p1" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>,
<a href="#S5.p4" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>,
<a href="#thechapter-at-IDb.p1" title="Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Chapter \thechapter</span></a>.
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">S. Deorowicz and M.G. Ciura (2005)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Correcting spelling errors by modelling their causes</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">International Journal of Applied Mathematics and Computer Science</span> <span class="ltx_text ltx_bib_volume">15</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 275–285</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S25.p1" title="25 Other Finite-State Error Models ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">25</span></a>,
<a href="#thechapter-at-IDf.p1" title="Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Chapter \thechapter</span></a>.
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">M. Federico, N. Bertoldi and M. Cettolo (2008)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">IRSTLM: an open source toolkit for handling large scale language models.</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Interspeech</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1618–1621</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S16.SS2.p1" title="16.2 Related Works ‣ 16 The Statistical Training of Compounding Language Models ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16.2</span></a>.
</span>
</li>
<li id="bib.bib125" class="ltx_bibitem ltx_bib_proceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">A. F. Gelbukh (Ed.) (2008)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Computational linguistics and intelligent text processing, 9th international conference, cicling 2008, haifa, israel, february 17-23, 2008, proceedings</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Springer</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#bib.bib118" title="Real-word spelling correction with trigrams: a reconsideration of the Mays, Damerau, and Mercer model" class="ltx_ref">L. A. Wilcox-O’Hearn, G. Hirst and A. Budanitsky (2008)</a>.
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">A.R. Golding and D. Roth (1999)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A winnow-based approach to context-sensitive spelling correction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Machine Learning</span> <span class="ltx_text ltx_bib_volume">34</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 107–130</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.T1" title="Table 1 ‣ 5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a href="#S5.p15" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">A.R. Golding (1995)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A Bayesian hybrid method for context-sensitive spelling correction</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Third Workshop on Very Large Corpora</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">3</span>, <span class="ltx_text ltx_bib_pages"> pp. 39–53</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.T1" title="Table 1 ‣ 5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a href="#S5.p15" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">R. E. Gorin (1971)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">SPELL: a spelling checking and correction program</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">On-line manual; web pageRetrieved 1st of August 2013: <span class="ltx_ERROR undefined">\url</span>http://www.saildart.org/, entries
UP, DOC: SPELL.REG</span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.T1" title="Table 1 ‣ 5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a href="#S5.p6" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>,
<a href="#S5.p8" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">J. H. Greenberg (1960)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A quantitative approach to the morphological typology of language</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">International Journal of American Linguistics</span> <span class="ltx_text ltx_bib_volume">26</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp. 178–194</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p3" title="2 Morphological Complexity ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem ltx_bib_thesis">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K. Greenfield and S. Judd (2010)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Open source natural language processing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Master’s Thesis</span>, <span class="ltx_text ltx_bib_publisher">Worchester Polytechnic Institute</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.wpi.edu/Pubs/E-project/Available/E-project-042810-055257/" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S9.SS2.p1" title="9.2 Related Works ‣ 9 Compiling Hunspell Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9.2</span></a>.
</span>
</li>
<li id="bib.bib122" class="ltx_bibitem ltx_bib_book">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">M. Haspelmath, H. Bibiko, J. Hagen and C. Schmidt (2005)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The World Atlas of Language Structures</span>.
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">125</span>,  <span class="ltx_text ltx_bib_publisher">Oxford University Press Oxford</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p3" title="2 Morphological Complexity ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">A. Hassan, S. Noeman and H. Hassan (2008)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Language independent text correction using finite state automata</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Third International Joint Conference on Natural Language Processing</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">2</span>, <span class="ltx_text ltx_bib_pages"> pp. 913–918</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p5" title="2 Morphological Complexity ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">M. HuldÃ©n (2009)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Fast approximate string matching with finite automata</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Procesamiento del Lenguaje Natural</span> <span class="ltx_text ltx_bib_volume">43</span>, <span class="ltx_text ltx_bib_pages"> pp. 57–64</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S20.p1" title="20 The Application of Finite-State Error Models ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a>,
<a href="#S20.p2" title="20 The Application of Finite-State Error Models ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a>,
<a href="#S5.T1" title="Table 1 ‣ 5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a href="#S7.p4" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">M. HuldÃ©n (2013)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Weighted and unweighted transducers for tweet normalization</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Tweet Normalisation Workshop at 29th Conference of the Spanish Society for Natural Language Processing (SEPLN 2013)</span>,  <span class="ltx_text ltx_bib_editor">I. Alegria, N. Aranberri, V. Fresno, P. Gamallo, L. PadrÃ³, I. S. Vicente, J. Turmo and A. Zubiaga (Eds.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 69–72</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://ceur-ws.org/Vol-1086/paper14.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.p3" title="6 Related Subfields and Research Results ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">F. Karlsson (1992)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">SWETWOL: a comprehensive morphological analyser for Swedish</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_volume">15</span>, <span class="ltx_text ltx_bib_pages"> pp. 1–45</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S15.SS2.p1" title="15.2 Related Works ‣ 15 Language Models for Languages with Compounding ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15.2</span></a>.
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">F. Karlsson (2007)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Constraints on multiple center-embedding of clauses</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Journal of Linguistics</span> <span class="ltx_text ltx_bib_volume">43</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 34</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p1" title="3 Finite-State Technology in Natural Language Processing ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">L. Karttunen, R. M. Kaplan and A. Zaenen (1992)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Two-level morphology with composition</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">COLING 1992</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 141–148</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S9.SS3.p4" title="9.3 Results ‣ 9 Compiling Hunspell Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9.3</span></a>.
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">L. Karttunen (1995)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The replace operator</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 33rd Annual Meeting on Association for Computational Linguistics</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 16–23</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S9.SS3.p4" title="9.3 Results ‣ 9 Compiling Hunspell Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9.3</span></a>.
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">L. Karttunen (2006)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Numbers and Finnish numerals</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">SKY Journal of Linguistics</span> <span class="ltx_text ltx_bib_volume">19</span>, <span class="ltx_text ltx_bib_pages"> pp. 407–421</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S12.SS2.p1" title="12.2 Related Works ‣ 12 Maintenance of the Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12.2</span></a>,
<a href="#S2.SS1.p3" title="2.1 On the Infinity of a Dictionary ‣ 2 Morphological Complexity ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>.
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">M.D. Kernighan, K.W. Church and W.A. Gale (1990)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A spelling correction program based on a noisy channel model</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 13th conference on Computational Linguistics</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 205–210</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p9" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem ltx_bib_book">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">D.E. Knuth (1973)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The art of computer programming</span>.
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">3</span>,  <span class="ltx_text ltx_bib_publisher">Addison-Wesley, Reading, MA</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p7" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">D. E. Knuth (1984)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Literate programming</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">The Computer Journal</span> <span class="ltx_text ltx_bib_volume">27</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 97–111</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S12.SS2.p1" title="12.2 Related Works ‣ 12 Maintenance of the Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12.2</span></a>.
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">C. Kobus, F. Yvon and G. Damnati (2008)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Normalizing SMS: are two metaphors better than one</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 22nd International Conference on Computational Linguistics (COLING 2008)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 441–448</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.p3" title="6 Related Subfields and Research Results ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">D. Kokkinakis (28-30)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A semantically annotated swedish medical corpus</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08)</span>,  <span class="ltx_text ltx_bib_editor">N. Calzolari, K. Choukri, B. Maegaard, J. M. , J. Odijk, S. Piperidis and D. Tapias (Eds.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Marrakech, Morocco</span> (<span class="ltx_text ltx_bib_language">english</span>).
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">http://www.lrec-conf.org/proceedings/lrec2008/</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 2-9517408-4-0</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S15.SS2.p1" title="15.2 Related Works ‣ 15 Language Models for Languages with Compounding ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15.2</span></a>.
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">A. Kornai (2002)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">How many words are there</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Glottometrics</span> <span class="ltx_text ltx_bib_volume">4</span>, <span class="ltx_text ltx_bib_pages"> pp. 61–86</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S7.p11" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem ltx_bib_thesis">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K. Koskenniemi (1983)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Two-level morphology: a general computational model for word-form recognition and production</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Ph.D. Thesis</span>, <span class="ltx_text ltx_bib_publisher">University of Helsinki</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.ling.helsinki.fi/koskenni/doc/Two-LevelMorphology.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p7" title="2 Morphological Complexity ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
<a href="#thechapter-at-IDb.p3" title="Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Chapter \thechapter</span></a>.
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K. Kukich (1992)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Spelling correction for the telecommunications network for the deaf</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Commun. ACM</span> <span class="ltx_text ltx_bib_volume">35</span> (<span class="ltx_text ltx_bib_number">5</span>), <span class="ltx_text ltx_bib_pages"> pp. 80–90</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0001-0782</span>,
<a href="http://doi.acm.org/10.1145/129875.129882" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1145/129875.129882" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S28.SS2.p1" title="28.2 Results ‣ 28 Precision of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">28.2</span></a>,
<a href="#thechapter-at-IDc.p1" title="Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Chapter \thechapter</span></a>,
<a href="#thechapter-at-IDf.p1" title="Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Chapter \thechapter</span></a>.
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K. Kukich (1992)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Techniques for automatically correcting words in text</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">ACM Comput. Surv.</span> <span class="ltx_text ltx_bib_volume">24</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 377–439</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0360-0300</span>,
<a href="http://dx.doi.org/http://doi.acm.org/10.1145/146370.146380" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p3" title="1 Components of Spell-Checking and Correction ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S1.p4" title="1 Components of Spell-Checking and Correction ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S5.p4" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">V. I. Levenshtein (1966)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Binary codes capable of correcting deletions, insertions, and reversals</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Soviet Physics—Doklady 10, 707â710. Translated from Doklady Akademii Nauk SSSR</span>, <span class="ltx_text ltx_bib_pages"> pp. 845–848</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">Untranslated version 1965</span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S21.p1" title="21 Edit Distance Measures ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a>,
<a href="#S5.p1" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">M. Liberman (2012)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Noisily channeling Claude Shannon</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">Blog post in Language LogRetrieved 1st of August 2013: <span class="ltx_ERROR undefined">\url</span>http://languagelog.ldc.upenn.edu/nll/?p=4115</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://languagelog.ldc.upenn.edu/nll/?p=4115" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p5" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K. LindÃ©n, E. Axelson, S. Hardwick, T. A. Pirinen and M.  (2011)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Hfstâframework for compiling and applying morphologies</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Systems and Frameworks for Computational Morphology</span>, <span class="ltx_text ltx_bib_pages"> pp. 67–85</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S20.p1" title="20 The Application of Finite-State Error Models ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a>,
<a href="#S4.SS2.p1" title="4.2 Division of Labour ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#thechapter-at-IDg.p5" title="Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Chapter \thechapter</span></a>.
</span>
</li>
<li id="bib.bib2a" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K. LindÃ©n and T. Pirinen (2009)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Weighting finite-state morphological analyzers using HFST tools</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">FSMNLP 2009</span>,  <span class="ltx_text ltx_bib_editor">B. Watson, D. Courie, L. Cleophas and P. Rautenbach (Eds.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1–12</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-86854-743-2</span>,
<a href="http://www.ling.helsinki.fi/klinden/pubs/fsmnlp2009weighting.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.ix2" title="II ‣ Original Papers ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><cite class="ltx_cite ltx_citemacro_citealias"><span class="ltx_ref">II</span></cite></span></a>,
<a href="#S4.SS1.p1" title="4.1 Chronological Order ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>,
<a href="#S4.SS2.p1" title="4.2 Division of Labour ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S4.SS3.p1" title="4.3 My Contributions ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.
</span>
</li>
<li id="bib.bib1" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K. LindÃ©n and T. Pirinen (2009)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Weighted finite-state morphological analysis of Finnish compounds with HFST-LEXC</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Nodalida 2009</span>,  <span class="ltx_text ltx_bib_editor">K. Jokinen and E. Bick (Eds.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">NEALT Proceedings</span>, Vol. <span class="ltx_text ltx_bib_volume">4</span>, <span class="ltx_text ltx_bib_pages"> pp. 89–95</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.ling.helsinki.fi/klinden/pubs/linden09dnodalida.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.ix1" title="I ‣ Original Papers ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><cite class="ltx_cite ltx_citemacro_citealias"><span class="ltx_ref">I</span></cite></span></a>,
<a href="#S15.p1" title="15 Language Models for Languages with Compounding ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>,
<a href="#S16.SS1.p1" title="16.1 Motivation ‣ 16 The Statistical Training of Compounding Language Models ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16.1</span></a>,
<a href="#S16.SS3.p1" title="16.3 Results ‣ 16 The Statistical Training of Compounding Language Models ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16.3</span></a>,
<a href="#S30.p1" title="30 Scholarly Contributions ‣ Chapter \thechapter Conclusion ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">30</span></a>,
<a href="#S4.SS1.p1" title="4.1 Chronological Order ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>,
<a href="#S4.SS2.p1" title="4.2 Division of Labour ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S4.SS3.p1" title="4.3 My Contributions ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K. LindÃ©n, M. Silfverberg and T. Pirinen (2009)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">HFST tools for morphology—an efficient open-source package for construction of morphological analyzers</span>.
</span>
<span class="ltx_bibblock">See <span class="ltx_text ltx_bib_crossref"><cite class="ltx_cite"><a href="#bib.bib128" title="Workshop on systems and frameworks for computational morphology, sfcm 2009, zÃ¼rich, switzerland, september 2009, proceedings" class="ltx_ref">Workshop on systems and frameworks for computational morphology, sfcm 2009, zÃ¼rich, switzerland, september 2009, proceedings, Mahlow and Piotrowski</a></cite></span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 28–47</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S10.SS3.p3" title="10.3 Results ‣ 10 Using Rule-Based Machine Translation Dictionaries ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10.3</span></a>,
<a href="#S4.SS2.p1" title="4.2 Division of Labour ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S9.SS3.p2" title="9.3 Results ‣ 9 Compiling Hunspell Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9.3</span></a>.
</span>
</li>
<li id="bib.bib128" class="ltx_bibitem ltx_bib_proceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">C. Mahlow and M. Piotrowski (Eds.) (2009)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Workshop on systems and frameworks for computational morphology, sfcm 2009, zÃ¼rich, switzerland, september 2009, proceedings</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">Lecture Notes in Computer Science</span>, Vol. <span class="ltx_text ltx_bib_volume">41</span>,  <span class="ltx_text ltx_bib_publisher">Springer</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-3-642-04130-3</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#bib.bib46" title="HFST tools for morphology—an efficient open-source package for construction of morphological analyzers" class="ltx_ref">K. LindÃ©n, M. Silfverberg and T. Pirinen (2009)</a>.
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem ltx_bib_book">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">C.D. Manning and H. Schütze (1999)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Foundations of statistical natural language processing</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">MIT Press</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S14.p1" title="14 Theory of Statistical Finite-State Models ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>,
<a href="#S7.p14" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">M. Maxwell and A. David (2008)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Joint grammar development by linguists and computer scientists</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Workshop on NLP for Less Privileged Languages, Third International Joint Conference on Natural Language Processing</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Hyderabad, India</span>, <span class="ltx_text ltx_bib_pages"> pp. 27–34</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S12.SS2.p1" title="12.2 Related Works ‣ 12 Maintenance of the Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12.2</span></a>.
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">E. Mays, F. J. Damerau and R. L. Mercer (1991)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Context based spelling correction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Inf. Process. Manage.</span> <span class="ltx_text ltx_bib_volume">27</span> (<span class="ltx_text ltx_bib_number">5</span>), <span class="ltx_text ltx_bib_pages"> pp. 517–522</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0306-4573</span>,
<a href="http://dx.doi.org/http://dx.doi.org/10.1016/0306-4573(91)90066-U" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S24.SS2.p1" title="24.2 Related Works ‣ 24 Context-Aware Spelling Correction ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24.2</span></a>,
<a href="#S5.p11" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">R. Mitton (1987)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Spelling checkers, spelling correctors and the misspellings of poor spellers</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Inf. Process. Manage.</span> <span class="ltx_text ltx_bib_volume">23</span> (<span class="ltx_text ltx_bib_number">5</span>), <span class="ltx_text ltx_bib_pages"> pp. 495–505</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0306-4573</span>,
<a href="http://dx.doi.org/10.1016/0306-4573(87)90116-6" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1016/0306-4573(87)90116-6" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S27.SS3.p4" title="27.3 Results ‣ 27 Speed of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">27.3</span></a>.
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">R. Mitton (2009)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Ordering the suggestions of a spellchecker without using context</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nat. Lang. Eng.</span> <span class="ltx_text ltx_bib_volume">15</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 173–192</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 1351-3249</span>,
<a href="http://dx.doi.org/http://dx.doi.org/10.1017/S1351324908004804" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#thechapter-at-IDc.p1" title="Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Chapter \thechapter</span></a>,
<a href="#thechapter-at-IDf.p1" title="Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Chapter \thechapter</span></a>.
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">M. Mohri (2003)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Edit-distance of weighted automata: general definitions and algorithms</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">International Journal of Foundations of Computer Science</span> <span class="ltx_text ltx_bib_volume">14</span> (<span class="ltx_text ltx_bib_number">6</span>), <span class="ltx_text ltx_bib_pages"> pp. 957–982</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S20.p1" title="20 The Application of Finite-State Error Models ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a>,
<a href="#S21.p4" title="21 Edit Distance Measures ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a>,
<a href="#S5.T1" title="Table 1 ‣ 5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a href="#S7.p4" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>,
<a href="#thechapter-at-IDf.p1" title="Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Chapter \thechapter</span></a>.
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">M. Mohri (1997)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Finite-state transducers in language and speech processing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Comp. Linguistics</span> <span class="ltx_text ltx_bib_volume">23</span>, <span class="ltx_text ltx_bib_pages"> pp. 269–311</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S7.p1" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>,
<a href="#S7.p2" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span>
</li>
<li id="bib.bib124" class="ltx_bibitem ltx_bib_proceedings">
<span class="ltx_bibtag ltx_bib_key ltx_role_refnum">[62]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_year"> (2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">NODALIDA 2003, the 14th nordic conference of computational linguistics, reykjavik, may 30-31, 2003, proceedings</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#bib.bib15" title="AutoEval and Missplel: two generic tools for automatic evaluation" class="ltx_ref">J. Bigert, L. Ericson and A. Solis (2003)</a>.
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">P. Norvig (2010)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">How to write a spelling corrector</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">Web pageretrieved first of November 2011, available <span class="ltx_ERROR undefined">\url</span>http://norvig.com/spell-correct.html</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://norvig.com/spell-correct.html" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S11.p2" title="11 Other Possibilities for Generic Morphological Formula ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>,
<a href="#thechapter-at-IDg.p3" title="Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Chapter \thechapter</span></a>.
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K. Oflazer (1996)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computational Linguistics</span> <span class="ltx_text ltx_bib_volume">22</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 73–89</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.T1" title="Table 1 ‣ 5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a href="#S5.p14" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>,
<a href="#S5.p15" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>,
<a href="#S7.p1" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>,
<a href="#S7.p4" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span>
</li>
<li id="bib.bib100" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">S. OrtÃ­z-Rojas, M. L. Forcada and G. R. SÃ¡nchez (2005)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">ConstrucciÃ³n y minimizaciÃ³n eficiente de transductores de letras a partir de diccionarios con paradigmas</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Procesamiento del lenguaje natural</span> <span class="ltx_text ltx_bib_volume">35</span>, <span class="ltx_text ltx_bib_pages"> pp. 51–57</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S10.SS2.p1" title="10.2 Related Works ‣ 10 Using Rule-Based Machine Translation Dictionaries ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10.2</span></a>.
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">J. Otero, J. Grana and M. Vilares (2007)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Contextual spelling correction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computer Aided Systems Theory–EUROCAST 2007</span>, <span class="ltx_text ltx_bib_pages"> pp. 290–296</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S24.SS2.p1" title="24.2 Related Works ‣ 24 Context-Aware Spelling Correction ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24.2</span></a>,
<a href="#S27.SS3.p4" title="27.3 Results ‣ 27 Speed of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">27.3</span></a>,
<a href="#S5.T1" title="Table 1 ‣ 5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a href="#S5.p13" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">E. Petterson, B. Megyesi and J. Tiedemann (2013)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An SMT approach to automatic annotation of historical text</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Workshop on Computational Historical Linguistics at NODALIDA 2013</span>,  <span class="ltx_text ltx_bib_editor">Ã. EyÃ¾Ã³rsson, L. Borin, D. Haug and E. RÃ¶gnvaldsson (Eds.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">LinkÃ¶ping Electronic Conference Proceedings</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-91-7519-587-2</span>,
<a href="http://www.ep.liu.se/ecp_article/index.en.aspx?issue=087;article=005" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.p4" title="6 Related Subfields and Research Results ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">L. Philips (1990)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Hanging on the metaphone</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computer Language</span> <span class="ltx_text ltx_bib_volume">7</span> (<span class="ltx_text ltx_bib_number">12</span>), <span class="ltx_text ltx_bib_pages"> pp. 39–44</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p10" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">L. Philips (2000)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The double metaphone search algorithm</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">C/C++ Users Journal</span> <span class="ltx_text ltx_bib_volume">18</span> (<span class="ltx_text ltx_bib_number">6</span>), <span class="ltx_text ltx_bib_pages"> pp. 38–43</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S22.SS2.p2" title="22.2 Results ‣ 22 Hunspell Error Correction ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22.2</span></a>,
<a href="#S23.p1" title="23 Phonemic Key Corrections ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">23</span></a>,
<a href="#S5.p10" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib5a" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">T.A. Pirinen and K. LindÃ©n (2010a)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Creating and weighting Hunspell dictionaries as finite-state automata</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Investigationes Linguisticae</span> <span class="ltx_text ltx_bib_volume">21</span>, <span class="ltx_text ltx_bib_pages"> pp. 1–16</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.ix5" title="V ‣ Original Papers ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><cite class="ltx_cite ltx_citemacro_citealias"><span class="ltx_ref">V</span></cite></span></a>,
<a href="#S4.SS1.p1" title="4.1 Chronological Order ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>,
<a href="#S4.SS2.p1" title="4.2 Division of Labour ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S4.SS3.p1" title="4.3 My Contributions ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">T.A. Pirinen and F.M. Tyers (2012)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Compiling Apertium morphological dictionaries with HFST and using them in HFST applications</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Language Technology for Normalisation of Less-Resourced Languages</span>, <span class="ltx_text ltx_bib_pages"> pp. 25–29</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.ix7" title="VII ‣ Original Papers ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><cite class="ltx_cite ltx_citemacro_citealias"><span class="ltx_ref">VII</span></cite></span></a>,
<a href="#S10.SS3.p2" title="10.3 Results ‣ 10 Using Rule-Based Machine Translation Dictionaries ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10.3</span></a>,
<a href="#S10.T3" title="Table 3 ‣ 10.3 Results ‣ 10 Using Rule-Based Machine Translation Dictionaries ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 3</span></a>,
<a href="#S4.SS1.p1" title="4.1 Chronological Order ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>,
<a href="#S4.SS2.p1" title="4.2 Division of Labour ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S4.SS3.p1" title="4.3 My Contributions ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.
</span>
</li>
<li id="bib.bib8a" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">T. A. Pirinen and S. Hardwick (2012)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Effect of language and error models on efficiency of finite-state spell-checking and correction</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 10th International Workshop on Finite State
Methods and Natural Language Processing</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Donostia–San SebastiÃ¡n</span>, <span class="ltx_text ltx_bib_pages"> pp. 1–9</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.aclweb.org/anthology/W12-6201" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.ix9" title="IX ‣ Original Papers ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><cite class="ltx_cite ltx_citemacro_citealias"><span class="ltx_ref">IX</span></cite></span></a>,
<a href="#S27.SS3.p1" title="27.3 Results ‣ 27 Speed of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">27.3</span></a>,
<a href="#S27.T5" title="Table 5 ‣ 27.3 Results ‣ 27 Speed of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a href="#S4.SS1.p1" title="4.1 Chronological Order ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>,
<a href="#S4.SS2.p1" title="4.2 Division of Labour ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S4.SS3.p1" title="4.3 My Contributions ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>.
</span>
</li>
<li id="bib.bib3a" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">T. A. Pirinen and K. LindÃ©n (2010b)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Building and using existing Hunspell dictionaries and TeX hyphenators as finite-state automata</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proccedings of Computational Linguistics - Applications, 2010</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Wisła, Poland</span>, <span class="ltx_text ltx_bib_pages"> pp. 25–32</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.helsinki.fi/%7Etapirine/publications/Pirinen-cla-2010.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.ix4" title="IV ‣ Original Papers ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><cite class="ltx_cite ltx_citemacro_citealias"><span class="ltx_ref">IV</span></cite></span></a>,
<a href="#S10.SS3.p3" title="10.3 Results ‣ 10 Using Rule-Based Machine Translation Dictionaries ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10.3</span></a>,
<a href="#S4.SS1.p1" title="4.1 Chronological Order ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>,
<a href="#S4.SS2.p1" title="4.2 Division of Labour ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S4.SS3.p1" title="4.3 My Contributions ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>,
<a href="#S9.SS3.p2" title="9.3 Results ‣ 9 Compiling Hunspell Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9.3</span></a>,
<a href="#S9.T2" title="Table 2 ‣ 9.3 Results ‣ 9 Compiling Hunspell Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
</span>
</li>
<li id="bib.bib4a" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">T. A. Pirinen and K. LindÃ©n (2010c)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Finite-state spell-checking with weighted language and error models</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Seventh SaLTMiL Workshop on Creation and Use of Basic Lexical Resources for Less-resourced Languages</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Valletta, Malta</span>, <span class="ltx_text ltx_bib_pages"> pp. 13–18</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://siuc01.si.ehu.es/%7Ejipsagak/SALTMIL2010%5C_Proceedings.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.ix3" title="III ‣ Original Papers ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><cite class="ltx_cite ltx_citemacro_citealias"><span class="ltx_ref">III</span></cite></span></a>,
<a href="#S20.p1" title="20 The Application of Finite-State Error Models ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a>,
<a href="#S4.SS1.p1" title="4.1 Chronological Order ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>,
<a href="#S4.SS2.p1" title="4.2 Division of Labour ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S4.SS3.p1" title="4.3 My Contributions ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>,
<a href="#S5.T1" title="Table 1 ‣ 5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a href="#S5.p14" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">T. A. Pirinen (2011)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Modularisation of Finnish finite-state language descriptionâtowards wide collaboration in open source development of morphological analyser</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of Nodalida</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">NEALT proceedings</span>, Vol. <span class="ltx_text ltx_bib_volume">18</span>, <span class="ltx_text ltx_bib_pages"> pp. 299–302</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.helsinki.fi/%5C%7etapirine/publications/Pirinen-nodalida-2011-omorfi.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.ix6" title="VI ‣ Original Papers ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><cite class="ltx_cite ltx_citemacro_citealias"><span class="ltx_ref">VI</span></cite></span></a>,
<a href="#S4.SS1.p1" title="4.1 Chronological Order ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>,
<a href="#S4.SS2.p1" title="4.2 Division of Labour ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">T. A. Pirinen (forthcoming)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Quality and speed trade-offs in weighted finite-state spell-checking</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">—</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">submitted for review</span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.ix10" title="X ‣ Original Papers ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><cite class="ltx_cite ltx_citemacro_citealias"><span class="ltx_ref">X</span></cite></span></a>,
<a href="#S28.SS2.p1" title="28.2 Results ‣ 28 Precision of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">28.2</span></a>,
<a href="#S28.T6" title="Table 6 ‣ 28.2 Results ‣ 28 Precision of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 6</span></a>,
<a href="#S4.SS1.p1" title="4.1 Chronological Order ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>,
<a href="#S4.SS2.p1" title="4.2 Division of Labour ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S5.p10" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib9a" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">T. Pirinen, M. Silfverberg and K. LindÃ©n (2012)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Improving finite-state spell-checker suggestions with part of speech n-grams</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Computational Linguistics and Intelligent Text Processing 13th International Conference, CICLing 2012</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.ix8" title="VIII ‣ Original Papers ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><cite class="ltx_cite ltx_citemacro_citealias"><span class="ltx_ref">VIII</span></cite></span></a>,
<a href="#S24.SS2.p1" title="24.2 Related Works ‣ 24 Context-Aware Spelling Correction ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24.2</span></a>,
<a href="#S24.SS3.p1" title="24.3 Results ‣ 24 Context-Aware Spelling Correction ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24.3</span></a>,
<a href="#S24.T4" title="Table 4 ‣ 24.3 Results ‣ 24 Context-Aware Spelling Correction ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a href="#S25.p2" title="25 Other Finite-State Error Models ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">25</span></a>,
<a href="#S32.p4" title="32 Future Work ‣ Chapter \thechapter Conclusion ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">32</span></a>,
<a href="#S32.p5" title="32 Future Work ‣ Chapter \thechapter Conclusion ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">32</span></a>,
<a href="#S4.SS1.p1" title="4.1 Chronological Order ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>,
<a href="#S4.SS2.p1" title="4.2 Division of Labour ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S4.SS3.p1" title="4.3 My Contributions ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>,
<a href="#S5.p13" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem ltx_bib_thesis">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">T. Pirinen (2008)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Suomen kielen Ã¤Ã¤rellistilainen automaattinen morfologinen analyysi avoimen lÃ¤hdekoodin menetelmin</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Master’s Thesis</span>, <span class="ltx_text ltx_bib_publisher">Helsingin yliopisto</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.helsinki.fi/%7Etapirine/gradu/" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S15.SS1.p1" title="15.1 Motivation ‣ 15 Language Models for Languages with Compounding ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15.1</span></a>,
<a href="#S4.SS1.p1" title="4.1 Chronological Order ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem ltx_bib_report">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">H. Pitkänen (2006)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Hunspell-in kesäkoodi 2006: final report</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_type">Technical report</span>
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">COSS.fi</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.puimula.org/htp/archive/kesakoodi2006-report.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p7" title="2 Morphological Complexity ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">J. Porta, J. Sancho and J. GÃ³mez (2013)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Edit transducers for spelling variation in Old Spanish</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the workshop on computational historical linguistics at NODALIDA 2013</span>,  <span class="ltx_text ltx_bib_editor">Ã. EyÃ¾Ã³rsson, L. Borin, D. Haug and E. RÃ¶gnvaldsson (Eds.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">LinkÃ¶ping Electronic Conference Proceedings</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-91-7519-587-2</span>,
<a href="http://www.ep.liu.se/ecp/087/ecp13087.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.p4" title="6 Related Subfields and Research Results ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
</span>
</li>
<li id="bib.bib97" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">A. Ranta (2008)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">How predictable is Finnish morphology? An experiment on lexicon construction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Resourceful Language Technology: Festschrift in Honor of Anna Sågvall Hein</span>, <span class="ltx_text ltx_bib_pages"> pp. 130–148</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S12.SS2.p1" title="12.2 Related Works ‣ 12 Maintenance of the Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12.2</span></a>.
</span>
</li>
<li id="bib.bib98" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">J. Raviv (1967)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Decision making in Markov chains applied to the problem of pattern recognition</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Information Theory, IEEE Transactions on</span> <span class="ltx_text ltx_bib_volume">13</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 536–551</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p5" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib102" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">R. Russell and M. Odell (1918)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Soundex</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">US Patent</span> <span class="ltx_text ltx_bib_volume">1261167</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S23.p1" title="23 Phonemic Key Corrections ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">23</span></a>,
<a href="#S5.p10" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib1a" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">A. Savary (2002)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Typographical nearest-neighbor search in a finite-state lexicon and its application to spelling correction</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">CIAA 2001: Revised Papers from the 6th International Conference on Implementation and Application of Automata</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">London, UK</span>, <span class="ltx_text ltx_bib_pages"> pp. 251–260</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 3-540-00400-9</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.T1" title="Table 1 ‣ 5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a href="#S5.p14" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>,
<a href="#S7.p5" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>,
<a href="#thechapter-at-IDf.p1" title="Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Chapter \thechapter</span></a>.
</span>
</li>
<li id="bib.bib103" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">A. Schiller (2006)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">German compound analysis with wfsc</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Finite-State Methods and Natural Language Processing</span>, <span class="ltx_text ltx_bib_pages"> pp. 239–246</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S15.SS2.p1" title="15.2 Related Works ‣ 15 Language Models for Languages with Compounding ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15.2</span></a>,
<a href="#S16.SS2.p1" title="16.2 Related Works ‣ 16 The Statistical Training of Compounding Language Models ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16.2</span></a>.
</span>
</li>
<li id="bib.bib105" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K. Schulz and S. Mihov (2002)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Fast string correction with Levenshtein-automata</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">International Journal of Document Analysis and Recognition</span> <span class="ltx_text ltx_bib_volume">5</span>, <span class="ltx_text ltx_bib_pages"> pp. 67–85</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S20.p1" title="20 The Application of Finite-State Error Models ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">20</span></a>,
<a href="#S21.p1" title="21 Edit Distance Measures ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">21</span></a>,
<a href="#S5.T1" title="Table 1 ‣ 5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>.
</span>
</li>
<li id="bib.bib106" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">C.E. Shannon (1948)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A mathematical theory of communications, I and II</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Bell Syst. Tech. J</span> <span class="ltx_text ltx_bib_volume">27</span>, <span class="ltx_text ltx_bib_pages"> pp. 379–423</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p5" title="5 Brief History and Prior Work ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
</span>
</li>
<li id="bib.bib108" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">M. Silfverberg and K. LindÃ©n (2009)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">HFST runtime format—a compacted transducer format allowing for fast lookup</span>.
</span>
<span class="ltx_bibblock">See <span class="ltx_text ltx_bib_crossref"><cite class="ltx_cite"><a href="#bib.bib126" title="Pre-proceedings of the eighth international workshop on finite-state methods and natural language processing (fsmnlp 2009), pretoria, south africa, july 21st - 24th 2009" class="ltx_ref">Pre-proceedings of the eighth international workshop on finite-state methods and natural language processing (fsmnlp 2009), pretoria, south africa, july 21st - 24th 2009, Watson<span class="ltx_text ltx_bib_etal"> et al.</span></a></cite></span>,
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.ling.helsinki.fi/klinden/pubs/fsmnlp2009runtime.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S10.SS3.p1" title="10.3 Results ‣ 10 Using Rule-Based Machine Translation Dictionaries ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10.3</span></a>,
<a href="#S27.SS1.p2" title="27.1 Motivation ‣ 27 Speed of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">27.1</span></a>.
</span>
</li>
<li id="bib.bib109" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">M. Silfverberg and K. LindÃ©n (2010)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Part-of-speech tagging using parallel weighted finite-state transducers</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Advances in Natural Language Processing — Proceedigns of the 7th International Conference on NLP, IceTAL 2010</span>,  <span class="ltx_text ltx_bib_editor">H. Loftsson, E. RÃ¶gnvaldsson and S. HelgadÃ³ttir (Eds.)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">6233</span>, <span class="ltx_text ltx_bib_place">Reykjavik, Iceland</span>, <span class="ltx_text ltx_bib_pages"> pp. 369–381</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-3-642-14769-2</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS2.p1" title="4.2 Division of Labour ‣ 4 Overview of Thesis Articles ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>,
<a href="#S6.p2" title="6 Related Subfields and Research Results ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.
</span>
</li>
<li id="bib.bib111" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">A. Stolcke (2002)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">SRILM-an extensible language modeling toolkit</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the International Conference on Spoken Language Processing</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">2</span>, <span class="ltx_text ltx_bib_pages"> pp. 901–904</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S16.SS2.p1" title="16.2 Related Works ‣ 16 The Statistical Training of Compounding Language Models ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16.2</span></a>.
</span>
</li>
<li id="bib.bib112" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">V. TrÃ³n, A. Kornai, G. Gyepesi, L. NÃ©meth, P. HalÃ¡csy and D.  (2005)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Hunmorph: open source word analysis</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Workshop on Software</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 77–85</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S27.SS3.p4" title="27.3 Results ‣ 27 Speed of Finite-State Spell-Checking ‣ Chapter \thechapter Efficiency of Finite-State Spell-Checking ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">27.3</span></a>,
<a href="#S9.SS2.p1" title="9.2 Related Works ‣ 9 Compiling Hunspell Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9.2</span></a>.
</span>
</li>
<li id="bib.bib113" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">Unicode Consortium (2013)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The Unicode standard</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">WWW pageRetrieved 1st of August 2013 (unversioned),
refered version 6.2.0; <span class="ltx_ERROR undefined">\url</span>http://www.unicode.org/versions/latest/</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-936213-07-8</span>,
<a href="http://www.unicode.org/versions/latest/" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S7.p2" title="7 Theory of Finite-State Models ‣ Chapter \thechapter Background ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span>
</li>
<li id="bib.bib114" class="ltx_bibitem ltx_bib_incollection">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">G. Van Noord and D. Gerdemann (2001)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">An extendible regular expression compiler for finite-state approaches in natural language processing</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Automata Implementation</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 122–139</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#thechapter-at-IDf.p1" title="Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Chapter \thechapter</span></a>.
</span>
</li>
<li id="bib.bib117" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">B.W. Watson (2003)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A new algorithm for the construction of minimal acyclic DFAs</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Science of Computer Programming</span> <span class="ltx_text ltx_bib_volume">48</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 81–97</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p5" title="2 Morphological Complexity ‣ Chapter \thechapter Introduction ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span>
</li>
<li id="bib.bib126" class="ltx_bibitem ltx_bib_proceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">B. Watson, D. Courie, L. Cleophas and P. Rautenbach (Eds.) (2009)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Pre-proceedings of the eighth international workshop on finite-state methods and natural language processing (fsmnlp 2009), pretoria, south africa, july 21st - 24th 2009</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series"></span>, Vol. <span class="ltx_text ltx_bib_volume"></span>,  <span class="ltx_text ltx_bib_publisher"></span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">Handout CD-ROM containing the accompanying papers for the presentations during the FSMNLP 2009 workshop. Published by the University of Pretoria, Pretoria, South Africa.</span>
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-86854-743-2</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#bib.bib108" title="HFST runtime format—a compacted transducer format allowing for fast lookup" class="ltx_ref">M. Silfverberg and K. LindÃ©n (2009)</a>.
</span>
</li>
<li id="bib.bib118" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">L. A. Wilcox-O’Hearn, G. Hirst and A. Budanitsky (2008)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Real-word spelling correction with trigrams: a reconsideration of the Mays, Damerau, and Mercer model</span>.
</span>
<span class="ltx_bibblock">See <span class="ltx_text ltx_bib_crossref"><cite class="ltx_cite"><a href="#bib.bib125" title="Computational linguistics and intelligent text processing, 9th international conference, cicling 2008, haifa, israel, february 17-23, 2008, proceedings" class="ltx_ref">Computational linguistics and intelligent text processing, 9th international conference, cicling 2008, haifa, israel, february 17-23, 2008, proceedings, Gelbukh</a></cite></span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 605–616</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S24.SS2.p1" title="24.2 Related Works ‣ 24 Context-Aware Spelling Correction ‣ Chapter \thechapter Error Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24.2</span></a>.
</span>
</li>
<li id="bib.bib119" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">S. Wintner (2008)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Strengths and weaknesses of finite-state technology: a case study in morphological grammar development</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nat. Lang. Eng.</span> <span class="ltx_text ltx_bib_volume">14</span>, <span class="ltx_text ltx_bib_pages"> pp. 457–469</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 1351-3249</span>,
<a href="http://portal.acm.org/citation.cfm?id=1520025.1520027" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1017/S1351324907004676" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S12.SS2.p1" title="12.2 Related Works ‣ 12 Maintenance of the Language Models ‣ Chapter \thechapter Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12.2</span></a>.
</span>
</li>
<li id="bib.bib121" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">A. Yli-Jyrä (2005)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Toward a widely usable finite-state morphology workbench for less studied languages, 1: desiderata</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nordic Journal of African Studies</span> <span class="ltx_text ltx_bib_volume">14</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 479–491</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S18.SS2.p1" title="18.2 Related Works ‣ 18 Lesser-Resourced Languages in Statistical Spelling Correction ‣ Chapter \thechapter Statistical Language Models ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">18.2</span></a>.
</span>
</li>
</ul>
</section>
<section id="thechapter-at-IDi" class="ltx_chapter">
<h2 class="ltx_title ltx_title_chapter">
<span class="ltx_tag ltx_tag_chapter">Chapter <span class="ltx_ERROR undefined">\thechapter</span> </span>Appendices</h2>

</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Copyrights of Introduction and Articles</h2>

<div id="A1.p1" class="ltx_para">
<p class="ltx_p">This introduction is published under <em class="ltx_emph">Creative Commons
Attribution–NonCommercial-NoDerivs 3.0 Unported</em> licence, which can be found in
Appendix <a href="#A3" title="Appendix C Licence ‣ Weighted Finite-State Methods for Spell-Checking and Correction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Original Articles</h2>

<div id="A2.p1" class="ltx_para">
<p class="ltx_p">This electronic publication does not contain reproductions of the original
articles. The viewers of this electronic
version of the dissertation should obtain the articles refered from my
homepage<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">40</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">40</sup><a href="http://www.helsinki.fi/%7etapirine/publications" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.helsinki.fi/%7etapirine/publications</a></span></span></span> or
search for the articles using Google
Scholar.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">41</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">41</sup><a href="http://scholar.google.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://scholar.google.com</a></span></span></span></p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Licence</h2>

<div id="A3.p1" class="ltx_para">
<p class="ltx_p">This text is <em class="ltx_emph">Creative Commons Attribution-NonCommercial-NoDerivs 3.0
Unported</em>.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">42</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">42</sup><a href="http://creativecommons.org/licenses/by-nc-nd/3.0/legalcode" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://creativecommons.org/licenses/by-nc-nd/3.0/legalcode</a></span></span></span></p>
</div>
<div id="A3.p2" class="ltx_para">
<p class="ltx_p">CREATIVE COMMONS CORPORATION IS NOT A LAW FIRM AND DOES NOT PROVIDE LEGAL
SERVICES. DISTRIBUTION OF THIS LICENSE DOES NOT CREATE AN ATTORNEY-CLIENT
RELATIONSHIP. CREATIVE COMMONS PROVIDES THIS INFORMATION ON AN ”AS-IS” BASIS.
CREATIVE COMMONS MAKES NO WARRANTIES REGARDING THE INFORMATION PROVIDED, AND
DISCLAIMS LIABILITY FOR DAMAGES RESULTING FROM ITS USE. License</p>
</div>
<div id="A3.p3" class="ltx_para">
<p class="ltx_p">THE WORK (AS DEFINED BELOW) IS PROVIDED UNDER THE TERMS OF THIS CREATIVE
COMMONS PUBLIC LICENSE (”CCPL” OR ”LICENSE”). THE WORK IS PROTECTED BY
COPYRIGHT AND/OR OTHER APPLICABLE LAW. ANY USE OF THE WORK OTHER THAN AS
AUTHORIZED UNDER THIS LICENSE OR COPYRIGHT LAW IS PROHIBITED.</p>
</div>
<div id="A3.p4" class="ltx_para">
<p class="ltx_p">BY EXERCISING ANY RIGHTS TO THE WORK PROVIDED HERE, YOU ACCEPT AND AGREE TO BE
BOUND BY THE TERMS OF THIS LICENSE. TO THE EXTENT THIS LICENSE MAY BE
CONSIDERED TO BE A CONTRACT, THE LICENSOR GRANTS YOU THE RIGHTS CONTAINED HERE
IN CONSIDERATION OF YOUR ACCEPTANCE OF SUCH TERMS AND CONDITIONS.</p>
</div>
<div id="A3.p5" class="ltx_para">
<ol id="I3" class="ltx_enumerate">
<li id="I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">1.</span>
<div id="I3.i1.p1" class="ltx_para">
<p class="ltx_p">Definitions</p>
</div>
<div id="I3.i1.p2" class="ltx_para">
<ol id="I3.I1" class="ltx_enumerate">
<li id="I3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(a)</span>
<div id="I3.I1.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">“Adaptation”</span> means a work based upon the Work, or
upon the Work and other pre-existing works, such as a
translation, adaptation, derivative work, arrangement of music
or other alterations of a literary or artistic work, or
phonogram or performance and includes cinematographic
adaptations or any other form in which the Work may be recast,
transformed, or adapted including in any form recognizably
derived from the original, except that a work that constitutes
a Collection will not be considered an Adaptation for the
purpose of this License. For the avoidance of doubt, where the
Work is a musical work, performance or phonogram, the
synchronization of the Work in timed-relation with a moving
image (”synching”) will be considered an Adaptation for the
purpose of this License.</p>
</div>
</li>
<li id="I3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(b)</span>
<div id="I3.I1.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">“Collection”</span> means a collection of literary or
artistic works, such as encyclopedias and anthologies, or
performances, phonograms or broadcasts, or other works or
subject matter other than works listed in Section 1(f) below,
which, by reason of the selection and arrangement of their
contents, constitute intellectual creations, in which the Work
is included in its entirety in unmodified form along with one
or more other contributions, each constituting separate and
independent works in themselves, which together are assembled
into a collective whole. A work that constitutes a Collection
will not be considered an Adaptation (as defined above) for the
purposes of this License.</p>
</div>
</li>
<li id="I3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(c)</span>
<div id="I3.I1.i3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">“Distribute”</span> means to make available to the public
the original and copies of the Work through sale or other
transfer of ownership.</p>
</div>
</li>
<li id="I3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(d)</span>
<div id="I3.I1.i4.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">“Licensor”</span> means the individual, individuals,
entity or entities that offer(s) the Work under the terms of
this License.</p>
</div>
</li>
<li id="I3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(e)</span>
<div id="I3.I1.i5.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">“Original Author”</span> means, in the case of a literary
or artistic work, the individual, individuals, entity or
entities who created the Work or if no individual or entity can
be identified, the publisher; and in addition (i) in the case
of a performance the actors, singers, musicians, dancers, and
other persons who act, sing, deliver, declaim, play in,
interpret or otherwise perform literary or artistic works or
expressions of folklore; (ii) in the case of a phonogram the
producer being the person or legal entity who first fixes the
sounds of a performance or other sounds; and, (iii) in the case
of broadcasts, the organization that transmits the broadcast.</p>
</div>
</li>
<li id="I3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(f)</span>
<div id="I3.I1.i6.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">“Work”</span> means the literary and/or artistic work
offered under the terms of this License including without
limitation any production in the literary, scientific and
artistic domain, whatever may be the mode or form of its
expression including digital form, such as a book, pamphlet and
other writing; a lecture, address, sermon or other work of the
same nature; a dramatic or dramatico-musical work; a
choreographic work or entertainment in dumb show; a musical
composition with or without words; a cinematographic work to
which are assimilated works expressed by a process analogous to
cinematography; a work of drawing, painting, architecture,
sculpture, engraving or lithography; a photographic work to
which are assimilated works expressed by a process analogous to
photography; a work of applied art; an illustration, map, plan,
sketch or three-dimensional work relative to geography,
topography, architecture or science; a performance; a
broadcast; a phonogram; a compilation of data to the extent it
is protected as a copyrightable work; or a work performed by a
variety or circus performer to the extent it is not otherwise
considered a literary or artistic work.</p>
</div>
</li>
<li id="I3.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(g)</span>
<div id="I3.I1.i7.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">“You”</span> means an individual or entity exercising
rights under this License who has not previously violated the
terms of this License with respect to the Work, or who has
received express permission from the Licensor to exercise
rights under this License despite a previous violation.
</p>
</div>
</li>
<li id="I3.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(h)</span>
<div id="I3.I1.i8.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">“Publicly Perform”</span> means to perform public
recitations of the Work and to communicate to the public those
public recitations, by any means or process, including by wire
or wireless means or public digital performances; to make
available to the public Works in such a way that members of the
public may access these Works from a place and at a place
individually chosen by them; to perform the Work to the public
by any means or process and the communication to the public of
the performances of the Work, including by public digital
performance; to broadcast and rebroadcast the Work by any means
including signs, sounds or images.</p>
</div>
</li>
<li id="I3.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(i)</span>
<div id="I3.I1.i9.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">“Reproduce”</span> means to make copies of the Work by
any means including without limitation by sound or visual
recordings and the right of fixation and reproducing fixations
of the Work, including storage of a protected performance or
phonogram in digital form or other electronic medium.</p>
</div>
</li>
</ol>
</div>
</li>
<li id="I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">2.</span>
<div id="I3.i2.p1" class="ltx_para">
<p class="ltx_p">Fair Dealing Rights. Nothing in this License is intended to reduce,
limit, or restrict any uses free from copyright or rights arising from
limitations or exceptions that are provided for in connection with the
copyright protection under copyright law or other applicable laws.
</p>
</div>
</li>
<li id="I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">3.</span>
<div id="I3.i3.p1" class="ltx_para">
<p class="ltx_p">License Grant. Subject to the terms and conditions of this License,
Licensor hereby grants You a worldwide, royalty-free, non-exclusive,
perpetual (for the duration of the applicable copyright) license to
exercise the rights in the Work as stated below:</p>
</div>
<div id="I3.i3.p2" class="ltx_para">
<ol id="I3.I2" class="ltx_enumerate">
<li id="I3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(a)</span>
<div id="I3.I2.i1.p1" class="ltx_para">
<p class="ltx_p">to Reproduce the Work, to incorporate the Work into one or
more Collections, and to Reproduce the Work as incorporated in
the Collections; and,</p>
</div>
</li>
<li id="I3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(b)</span>
<div id="I3.I2.i2.p1" class="ltx_para">
<p class="ltx_p">to Distribute and Publicly Perform the Work including as
incorporated in Collections.</p>
</div>
</li>
</ol>
</div>
<div id="I3.i3.p3" class="ltx_para">
<p class="ltx_p">The above rights may be exercised in all media and formats whether now known or
hereafter devised. The above rights include the right to make such
modifications as are technically necessary to exercise the rights in other
media and formats, but otherwise you have no rights to make Adaptations.
Subject to 8(f), all rights not expressly granted by Licensor are hereby
reserved, including but not limited to the rights set forth in Section 4(d).</p>
</div>
</li>
<li id="I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">4.</span>
<div id="I3.i4.p1" class="ltx_para">
<p class="ltx_p">Restrictions. The license granted in Section 3 above is expressly
made subject to and limited by the following restrictions:</p>
</div>
<div id="I3.i4.p2" class="ltx_para">
<ol id="I3.I3" class="ltx_enumerate">
<li id="I3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(a)</span>
<div id="I3.I3.i1.p1" class="ltx_para">
<p class="ltx_p">You may Distribute or Publicly Perform the Work only under
the terms of this License. You must include a copy of, or the
Uniform Resource Identifier (URI) for, this License with every
copy of the Work You Distribute or Publicly Perform. You may
not offer or impose any terms on the Work that restrict the
terms of this License or the ability of the recipient of the
Work to exercise the rights granted to that recipient under the
terms of the License. You may not sublicense the Work. You must
keep intact all notices that refer to this License and to the
disclaimer of warranties with every copy of the Work You
Distribute or Publicly Perform. When You Distribute or Publicly
Perform the Work, You may not impose any effective
technological measures on the Work that restrict the ability of
a recipient of the Work from You to exercise the rights granted
to that recipient under the terms of the License. This Section
4(a) applies to the Work as incorporated in a Collection, but
this does not require the Collection apart from the Work itself
to be made subject to the terms of this License. If You create
a Collection, upon notice from any Licensor You must, to the
extent practicable, remove from the Collection any credit as
required by Section 4(c), as requested.
</p>
</div>
</li>
<li id="I3.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(b)</span>
<div id="I3.I3.i2.p1" class="ltx_para">
<p class="ltx_p">You may not exercise any of the rights granted to You in
Section 3 above in any manner that is primarily intended for or
directed toward commercial advantage or private monetary
compensation. The exchange of the Work for other copyrighted
works by means of digital file-sharing or otherwise shall not
be considered to be intended for or directed toward commercial
advantage or private monetary compensation, provided there is
no payment of any monetary compensation in connection with the
exchange of copyrighted works.</p>
</div>
</li>
<li id="I3.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(c)</span>
<div id="I3.I3.i3.p1" class="ltx_para">
<p class="ltx_p">If You Distribute, or Publicly Perform the Work or
Collections, You must, unless a request has been made pursuant
to Section 4(a), keep intact all copyright notices for the Work
and provide, reasonable to the medium or means You are
utilizing: (i) the name of the Original Author (or pseudonym,
if applicable) if supplied, and/or if the Original Author
and/or Licensor designate another party or parties (e.g., a
sponsor institute, publishing entity, journal) for attribution
(”Attribution Parties”) in Licensor’s copyright notice, terms
of service or by other reasonable means, the name of such party
or parties; (ii) the title of the Work if supplied; (iii) to
the extent reasonably practicable, the URI, if any, that
Licensor specifies to be associated with the Work, unless such
URI does not refer to the copyright notice or licensing
information for the Work. The credit required by this Section
4(c) may be implemented in any reasonable manner; provided,
however, that in the case of a Collection, at a minimum such
credit will appear, if a credit for all contributing authors of
Collection appears, then as part of these credits and in a
manner at least as prominent as the credits for the other
contributing authors. For the avoidance of doubt, You may only
use the credit required by this Section for the purpose of
attribution in the manner set out above and, by exercising Your
rights under this License, You may not implicitly or explicitly
assert or imply any connection with, sponsorship or endorsement
by the Original Author, Licensor and/or Attribution Parties, as
appropriate, of You or Your use of the Work, without the
separate, express prior written permission of the Original
Author, Licensor and/or Attribution Parties.</p>
</div>
</li>
<li id="I3.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(d)</span>
<div id="I3.I3.i4.p1" class="ltx_para">
<p class="ltx_p">For the avoidance of doubt:</p>
</div>
<div id="I3.I3.i4.p2" class="ltx_para">
<ol id="I3.I3.I1" class="ltx_enumerate">
<li id="I3.I3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">i.</span>
<div id="I3.I3.I1.i1.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Non-waivable Compulsory License Schemes.</span> In
those jurisdictions in which the right to collect
royalties through any statutory or compulsory licensing
scheme cannot be waived, the Licensor reserves the
exclusive right to collect such royalties for any
exercise by You of the rights granted under this
License;</p>
</div>
</li>
<li id="I3.I3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">ii.</span>
<div id="I3.I3.I1.i2.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Waivable Compulsory License Schemes.</span> In
those jurisdictions in which the right to collect
royalties through any statutory or compulsory licensing
scheme can be waived, the Licensor reserves the
exclusive right to collect such royalties for any
exercise by You of the rights granted under this
License if Your exercise of such rights is for a
purpose or use which is otherwise than noncommercial as
permitted under Section 4(b) and otherwise waives the
right to collect royalties through any statutory or
compulsory licensing scheme; and,</p>
</div>
</li>
<li id="I3.I3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">iii.</span>
<div id="I3.I3.I1.i3.p1" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Voluntary License Schemes.</span> The Licensor
reserves the right to collect royalties, whether
individually or, in the event that the Licensor is a
member of a collecting society that administers
voluntary licensing schemes, via that society, from any
exercise by You of the rights granted under this
License that is for a purpose or use which is otherwise
than noncommercial as permitted under Section 4(b).</p>
</div>
</li>
</ol>
</div>
</li>
<li id="I3.I3.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(e)</span>
<div id="I3.I3.i5.p1" class="ltx_para">
<p class="ltx_p">Except as otherwise agreed in writing by the Licensor or as
may be otherwise permitted by applicable law, if You Reproduce,
Distribute or Publicly Perform the Work either by itself or as
part of any Collections, You must not distort, mutilate, modify
or take other derogatory action in relation to the Work which
would be prejudicial to the Original Author’s honor or
reputation.</p>
</div>
</li>
</ol>
</div>
</li>
<li id="I3.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">5.</span>
<div id="I3.i5.p1" class="ltx_para">
<p class="ltx_p">Representations, Warranties and Disclaimer</p>
</div>
<div id="I3.i5.p2" class="ltx_para">
<p class="ltx_p">UNLESS OTHERWISE MUTUALLY AGREED BY THE PARTIES IN WRITING, LICENSOR OFFERS THE
WORK AS-IS AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING
THE WORK, EXPRESS, IMPLIED, STATUTORY OR OTHERWISE, INCLUDING, WITHOUT
LIMITATION, WARRANTIES OF TITLE, MERCHANTIBILITY, FITNESS FOR A PARTICULAR
PURPOSE, NONINFRINGEMENT, OR THE ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY,
OR THE PRESENCE OF ABSENCE OF ERRORS, WHETHER OR NOT DISCOVERABLE. SOME
JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES, SO SUCH
EXCLUSION MAY NOT APPLY TO YOU.</p>
</div>
</li>
<li id="I3.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">6.</span>
<div id="I3.i6.p1" class="ltx_para">
<p class="ltx_p">Limitation on Liability. EXCEPT TO THE EXTENT REQUIRED BY APPLICABLE
LAW, IN NO EVENT WILL LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY FOR
ANY SPECIAL, INCIDENTAL, CONSEQUENTIAL, PUNITIVE OR EXEMPLARY DAMAGES
ARISING OUT OF THIS LICENSE OR THE USE OF THE WORK, EVEN IF LICENSOR
HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p>
</div>
</li>
<li id="I3.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">7.</span>
<div id="I3.i7.p1" class="ltx_para">
<p class="ltx_p">Termination</p>
</div>
<div id="I3.i7.p2" class="ltx_para">
<ol id="I3.I4" class="ltx_enumerate">
<li id="I3.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(a)</span>
<div id="I3.I4.i1.p1" class="ltx_para">
<p class="ltx_p">This License and the rights granted hereunder will terminate
automatically upon any breach by You of the terms of this
License. Individuals or entities who have received Collections
from You under this License, however, will not have their
licenses terminated provided such individuals or entities
remain in full compliance with those licenses. Sections 1, 2,
5, 6, 7, and 8 will survive any termination of this License.</p>
</div>
</li>
<li id="I3.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(b)</span>
<div id="I3.I4.i2.p1" class="ltx_para">
<p class="ltx_p">Subject to the above terms and conditions, the license
granted here is perpetual (for the duration of the applicable
copyright in the Work). Notwithstanding the above, Licensor
reserves the right to release the Work under different license
terms or to stop distributing the Work at any time; provided,
however that any such election will not serve to withdraw this
License (or any other license that has been, or is required to
be, granted under the terms of this License), and this License
will continue in full force and effect unless terminated as
stated above.
</p>
</div>
</li>
</ol>
</div>
</li>
<li id="I3.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">8.</span>
<div id="I3.i8.p1" class="ltx_para">
<p class="ltx_p">Miscellaneous</p>
</div>
<div id="I3.i8.p2" class="ltx_para">
<ol id="I3.I5" class="ltx_enumerate">
<li id="I3.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(a)</span>
<div id="I3.I5.i1.p1" class="ltx_para">
<p class="ltx_p">Each time You Distribute or Publicly Perform the Work or a
Collection, the Licensor offers to the recipient a license to
the Work on the same terms and conditions as the license
granted to You under this License.</p>
</div>
</li>
<li id="I3.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(b)</span>
<div id="I3.I5.i2.p1" class="ltx_para">
<p class="ltx_p">If any provision of this License is invalid or unenforceable
under applicable law, it shall not affect the validity or
enforceability of the remainder of the terms of this License,
and without further action by the parties to this agreement,
such provision shall be reformed to the minimum extent
necessary to make such provision valid and enforceable.</p>
</div>
</li>
<li id="I3.I5.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(c)</span>
<div id="I3.I5.i3.p1" class="ltx_para">
<p class="ltx_p">No term or provision of this License shall be deemed waived
and no breach consented to unless such waiver or consent shall
be in writing and signed by the party to be charged with such
waiver or consent.</p>
</div>
</li>
<li id="I3.I5.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(d)</span>
<div id="I3.I5.i4.p1" class="ltx_para">
<p class="ltx_p">This License constitutes the entire agreement between the
parties with respect to the Work licensed here. There are no
understandings, agreements or representations with respect to
the Work not specified here. Licensor shall not be bound by any
additional provisions that may appear in any communication from
You. This License may not be modified without the mutual
written agreement of the Licensor and You.</p>
</div>
</li>
<li id="I3.I5.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_enumerate">(e)</span>
<div id="I3.I5.i5.p1" class="ltx_para">
<p class="ltx_p">The rights granted under, and the subject matter referenced,
in this License were drafted utilizing the terminology of the
Berne Convention for the Protection of Literary and Artistic
Works (as amended on September 28, 1979), the Rome Convention
of 1961, the WIPO Copyright Treaty of 1996, the WIPO
Performances and Phonograms Treaty of 1996 and the Universal
Copyright Convention (as revised on July 24, 1971). These
rights and subject matter take effect in the relevant
jurisdiction in which the License terms are sought to be
enforced according to the corresponding provisions of the
implementation of those treaty provisions in the applicable
national law. If the standard suite of rights granted under
applicable copyright law includes additional rights not granted
under this License, such additional rights are deemed to be
included in the License; this License is not intended to
restrict the license of any rights under applicable law.</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div id="A3.p6" class="ltx_para">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Creative Commons Notice</span></p>
</div>
<div id="A3.p7" class="ltx_para">
<p class="ltx_p">Creative Commons is not a party to this License, and makes no warranty
whatsoever in connection with the Work. Creative Commons will not be liable to
You or any party on any legal theory for any damages whatsoever, including
without limitation any general, special, incidental or consequential damages
arising in connection to this license. Notwithstanding the foregoing two (2)
sentences, if Creative Commons has expressly identified itself as the Licensor
hereunder, it shall have all rights and obligations of Licensor.</p>
</div>
<div id="A3.p8" class="ltx_para">
<p class="ltx_p">Except for the limited purpose of indicating to the public that the Work is
licensed under the CCPL, Creative Commons does not authorize the use by either
party of the trademark ”Creative Commons” or any related trademark or logo of
Creative Commons without the prior written consent of Creative Commons. Any
permitted use will be in compliance with Creative Commons’ then-current
trademark usage guidelines, as may be published on its website or otherwise
made available upon request from time to time. For the avoidance of doubt, this
trademark restriction does not form part of this License.</p>
</div>
<div id="A3.p9" class="ltx_para">
<p class="ltx_p">Creative Commons may be contacted at <a href="http://creativecommons.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://creativecommons.org/</a>.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 28 15:53:50 2017 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>
</body>
</html>

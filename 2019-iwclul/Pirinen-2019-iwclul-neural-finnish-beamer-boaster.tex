\documentclass{beamer}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{fontspec}
\usepackage{polyglossia}
\usepackage{graphicx}
\usepackage{color}
\usepackage{url}
\usepackage{textpos}
\usepackage{xspace}
\usepackage{array}

\title{Neural vs. Rule-Based Finnish\\
\scriptsize{in/at IWCLUL 2019, Tartu}}
\author{Tommi A Pirinen\scriptsize
\guilsinglleft{}tommi.antero.pirinen@uni-hamburg.de\guilsinglright{}}
\institute{Universität Hamburg\\
Hamburger Zentrum für Sprachkorpora}
\date{\today}

\begin{document}

\selectlanguage{english}

\maketitle

\begin{frame}
    \frametitle{Initial experiments with Rule-based and Neural models}
    \begin{itemize}
        \item Omorfi is well established by now
        \item Turku Neural Parsing Pipeline has better results
        \item Neural machine translation is supposed to be good out of the box
        \item Apertium-fin-deu is \ldots there
    \end{itemize}
\end{frame}


\begin{frame}
    \frametitle{Evaluations with Shared Tasks! Why?}
    \begin{itemize}
        \item Surely old rule-based methods are still better in basic tasks like
            morphology\ldots?
        \item it turns out not according to the shared task metrics---without
            tuning at least
        \item (one could check other metrics, recall and then F score, to get
            potentially different results)
        \item I turned shared tasks into \textit{Continuous integration tests}
            for omorfi
        \item Translation quality out-of-the-box is not good
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Poster looks like this:}
    \includegraphics[height=0.8\textheight]{%
        Pirinen-2019-iwclul-neural-finnish-poster.pdf}
\end{frame}

\end{document}
% vim: set spell:

<!DOCTYPE html><html>
<head>
<title>Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/    </title>
<!--Generated on Fri Dec 21 17:16:26 2018 by LaTeXML (version 0.8.2) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on .-->

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../latexml/LaTeXML.css" type="text/css">
<link rel="stylesheet" href="../latexml/ltx-article.css" type="text/css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Neural and rule-based Finnish NLP models—expectations, experiments and
experiences<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>This
work is licensed under a Creative Commons Attribution
4.0 International Licence. Licence details:
<a href="http://creativecommons.org/licenses/by/4.0/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://creativecommons.org/licenses/by/4.0/</a>
</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tommi A Pirinen
<br class="ltx_break">Universität Hamburg
<br class="ltx_break">Hamburger Zentrum für Sprachkorpora
<br class="ltx_break">Max-Brauer-Allee 60, D-22765 Hamburg
<br class="ltx_break"><a href="tommi.antero.pirinen@uni-hamburg.de" title="" class="ltx_ref ltx_url ltx_font_typewriter">tommi.antero.pirinen@uni-hamburg.de</a>

</span></span>
</div>
<div class="ltx_date ltx_role_creation"></div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
    
<p class="ltx_p">In this article I take a critical look at some recent results in the
field of neural language modeling of Finnish in terms of popular shared
tasks. One novel point of view I present is comparing the neural methods’
results to traditional rule-based systems for the given tasks, since most of
the shared tasks have concentrated on the supervised learning
concept. The shared task results I re-evaluate, are morphological
regeneration by SIGMORPHON 2016, universal dependency parsing by CONLL-2018
and a machine translation application that imitates WMT 2018 for German
instead of English. The Uralic language used throughout is Finnish. I use
out of the box, best performing neural systems and rule-based systems and
evaluate their results.</p>
  
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Tiivistelmä</h6>
    
<p class="ltx_p">Tässä artikkelissa tarkastelemme joitain hiljattaisia tuloksia
niinkutsutuissa shared task -kilpailuissa suomen kielen hemroverkkomallien
osalta. Yksi tämän artikkelin kontribuutioista on hermoverkkomallien
tuottamien tulosten vertailu perinteisiin sääntöpohjaisiin
kielimallituloksiin, sillä shared task -kisailut pääosin keskittyvät täysin
tai osittain hallitsemattomien mallien oppimisen konseptiin. Shared taskit
joita tässä artikkelissa tarkastelemme ovat SIGMORPHONin 2016 morfologisen
uudelleengeneroinnin kisa, CONLL:n 2018 jäsennyskilpailu sekä WMT 2018:n
konekäännöskilpailu uudelleensovellettuna saksan kielelle. Uralilainen kieli
jota käytämme kaikissa kokeissa on suomi. Järjestelmät joita käytetään ovat
avoimen lähdekoodin järjestelmiä jotka ovat olleet parhaita näissä
kilpailuissa.</p>
  
</div>

<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">The popularity of the neural networks in natural language processing is at the
moment climbing very rapidly to the extent that we commonly get to hear that
non-neural methods should be abandoned. While naturally the majority of this
hype is based on English-centric or mostly European NLP, there are some reports
of good successes within the less resourced and more morphological languages,
including Uralic languages. In this paper I compare directly the
state-of-the-art methods between the neural and rule-based language processing
for Finnish. I specifically devised experiments based on the following shared
tasks and popular systems:</p>
</div>
<div id="S1.p2" class="ltx_para">
<ul id="I1" class="ltx_itemize">
<li id="I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i1.p1" class="ltx_para">
<p class="ltx_p">Generating morphology: Sigmorphon 2016
results <cite class="ltx_cite ltx_citemacro_citep">(Cotterell<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib29" title="The sigmorphon 2016 shared task—morphological reinflection" class="ltx_ref">2016</a>)</cite> vs. omorfi <cite class="ltx_cite ltx_citemacro_citep">(Pirinen, <a href="#bib.bib123" title="Development and use of computational morphology of finnish in the open source and open science era: notes on experiences with omorfi development." class="ltx_ref">2015a</a>)</cite></p>
</div>
</li>
<li id="I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i2.p1" class="ltx_para">
<p class="ltx_p">Parsing of morphosyntax: Turku neural parser <cite class="ltx_cite ltx_citemacro_citep">(Kanerva<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib56" title="Turku neural parser pipeline: an end-to-end system for the conll 2018 shared task" class="ltx_ref">2018</a>)</cite>
vs. omorfi <cite class="ltx_cite ltx_citemacro_citep">(Pirinen, <a href="#bib.bib123" title="Development and use of computational morphology of finnish in the open source and open science era: notes on experiences with omorfi development." class="ltx_ref">2015a</a>)</cite></p>
</div>
</li>
<li id="I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_itemize">•</span> 
<div id="I1.i3.p1" class="ltx_para">
<p class="ltx_p">Machine translation between Finnish and German:
OpenNMT <cite class="ltx_cite ltx_citemacro_citep">(Klein<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib65" title="OpenNMT: open-source toolkit for neural machine translation" class="ltx_ref">2017</a>)</cite> vs. apertium-fin-deu <cite class="ltx_cite ltx_citemacro_citep">(Pirinen, <a href="#bib.bib124" title="Rule-based machine-translation between finnish and german" class="ltx_ref">2018</a>)</cite></p>
</div>
</li>
</ul>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">Comparing a few different tasks gives us a good overview of the state of the art
in the neural processing of Finnish. Parsing tasks give an idea of the potential
usability of the language models in various linguistic tasks, such as corpus
annotation, whereas the machine translation task provides an important view on
the full capacity of the models for a more wide-ranging language understanding
task.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p class="ltx_p">One of the contributions of this paper is to gain more insight of the
similarities and differences of the traditional rule-based systems for the given
tasks, since the shared tasks are virtually always earmarked for more or less
supervised language learning, any evaluations between the neural and the
rule-based systems are not so commonly found in the literature.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p class="ltx_p">The rest of article is organised as follows: in Section <a href="#S2" title="2 Background ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>,
I introduce the shared tasks and rule-based systems at large, in
Section <a href="#S3" title="3 Methods and Datasets ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, I describe the systems used for the experiments,
in Section <a href="#S4" title="4 Experimental setup ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, I describe the system setup, in
Section <a href="#S5" title="5 Evaluation ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, I go through the experiments and results,
in Section <a href="#S6" title="6 Error Analysis ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, I perform the error analysis, in
Section <a href="#S7" title="7 Discussion ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, I relate the results to current state of the
art as well as practical usage and development of the systems and finally
in Section <a href="#S8" title="8 Conclusion ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, I summarise the findings.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">In recent years the neural network-based systems, especially so-called deep
neural systems, have been brought forward as a solution to all natural language
processing problems. Some results have also been provided for Uralic languages.
In the case of morphology, there was a popular task of morphological generation
as a shared task of the ACL 2016 SIGMORPHON
workshop <cite class="ltx_cite ltx_citemacro_citep">(Cotterell<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib29" title="The sigmorphon 2016 shared task—morphological reinflection" class="ltx_ref">2016</a>)</cite>, which included the Finnish generation,
and showed some very promising results. In the context of the machine
translation, the shared task of the WMT conference has had a Finnish task since
2015, and since 2017 the participants have predominantly been the neural systems
(e.g. for 2018 c.f <cite class="ltx_cite ltx_citemacro_citet">Bojar<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib150" title="Findings of the 2018 conference on machine translation (wmt18)" class="ltx_ref">2018</a>)</cite>). For the morphosyntax, the popular
shared task to test a parser with, is the CONLL task on the dependency
parsing <cite class="ltx_cite ltx_citemacro_citep">(Zeman<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib153" title="CoNLL 2018 shared task: multilingual parsing from raw text to universal dependencies" class="ltx_ref">2018</a>)</cite>. What is common with these shared tasks, is that
they are aimed for supervised learning of such language models, while in the
Uralic NLP the predominant methodology is rule-based, expert-written
systems <cite class="ltx_cite ltx_citemacro_citep">(Moshagen<span class="ltx_text ltx_bib_etal"> et al.</span>, <a href="#bib.bib96" title="Open-source infrastructures for collaborative work on under-resourced languages" class="ltx_ref">2014</a>)</cite>. In this article, I take a practical comparison
of building and using the systems for the given tasks as well as a tool in
actual linguistic research.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods and Datasets</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">Omorfi<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><a href="https://flammie.github.io/omorfi/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://flammie.github.io/omorfi/</a></span></span></span> is a lexical database
of Finnish, that can be compiled into a finite-state automaton for efficient
parsing. Omorfi has wide support for morphological analysis and generation
(matching the SIGMORPHON task of morphological regeneration) and parsing
(matching the CONLL task for parsing).
Apertium-fin-deu<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><a href="https://apertium.github.io/apertium-fin-deu" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://apertium.github.io/apertium-fin-deu</a></span></span></span> is
a hand-crafted rule-based machine translation system based on omorfi, with an
addition of a bilingual dictionary and some sets of bilingual rules. This can be
used with the apertium tools to translate between German and Finnish.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">The default mode of operation in a rule-based system is often based on the
concept of <span class="ltx_text ltx_font_italic">all possible hypotheses</span>, this is in contrast to shared
tasks, which are based on 1-best parsing instead; measuring the results is based
on only a single hypothesis per token. To bridge this gap between rule-based
morphology and shared tasks, I have used a combination of popular strategies
implemented with python scripting
language.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><a href="https://github.com/flammie/omorfi/tree/develop/src/python" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/flammie/omorfi/tree/develop/src/python</a></span></span></span>
These strategies build in principle on both constraint
grammar <cite class="ltx_cite ltx_citemacro_citep">(Karlsson, <a href="#bib.bib58" title="Constraint grammar as a framework for parsing unrestricted text" class="ltx_ref">1990</a>; Pirinen, <a href="#bib.bib121" title="Using weighted finite state morphology with visl cg-3-some experiments with free open source finnish resources" class="ltx_ref">2015b</a>)</cite> and my previous
experiences with unigram models in rule-based
morphologies <cite class="ltx_cite ltx_citemacro_citep">(LindÃ©n and Pirinen, <a href="#bib.bib112" title="Weighting finite-state morphological analyzers using HFST tools" class="ltx_ref">2009</a>)</cite>, it may, however, be noteworthy that
at the time of the writing the solution described is very much a work in
progress, so it should not be understood as having any specific advances over
the above-referred previous experiments yet. Furthermore, to perform the
SIGMORPHON and CONLL tasks I have written small python scripts to analyse and
map the analyses between omorfi’s formats and theirs. For machine translation I
use the <span class="ltx_text ltx_font_typewriter">apertium</span> command and discard the debugging symbols. Examples of
the output mangling we perform can be seen in listing <a href="#S3.F1" title="Figure 1 ‣ 3 Methods and Datasets ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
As can be seen in the example, the token 7 (<span class="ltx_text ltx_font_italic">2017</span>) has no rule-based
dependency analysis, since it is not covered by the very basic dependency
labeling script we use.
</p>
</div>
<figure id="S3.F1" class="ltx_figure"><pre class="ltx_verbatim ltx_font_typewriter" style="font-size:50%;">$ omorfi-analyse-text.sh -X test/test.text
Juankosken   WORD_ID=Juankoski UPOS=PROPN PROPER=GEO NUM=SG CASE=GEN
Juankosken   WORD_ID=juan UPOS=NOUN SEM=CURRENCY NUM=SG CASE=NOM
        BOUNDARY=COMPOUND WORD_ID=koski UPOS=NOUN NUM=SG CASE=GEN

kaupunki    WORD_ID=kaupunki UPOS=NOUN NUM=SG CASE=NOM

liittyy    WORD_ID=liittyä UPOS=VERB VOICE=ACT MOOD=INDV TENSE=PRESENT PERS=SG0

liittyy    WORD_ID=liittyä UPOS=VERB VOICE=ACT MOOD=INDV TENSE=PRESENT PERS=SG3


Kuopion    WORD_ID=Kuopio UPOS=PROPN PROPER=GEO NUM=SG CASE=GEN
Kuopion    WORD_ID=kuopia UPOS=VERB VOICE=ACT MOOD=OPT PERS=SG1 STYLE=ARCHAIC

kaupunkiin    WORD_ID=kaupunki UPOS=NOUN NUM=SG CASE=ILL

vuoden    WORD_ID=vuoden UPOS=ADV
vuoden    WORD_ID=vuosi UPOS=NOUN NUM=SG CASE=GEN

2017    WORD_ID=2017 UPOS=NUM SUBCAT=DIGIT NUMTYPE=CARD
2017    WORD_ID=2017 UPOS=NUM SUBCAT=DIGIT NUMTYPE=CARD NUM=SG CASE=NOM

alussa    WORD_ID=alku UPOS=NOUN NUM=SG CASE=INE
alussa    WORD_ID=alunen UPOS=NOUN NUM=SG CASE=ESS STYLE=ARCHAIC
alussa    WORD_ID=alussa UPOS=ADP ADPTYPE=POST
alussa    WORD_ID=alussa_2 UPOS=ADV

.    WORD_ID=. UPOS=PUNCT BOUNDARY=SENTENCE

$ omorfi-tokenise.py -a src/generated/omorfi.describe.hfst -O conllu -i
        test/test.text |
        omorfi-conllu.py -a src/generated/omorfi.describe.hfst
        --not-rules src/disamparsulation/omorfi.xml

# new doc id= test/test.text
# sent_id = 1
# text = Juankosken kaupunki liittyy Kuopion kaupunkiin vuoden 2017 alussa.
1   Juankosken   Juankoski   PROPN   N   Case=Gen|Number=Sing   2   nmod:poss
        _   Weight=0.01
2   kaupunki   kaupunki   NOUN   N   Case=Nom|Number=Sing   3   nsubj   _
        Weight=0.005
3   liittyy   liittyä   VERB   V
        Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act   0
        root   _   Weight=0.21000000000000002
4   Kuopion   Kuopio   PROPN   N   Case=Gen|Number=Sing   5   nmod:poss   _
        Weight=0.01
5   kaupunkiin   kaupunki   NOUN   N   Case=Ill|Number=Sing   3   obl   _
        Weight=0.01
6   vuoden   vuosi   NOUN   N   Case=Gen|Number=Sing   3   obj   _   Weight=0.015
7   2017   2017   NUM   Num   NumType=Card   _   _   _   Weight=500.0
8   alussa   alku   NOUN   N   Case=Ine|Number=Sing   3   obl   _   Weight=0.025
9   .   .   PUNCT   Punct   _   3   punct   _   Weight=0.03

    
</pre>
<p class="ltx_p"><span class="ltx_text" style="font-size:50%;">Same output is directly generated by TNPP:</span></p>
<pre class="ltx_verbatim ltx_font_typewriter" style="font-size:50%;">$ cat ~/github/flammie/omorfi/test/test.text |
        python3 full_pipeline_stream.py --conf models_fi_tdt/pipelines.yaml --pipeline parse_plaintext
# newdoc
# newpar
# sent_id = 1
# text = Juankosken kaupunki liittyy Kuopion kaupunkiin vuoden 2017 alussa.
1   Juankosken   Juankoski   PROPN   N   Case=Gen|Number=Sing   2   nmod:poss _
        _
2   kaupunki   kaupunki   NOUN   N   Case=Nom|Number=Sing   3   nsubj   _   _
3   liittyy   liittyä   VERB   V
        Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act   0
        root   _   _
4   Kuopion   Kuopio   PROPN   N   Case=Gen|Number=Sing   5   nmod:poss   _   _
5   kaupunkiin   kaupunki   NOUN   N   Case=Ill|Number=Sing   3   obl   _   _
6   vuoden   vuosi   NOUN   N   Case=Gen|Number=Sing   8   nmod:poss   _   _
7   2017   2017   NUM   Num   NumType=Card   6   nummod   _   _
8   alussa   alku   NOUN   N   Case=Ine|Derivation=U|Number=Sing   3   obl   _
        SpaceAfter=No
9   .   .   PUNCT   Punct   _   3   punct   _   SpacesAfter=\n



        
</pre>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example of omorfi’s outputs and the shared-task equivalents
converted.</figcaption>
</figure>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">Some statistics of the rule-based dictionaries can be seen in the
table <a href="#S3.T1" title="Table 1 ‣ 3 Methods and Datasets ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Dictionary</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Words</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Rules</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t"><span class="ltx_text ltx_font_bold">Omorfi</span></td>
<td class="ltx_td ltx_align_right ltx_border_t">445,453</td>
<td class="ltx_td ltx_align_right ltx_border_t">58</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text ltx_font_bold">Apertium-fin-deu</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb">13,119</td>
<td class="ltx_td ltx_align_right ltx_border_bb">93</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Size of the dictionaries in rule-based systems.
</figcaption>
</figure>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p">The <span class="ltx_text ltx_font_italic">Turku neural parsing pipeline</span> (refered from now on to as
TNPP)<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><a href="https://turkunlp.github.io/Turku-neural-parser-pipeline/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://turkunlp.github.io/Turku-neural-parser-pipeline/</a></span></span></span>
is a recent, popular parser for a language-independent parsing of the dependency
structures. They ranked highly in the 2018 CONLL shared task. For the
experiments of this paper, I have downloaded the system following the
instructions and have not changed any hyperparametres. The model used is
<span class="ltx_text ltx_font_typewriter">fi_tdt</span>.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p class="ltx_p">OpenNMT<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><a href="https://github.com/OpenNMT" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/OpenNMT</a></span></span></span> is one of the many popular
neural systems for machine translation. For these experiments I chose it because
it provides usable python bindings and it seemed most robust in our early
experiments.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p class="ltx_p">The training was performed based on the instructions in the OpenNMT
README<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><a href="https://github.com/OpenNMT/OpenNMT-py#quickstart" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/OpenNMT/OpenNMT-py#quickstart</a></span></span></span> and no
additional hyperparametre-tuning was performed. The training was based on
europarl version 7 <cite class="ltx_cite ltx_citemacro_citep">(Koehn, <a href="#bib.bib63" title="Europarl: a parallel corpus for statistical machine translation" class="ltx_ref">2005</a>)</cite>, pre-processed as suggested on
their website<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><a href="https://statmt.org/europarl/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://statmt.org/europarl/</a></span></span></span>. The resulting
corpus is summarised in Table <a href="#S3.T2" title="Table 2 ‣ 3 Methods and Datasets ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Corpus</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Sentences</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t"><span class="ltx_text ltx_font_bold">Europarl train</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t">1,768,817</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row"><span class="ltx_text ltx_font_bold">dev</span></th>
<td class="ltx_td ltx_align_right">1620</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span class="ltx_text ltx_font_bold">test</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb">1620</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Size of the corpora in sentences
</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental setup</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">An interesting part of this experiment is the setup, since one of the aspects we
present in this paper is usability testing of the neural vs. traditional methods
for use of an average Computational Uralist, I also want to get a feel of the
<span class="ltx_text ltx_font_italic">user experience</span> (UX).</p>
</div>
<div id="S4.p2" class="ltx_para">
<p class="ltx_p">The system setup for all the systems is quite similar, all the free and open
software used in these experiments are hosted by github. After cloning, the
traditional rule-based systems rely on classical command-line installations,
this means that user is expected to install dependencies the best they see and
then run compilation of the data using <span class="ltx_text ltx_font_typewriter">configure</span> and <span class="ltx_text ltx_font_typewriter">make</span>
scripts, and neural systems use <span class="ltx_text ltx_font_typewriter">python</span> equivalents. In terms of
dependencies, all systems are basically well covered with some easy way to
install necessary dependencies with single command, such as <span class="ltx_text ltx_font_typewriter">pip</span> or
<span class="ltx_text ltx_font_typewriter">apt-get</span>. A bit like rule-based systems, the neural systems need to
“compile” i.e. learn neural network binaries from large data, in practice the
experience for the end user is the same, except for the wait time, which is
slightly longer for the neural-based systems. For Finnish analysers an option
is provided to download readily compiled models, while for translation models
there is no option. This is equally true for both neural and rule-based models.
To parse or translate I have run the systems with default / suggested settings.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p class="ltx_p">To get an idea of intended mode of use (instant, batch processing over the
weekend) of the systems and steps, I have collected some of our usage times in
the table <a href="#S4.T3" title="Table 3 ‣ 4 Experimental setup ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The real bottleneck for our experiments was the
neural machine translation training time, the multi-day training period is
problematic in itself, but it is also fragile enough that minor impurities in
parallel corpus may ruin the whole model which means that on typical use case
user may need to train the model multiple times before reaching to a functional
one.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Phase, System</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">omorfi</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">TNPP</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">apertium</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">OpenNMT</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Compiling</th>
<td class="ltx_td ltx_align_right ltx_border_t">15 minutes</td>
<td class="ltx_td ltx_align_right ltx_border_t">—</td>
<td class="ltx_td ltx_align_right ltx_border_t">40 seconds</td>
<td class="ltx_td ltx_align_right ltx_border_t">¿5 days</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">(Downloading)</th>
<td class="ltx_td ltx_align_right">yes</td>
<td class="ltx_td ltx_align_right">yes</td>
<td class="ltx_td ltx_align_right">no</td>
<td class="ltx_td ltx_align_right">no</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Parsing / translating</th>
<td class="ltx_td ltx_align_right">5 minutes</td>
<td class="ltx_td ltx_align_right">10 minutes</td>
<td class="ltx_td ltx_align_right">5 seconds</td>
<td class="ltx_td ltx_align_right">30 minutes</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">(Speed)</th>
<td class="ltx_td ltx_align_right">5 sents/s</td>
<td class="ltx_td ltx_align_right">3 sents/s</td>
<td class="ltx_td ltx_align_right">324 sents/s</td>
<td class="ltx_td ltx_align_right">0.9 sents/s</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Model size</th>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">25 MiB</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">770 MiB</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">33 MiB</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">7420 MiB</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Usage times of rule-based and neural systems, time-units are
indicated in the table. For TNPP I have found no documentation on how to
repeat model building or what time it has taken. Sents/s stands for
average sentences per second. Model sizes gives you the total size of
binaries on disk in binary-prefixed bytes (by <span class="ltx_text ltx_font_typewriter">ls -h</span>).
</figcaption>
</figure>
<div id="S4.p4" class="ltx_para">
<p class="ltx_p">To know how much time to create a system takes from scratch it is also useful to
know the amount of data is needed to build it; for rule-based systems this is
the size of dictionary, and rule-sets, for neural system it is the training
data set size. Both of these factors are especially interesting for Uralistic
usage, since the availability of free and open data is rather scarce. The
dictionaries are summarised in Table <a href="#S3.T1" title="Table 1 ‣ 3 Methods and Datasets ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and the corpora in
Table <a href="#S3.T2" title="Table 2 ‣ 3 Methods and Datasets ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></p>
</div>
<div id="S4.p5" class="ltx_para">
<p class="ltx_p">For my OpenNMT setup I have created an autotools-based model builder / test
runner, that is available in github for repeatability
purposes<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><a href="https://github.com/flammie/autostuff-moses-smt/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/flammie/autostuff-moses-smt/</a></span></span></span>.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">I present an evaluation of the systems using the standard metrics from the
shared tasks.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p class="ltx_p">For morphological generation, the shared task was evaluated by measuring average
precisions over all languages, for this experiment I compare the results for
Finnish on 1-best predictions only, as I am interested in specific comparison
relevant for a single Uralic language. The results are summarized in
table <a href="#S5.T4" title="Table 4 ‣ 5 Evaluation ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Test set</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Baseline</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Winning system</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Omorfi</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Task 1</th>
<td class="ltx_td ltx_align_right ltx_border_t">64.45</td>
<td class="ltx_td ltx_align_right ltx_border_t">97.30</td>
<td class="ltx_td ltx_align_right ltx_border_t">93.92</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Task 2</th>
<td class="ltx_td ltx_align_right">59.59</td>
<td class="ltx_td ltx_align_right">97.40</td>
<td class="ltx_td ltx_align_right">93.20</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Task 3</th>
<td class="ltx_td ltx_align_right ltx_border_bb">56.95</td>
<td class="ltx_td ltx_align_right ltx_border_bb">96.56</td>
<td class="ltx_td ltx_align_right ltx_border_bb">92.18</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>1-Best precisions for SIGMORPHON shared task 2016 in Finnish,
the winning Neural system and omorfi scores.
</figcaption>
</figure>
<div id="S5.p3" class="ltx_para">
<p class="ltx_p">For morphosyntactic analysis the standard evaluations would be based on
attachment scores, however, the rule-based system only creates partial
dependency graphs with potentially ambiguous roots; this does not work with
the official evaluation scripts, so I provide instead a raw 1-best precision
result for the specific fields in the CONLL-U format. The results
are shown in table <a href="#S5.T5" title="Table 5 ‣ 5 Evaluation ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>; The <span class="ltx_text ltx_font_italic">lemma</span> row corresponds
3rd CONLL-U column, <span class="ltx_text ltx_font_italic">UPOS</span> 4th, <span class="ltx_text ltx_font_italic">Ufeats</span> 6th, <span class="ltx_text ltx_font_italic">XPOS</span> 5th,
<span class="ltx_text ltx_font_italic">Dephead</span> 7th, and <span class="ltx_text ltx_font_italic">Deplabel</span> 8th. The match is made on strict
equality on the string comparison of the whole content, i.e. no re-arranging or
approximate matching is performed.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Column</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Turku Neural parsing pipeline</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Omorfi</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span class="ltx_text ltx_font_bold">Lemma</span></th>
<td class="ltx_td ltx_align_right ltx_border_t">95.54</td>
<td class="ltx_td ltx_align_right ltx_border_t">82.63</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row"><span class="ltx_text ltx_font_bold">UPOS</span></th>
<td class="ltx_td ltx_align_right">96.91</td>
<td class="ltx_td ltx_align_right">83.88</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row"><span class="ltx_text ltx_font_bold">Ufeats</span></th>
<td class="ltx_td ltx_align_right">94.61</td>
<td class="ltx_td ltx_align_right">73.95</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row"><span class="ltx_text ltx_font_bold">XPOS</span></th>
<td class="ltx_td ltx_align_right">97.89</td>
<td class="ltx_td ltx_align_right">89.58</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row"><span class="ltx_text ltx_font_bold">Dephead</span></th>
<td class="ltx_td ltx_align_right">90.89</td>
<td class="ltx_td ltx_align_right">33.13</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span class="ltx_text ltx_font_bold">Deplabel</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb">92.61</td>
<td class="ltx_td ltx_align_right ltx_border_bb">49.01</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>1-best precisions of Turku neural parsing system and
omorfi. The numbers were measured with our script since the official
test script does not handle partial dependency graphs or multiple roots.
</figcaption>
</figure>
<div id="S5.p4" class="ltx_para">
<p class="ltx_p">For machine translation the standard shared task evaluation method is to
use well-known metrics that compare translations to reference, specifically
BLEU. In table <a href="#S5.T6" title="Table 6 ‣ 5 Evaluation ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> I measure the BLEU scores for europarl
translations.
</p>
</div>
<figure id="S5.T6" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Language pair</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">OpenNMT</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Apertium</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span class="ltx_text ltx_font_bold">German to Finnish</span></th>
<td class="ltx_td ltx_align_right ltx_border_t">7.09</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.6</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span class="ltx_text ltx_font_bold">Finnish to German</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb">7.12</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.3</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Automatic translation evaluations, metrics from WMT shared
tasks 2018 and corpora from europarl evaluation section. BLEU scores have
been measured with the tool <span class="ltx_text ltx_font_typewriter">mteval-14.perl</span>.
</figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Error Analysis</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">As a general trend I see that the precision of the neural systems as well as the
BLEU score of the neural machine translation are above of the rule-based
systems. I also wanted to know if there is any systematicity to the errors, that
the different approaches make. Interesting way forward would be to gain some
insight on how the errors for each system could be fixed if at all. One of the
commonly mentioned advantages of a rule-based system is that it is predictable
and easy to fix or extend; whether a missing form in generation or analysis is
caused by a missing word in lexicon, a missing word-form in paradigm or ordering
of the alternative forms, the solution is easy to see. With a neural system the
possibilities are limited to adding more data or modifying hyperparametres.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p class="ltx_p">When looking at the errors in the morphological regeneration test for rule-based
system, I can see several categories emerge: <span class="ltx_text ltx_font_italic">True OOV</span> for lexemes
missing from the database (e.g. <span class="ltx_text ltx_font_italic">ovovivivipaarisuus</span>), <span class="ltx_text ltx_font_italic">Wrong
paradigm</span> for wordforms that are generated but with some errors, such as wrong
vowel harmony or consonant gradation (e.g. <span class="ltx_text ltx_font_italic">manuskripteiltä</span> pro
<span class="ltx_text ltx_font_italic">manuskripteilta</span> (from manuscripts)) and <span class="ltx_text ltx_font_italic">Real allomorph /
homograph</span> for cases where the correct form is recalled but not at best-1 due to
ambiguous lexeme or free allomorphy (for example, I generate
<span class="ltx_text ltx_font_italic">köykistämäisillänsä</span> pro <span class="ltx_text ltx_font_italic">köykistämäisillään</span> (about to defeat),
but both are equally acceptable). In the leftover category I found among
others, actual bugs in the generation functionality. For example, I was unable
to generate the forms of <span class="ltx_text ltx_font_italic">aliluutnantti</span> (sub-lieutenant) since the
generation function failed to take into account extra semantic tags it
contains.<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup>a bug has been since fixed but I include the original error
analysis in the article for an interesting reference</span></span></span> I sampled a total of 65
errors and the results can be seen in the table <a href="#S6.T7" title="Table 7 ‣ 6 Error Analysis ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure id="S6.T7" class="ltx_table">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Type</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Count</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Percentage</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">OOVs</th>
<td class="ltx_td ltx_align_right ltx_border_t">23</td>
<td class="ltx_td ltx_align_right ltx_border_t">36 %</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Wrong paradigm</th>
<td class="ltx_td ltx_align_right">20</td>
<td class="ltx_td ltx_align_right">31 %</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Allomorphs</th>
<td class="ltx_td ltx_align_right">9</td>
<td class="ltx_td ltx_align_right">15 %</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">Others</th>
<td class="ltx_td ltx_align_right">13</td>
<td class="ltx_td ltx_align_right">20 %</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span class="ltx_text ltx_font_bold">Total</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">65</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">100%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Rule-based morphology generation errors classified.
.</figcaption>
</figure>
<div id="S6.p3" class="ltx_para">
<p class="ltx_p">In the dependency parsing task one of the most common errors in the rule-based
system seems to be the <span class="ltx_text ltx_font_typewriter">Person=0</span> feature with 766 occurrences in the
test set, as it is systematically ambiguous with <span class="ltx_text ltx_font_typewriter">Person=3</span> for all
singulars, it is probably a true ambiguity in that there are not many context
clues to disambiguate it. Another systematic source of errors seems to be the
systematic ambiguity between auxiliary and common verbs, which also shows up in
the parsing of copula structures and in the morphological features. Similarly, a
common problem of rule-based systems in parsing tasks is the etymological
systematic ambiguity created by derivation and lexicalisation, that affects
participle above anything, but also less productive features. It would appear
that OOV’s do not contribute here greatly to the error mass, despite consisting
total of 460 appearances the baseline guess of singular nominative nominal for
the OOV’s is surprisingly often sufficient.</p>
</div>
<div id="S6.p4" class="ltx_para">
<p class="ltx_p">Looking at both rule-based and neural systems for MT, it is easy to
tell that for example the OOV’s constitute a large part of errors, and
exist in most sentences. Judging the actual translation quality by
sampling the sentences also reveals a quality that is overall not sufficient
for computer aided translation or gisting, to the extent that I believe
further analysis may not be fruitful without further development of the
underlying models first.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">One of the goals in this experiment was to find out how usable the neural and
traditional models are for a computational linguist who might want to pick a
state-of-the-art parser off-the-shelf and use it for text analysis or
translation related tasks. Based on my initial impression, I would probably
recommend making use of the neural parsers for languages where enough training
data is available, and aiming to make training data where it is not. However,
for a low resource language, it might often be easier to create a sufficiently
large dictionary with rule-based model than to curate realistic corpus and
annotate it, and given that the results of a rule-based system are not such far
from the state-of-the-art in neural systems for the given metrics, they should
be well sufficient for parsing. On top of that, the resources created with a
rule-based system are a part of necessary NLP system for language survival
(writer’s tools, electronic dictionaries) that neural systems do not offer it
does not make sense to put all eggs in one neural network.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p class="ltx_p">One thing that has been left out of the experiment is what is required for
developing a new system: dictionaries and grammars for rule-based systems,
treebanks or parallel texts for the neural systems. These are available at the
moment for the main Uralic languages: Finnish, Hungarian and Estonian, and to
smaller extent also for Northern Sámi, Erzya. The question then remains, is it
easier for a minority Uralic language to develop a treebank and a parallel
corpus, or dictionaries and grammar, or both.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p class="ltx_p">One noteworthy point to the method of developing resources, as well as to our
evaluation, is, that the original Turku dependency treebank was in fact
developed based on the analyses provided by an old version of
omorfi <cite class="ltx_cite ltx_citemacro_cite">Haverinen<span class="ltx_text ltx_bib_etal"> et al.</span> (<a href="#bib.bib49" title="Building the essential resources for finnish: the turku dependency treebank" class="ltx_ref">2014</a>)</cite>,<span class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup>we thank the anonymous reviewer
for bringing point up</span></span></span> and that was used as a basis for building the
UD-Finnish-TDT treebank, that is used as a model for the TNPP analyser. So a
traditional way to build resources for neural parsers still requires an existing
high-quality rule-based parser as well as a lot of native human annotation work
on the one hand, on the other hand, the combination of rule-based parses and
human annotation does result in a parser that is more precise at predicting in
basic setup.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p class="ltx_p">One thing that might be a common expectation is, is that a rule-based systems
that have been developed for a long time, should score very highly in basic
tasks like morphological generation and parsing, since apart from real OOV’s and
bugs, correctly made morphology should virtually be able to generate 100 % of
the word-forms in its dictionary. For the precision of 1-best analyses however,
there can be small portion of word-forms that either exhibit unexpected (in
terms that writer of rule-based parser had expected form to be ungrammatical)
or free variation. For recall, which is typically the first goal for rule-based
analyser, the value is nearer to the virtual 100 % <cite class="ltx_cite ltx_citemacro_citep">(Pirinen, <a href="#bib.bib123" title="Development and use of computational morphology of finnish in the open source and open science era: notes on experiences with omorfi development." class="ltx_ref">2015a</a>)</cite>.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p class="ltx_p">One surprising thing I found out, that when testing the machine translations on
a non-English pair, the out-of-the-box results for both approaches are very
modest, suggesting that more work is needed to for a usable MT as a tool for
Uralist than just picking off-the-shelf product at the moment. While our test
was still based on non-Uralic language partly due to resource and time
constraints, I believe the results will still give a good indication of the
current state-of-the-art. Notably, it is not unlikely for a research group in
Uralistics to need machine translations of German or for example Russian as
well.</p>
</div>
<div id="S7.p6" class="ltx_para">
<p class="ltx_p">So far, I have only used the precision and BLEU measures to evaluate the
systems, it is likely that different metrics would show more favourable
results for a rule-based systems that typically maximise recall or coverage
first.</p>
</div>
<div id="S7.p7" class="ltx_para">
<p class="ltx_p">One of the surprising finds that I had when fitting the rule-based systems to
non-rule-based shared tasks is, is that I could repurpose the task as
a new automatic continuous integration test set for the lexical database, and
the tests have already proved useful for recognition several types of easily
fixable errors in the database. I note that, in the rule-based system fixing
the OOV-type errors and the paradigm type errors is typically a trivial fix
of one line of code taking less than a minute, however, improving the
allomorph selection or homograph disambiguation is an open research question.</p>
</div>
<div id="S7.p8" class="ltx_para">
<p class="ltx_p">For future work I will study both the neural and rule-based systems further
with hopefully intra-Uralic pairing as well, to find if it’s plausible for
actual use.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p class="ltx_p">I performed some experiments to find out what is the current state-of-the-art
status between neural and rule-based methods for Finnish, I have found out that
the neural methods perform admirably for all parsing approaches for the given
test sets that they were designed for, but rule-based methods are also still
within acceptable distance. For non-parsing task such as machine translation in
Uralic languages the methods are probably not yet sufficient to be efficiently
used as a tool for research, but further research and development is needed.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p class="ltx_p">The author was employed by CLARIN-D during the writing of the article.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="L1" class="ltx_biblist">
<li id="bib.bib150" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">O. Bojar, C. Federmann, M. Fishel, Y. Graham, B. Haddow, P. Koehn and C. Monz (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Findings of the 2018 conference on machine translation (wmt18)</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Third Conference on Machine Translation:
Shared Task Papers</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 272–303</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://aclweb.org/anthology/W18-6401" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Background ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">R. Cotterell, C. Kirov, J. Sylak-Glassman, D. Yarowsky, J. Eisner and M. Hulden (2016)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The sigmorphon 2016 shared task—morphological reinflection</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 14th SIGMORPHON Workshop on Computational
Research in Phonetics, Phonology, and Morphology</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 10–22</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://dx.doi.org/10.18653/v1/W16-2002" title="" class="ltx_ref doi ltx_bib_external">Document</a>,
<a href="http://aclweb.org/anthology/W16-2002" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.i1.p1" title="1 Introduction ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S2.p1" title="2 Background ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K. Haverinen, J. Nyblom, T. Viljanen, V. Laippala, S. Kohonen, A. Missilä, S. Ojala, T. Salakoski and F. Ginter (2014)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Building the essential resources for finnish: the turku dependency treebank</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Language Resources and Evaluation</span> <span class="ltx_text ltx_bib_volume">48</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp. 493–531</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S7.p3" title="7 Discussion ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">J. Kanerva, F. Ginter, N. Miekka, A. Leino and T. Salakoski (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Turku neural parser pipeline: an end-to-end system for the conll 2018 shared task</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the CoNLL 2018 Shared Task: Multilingual
Parsing from Raw Text to Universal Dependencies</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.i2.p1" title="1 Introduction ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">F. Karlsson (1990)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Constraint grammar as a framework for parsing unrestricted text</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 13th International Conference of Computational Linguistics</span>,  <span class="ltx_text ltx_bib_editor">H. Karlgren (Ed.)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">3</span>, <span class="ltx_text ltx_bib_place">Helsinki</span>, <span class="ltx_text ltx_bib_pages"> pp. 168–173</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p2" title="3 Methods and Datasets ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">G. Klein, Y. Kim, Y. Deng, J. Senellart and A. M. Rush (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">OpenNMT: open-source toolkit for neural machine translation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proc. ACL</span>,
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://doi.org/10.18653/v1/P17-4012" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.18653/v1/P17-4012" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.i3.p1" title="1 Introduction ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">P. Koehn (2005)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Europarl: a parallel corpus for statistical machine translation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">MT summit</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">5</span>, <span class="ltx_text ltx_bib_pages"> pp. 79–86</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p6" title="3 Methods and Datasets ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
</span>
</li>
<li id="bib.bib112" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">K. LindÃ©n and T. Pirinen (2009)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Weighting finite-state morphological analyzers using HFST tools</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">FSMNLP 2009</span>,  <span class="ltx_text ltx_bib_editor">B. Watson, D. Courie, L. Cleophas and P. Rautenbach (Eds.)</span>,
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-86854-743-2</span>,
<a href="http://www.ling.helsinki.fi/klinden/pubs/fsmnlp2009weighting.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p2" title="3 Methods and Datasets ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">S. Moshagen, J. Rueter, T. Pirinen, T. Trosterud and F. M. Tyers (2014)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Open-source infrastructures for collaborative work on under-resourced languages</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings ofî e Ninth International Conference on
Language Resources and Evaluation, LREC</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 71–77</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Background ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span>
</li>
<li id="bib.bib123" class="ltx_bibitem ltx_bib_article">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">T. A. Pirinen (2015a)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Development and use of computational morphology of finnish in the open source and open science era: notes on experiences with omorfi development.</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">SKY Journal of Linguistics</span> <span class="ltx_text ltx_bib_volume">28</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.i1.p1" title="1 Introduction ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#I1.i2.p1" title="1 Introduction ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>,
<a href="#S7.p4" title="7 Discussion ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.
</span>
</li>
<li id="bib.bib121" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">T. A. Pirinen (2015b)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Using weighted finite state morphology with visl cg-3-some experiments with free open source finnish resources</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Workshop on âConstraint Grammar-methods, tools
and applicationsâ at NODALIDA 2015, May 11-13, 2015, Institute of the
Lithuanian Language, Vilnius, Lithuania</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 29–33</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S3.p2" title="3 Methods and Datasets ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
</span>
</li>
<li id="bib.bib124" class="ltx_bibitem ltx_bib_misc">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">T. A. Pirinen (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Rule-based machine-translation between finnish and german</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://www.dgfs2018.uni-stuttgart.de/DGfS%5C_2018%5C_booklet%5C_ONLINE%5C_FINAL.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#I1.i3.p1" title="1 Introduction ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span>
</li>
<li id="bib.bib153" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_bibtag ltx_bib_author-year ltx_role_refnum">D. Zeman, J. Haji{č}, M. Popel, M. Potthast, M. Straka, F. Ginter, J. Nivre and S. Petrov (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">CoNLL 2018 shared task: multilingual parsing from raw text to universal dependencies</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the CoNLL 2018 Shared Task: Multilingual
Parsing from Raw Text to Universal Dependencies</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1–21</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://aclweb.org/anthology/K18-2001" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Background ‣ Neural and rule-based Finnish NLP models—expectations, experiments and experiencesThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Dec 21 17:16:26 2018 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>
</body>
</html>

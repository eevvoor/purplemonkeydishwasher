<!DOCTYPE html><html>
<head>
<title>An unsupervised method for weighting finite-state morphological analyzers   1  footnote 1  1  footnote 1  LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original</title>
<!--Generated on Mon May 25 23:09:27 2020 by LaTeXML (version 0.8.4) http://dlmf.nist.gov/LaTeXML/.-->

<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="../latexml/LaTeXML.css" type="text/css">
<link rel="stylesheet" href="../latexml/ltx-article.css" type="text/css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">An unsupervised method for weighting finite-state morphological analyzers
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup>
        <span class="ltx_tag ltx_tag_note">1</span>
        
        
        
      LREC proceedings is CC-BY-NC, copyrighted to ELRA, see
<a href="http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf</a> for
original</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Amr Keleg 
<br class="ltx_break">Faculty of Engineering 
<br class="ltx_break">Ain Shams University, 
<br class="ltx_break">Cairo 
<br class="ltx_break">amr.keleg@eng.asu.edu.eg 
<br class="ltx_break">
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Francis M. Tyers
Department of Linguistics, 
<br class="ltx_break">Indiana University 
<br class="ltx_break">Bloomington 
<br class="ltx_break">ftyers@iu.edu
<br class="ltx_break">
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nicholas Howell 
<br class="ltx_break">School of Linguistics
<br class="ltx_break">Higher School of Economics, 
<br class="ltx_break">Moscow 
<br class="ltx_break">nlhowell@gmail.com, 
<br class="ltx_break">
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tommi A. Pirinen 
<br class="ltx_break">Hamburger Zentrum für Sprachkorpora 
<br class="ltx_break">Hamburg University 
<br class="ltx_break">Hamburg 
<br class="ltx_break">tommi.antero.pirinen@uni-hamburg.de

</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
    
<p class="ltx_p">Morphological analysis is one of the tasks that have been studied for years.
Different techniques have been used to develop models for performing
morphological analysis. Models based on finite state transducers have proved to
be more suitable for languages with low available resources. In this paper, we
have developed a method for weighting a morphological analyzer built using
finite state transducers in order to disambiguate its results. The method is
based on a word2vec model that is trained in a completely unsupervised way using
raw untagged corpora and is able to capture the semantic meaning of the words.
Most of the methods used for disambiguating the results of a morphological
analyzer relied on having tagged corpora that need to manually built.
Additionally, the method developed uses information about the token irrespective
of its context unlike most of the other techniques that heavily rely on the
word’s context to disambiguate its set of candidate analyses.
<br class="ltx_break">
<br class="ltx_break"><span class="ltx_text ltx_font_bold">Keywords:</span> FSTs, FST weighting, constraint grammar, word2vec</p>
  
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p class="ltx_p">Morphological Analysis is the task of mapping an input token to its morphemes.
The morphological analysis of a token can change depending on its context (e.g.:
The word wound is a verb <span class="ltx_text ltx_font_typewriter">&lt;v&gt;</span> in the sentence “the device is wound with
copper wire” and a singular noun <span class="ltx_text ltx_font_typewriter">&lt;n&gt;&lt;sg&gt;</span> in “a knife wound”).
Researchers have been working on building models for languages using different
techniques. For low resourced language, it’s not an easy task to find / build a
sufficiently large tagged corpus that can be used to train supervised machine
learning models.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p class="ltx_p">Finite state transducers (FSTs) map a set of input sequences to a set of output
sequences. A transducer is defined by an input alphabet <math id="S1.p2.m1" class="ltx_Math" alttext="A" display="inline"><mi>A</mi></math>, an output
alphabet <math id="S1.p2.m2" class="ltx_Math" alttext="B" display="inline"><mi>B</mi></math> , a set of states <math id="S1.p2.m3" class="ltx_Math" alttext="S" display="inline"><mi>S</mi></math>, a set of initial states <math id="S1.p2.m4" class="ltx_Math" alttext="I" display="inline"><mi>I</mi></math>, a set of
final states <math id="S1.p2.m5" class="ltx_Math" alttext="F" display="inline"><mi>F</mi></math> and a set of transitions between different states of the
transducer <math id="S1.p2.m6" class="ltx_Math" alttext="T" display="inline"><mi>T</mi></math> where <math id="S1.p2.m7" class="ltx_Math" alttext="I\in S" display="inline"><mrow><mi>I</mi><mo>∈</mo><mi>S</mi></mrow></math>, <math id="S1.p2.m8" class="ltx_Math" alttext="F\in S" display="inline"><mrow><mi>F</mi><mo>∈</mo><mi>S</mi></mrow></math>, <math id="S1.p2.m9" class="ltx_Math" alttext="T\in S\times(A\cup\{\epsilon\})\times(B\cup\{\epsilon\})\times S" display="inline"><mrow><mi>T</mi><mo>∈</mo><mrow><mi>S</mi><mo>×</mo><mrow><mo stretchy="false">(</mo><mrow><mi>A</mi><mo>∪</mo><mrow><mo stretchy="false">{</mo><mi>ϵ</mi><mo stretchy="false">}</mo></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>×</mo><mrow><mo stretchy="false">(</mo><mrow><mi>B</mi><mo>∪</mo><mrow><mo stretchy="false">{</mo><mi>ϵ</mi><mo stretchy="false">}</mo></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>×</mo><mi>S</mi></mrow></mrow></math>. Each transition has an
input token, output token, source state and destination state. Given an input
sequence, the output sequences correspond to the set of valid paths between the
initial states and final states passing through a set of intermediate states.
Finite state transducers have been used in morphologically analyzing text to
convert tokens from the surface form to the lexical one
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="Computational Morphology and Finite-state Methods" class="ltx_ref">1</a>]</cite>. These transducers are non-determinstic if
multiple output sequences can be generated for the same input sequence/pattern.
A weighthed finite state transducer associates a weight to each transition. The
total weight of a path is corresponding to the multiplication of the path’s
transitions weights. For a tropical semiring, the multiplication operator
corresponds to the algebric summation such that the total weight of a path is
computed as the summation of the weights <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="Weighted Finite-state Transducer Algorithms. An Overview" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p class="ltx_p">In this paper, we are investigating the usage of word2vec as a word embedding
model to disambiguate the analyses generated by a finite state transducer. A
weighted morphological analyzer has a method of ordering for the ambiguous set
of analyses for an input token. Our model is completely unsupervised and uses
information about the word only irrespective of its context. Adding weights to a
morphological analyzer will help in finding the most common morphological
analysis for each token and also help in finding the most common tag for a
morphologically unambiguous token (e.g.: The word fine can have two valid
analyses, it can be considered an adjective &lt;adj&gt;”This is a
fine house”, and can be considered a singular noun &lt;n&gt;&lt;sg&gt;”I got a fine”.).
This paper is organized as follows: Section 2 gives a brief overview of the
research done in disambiguating morphological analyzers, Section 3 explains the
background of building weighted morphological analyzers using finite state
transducers, Section 4 describes the reference methods and the word2vec based
method that are used for weighting transducers, Section 5 shows the experimental
setup for the word2vec method, Section 6 reports the results for the weighting
methods and Section 7 provides our conclusion of the experiments.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p class="ltx_p">The task of disambiguating the morphological analyses for
a certain input token has been discussed for years.
Yuret et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib186" title="Learning Morphological Disambiguation Rules for Turkish" class="ltx_ref">14</a>]</cite> used a supervised approach to learn a set
of rules that can be used for disambiguating the analyses. They used an
algorithm called GPA (Greedy Prepend Algorithm) which is an update to the
Prepend Algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="Learning Decision Lists by Prepending Inferred Rules." class="ltx_ref">13</a>]</cite>. Initially, there is a single rule
that matches all the input tokens and assigns them to the most common class/tag
in the training set. Then, New candidate rules are generated by prepending an
attribute to all the rules that currently exist in the list. The gain of each
rule is defined as the number of instances that were wrongly classified and will
get classified correctly if the rule is prepended to the set of rules. Finally,
the rule with maximum gain will be prepended to the set of rules, which means it
will have higher priority than the previous ones. This seems logical since new
rules act as special cases to the old ones.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p class="ltx_p">Another way to build a rule-based morphological analyzer is to depend on a
constraint grammar to disambiguate the results of the tagger
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="A Part-of-speech Tagger for Irish Using Finite-state Morphology and Constraint Grammar Disambiguation" class="ltx_ref">11</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p class="ltx_p">Schiller <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib154" title="German compound analysis with wfsc" class="ltx_ref">9</a>]</cite> demonstrated a way to weight a finite state
transducer for segmenting tokens into their constituent compounds. Although
Segmentation differs from Morphological Analysis, The research demonstrated how
using unsupervised rules can lead to better models.
The author’s intuition is to favor compounds with less number of segments. Then,
using a training corpus, the uni-gram counts of token-compound pairs are used to
disambiguate between compounds having the same number of segments.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p class="ltx_p">Sánchez-Martínez et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib153" title="Using Target-language Information to Train Part-of-speech Taggers for Machine Translation" class="ltx_ref">8</a>]</cite> developed a way to
build a part of speech tagger / disambiguator by training a Hidden Markov Model
that is used in a pipeline of a machine translator. The authors proved how
making use of information from the target language yields better disambiguators.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p class="ltx_p">Shen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib160" title="The Role of Context in Neural Morphological Disambiguation" class="ltx_ref">10</a>]</cite> explored the usage of deep learning
techniques for the morphological disambiguation task through Long short
term-memory(LSTM)-based neural architectures depending on features extracted
from the word and its context. They used two different Embedding models, One for
the list of candidate analyses of the token and the other for the context’s
words and their respective analyses.
These deep learning models depend on having large tagged data-sets that can be used to optimize the cost function.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Background</h2>

<div id="S3.p1" class="ltx_para">
<p class="ltx_p">Finite state transducers can be used to transform words from surface form to
lexical form. A FST is a directed graph having a set of initial and final
states. Each edge in a weighted FST is characterized by three values: an input
token, an output token and a weight taking the form <span class="ltx_text ltx_font_typewriter">Input token:Output
token/Weight</span>. Moreover, the final states may have weight values associated to
them. For an input surface word, the corresponding lexical forms (analyses) can
be generated by finding the set of paths that start from one of the initial
states, end at one of the final states and pass through a set of edges such that
the concatenation of the edges’ input tokens matches the input surface word. A
FST may have epsilon transitions (in the form <math id="S3.p1.m1" class="ltx_Math" alttext="\varepsilon" display="inline"><mi>ε</mi></math><span class="ltx_text ltx_font_typewriter">:Output
token/Weight</span>) which are edges that generate an output token without consuming
an input token.
For a tropical semi-ring implementation of FSTs, the weight of a path is the
summation of the weights of the path’s edges and the weight of the final state.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p class="ltx_p">Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Background ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows how a simple FST can be used to transform
the word <span class="ltx_text ltx_font_typewriter">euro</span> to its corresponding lexical forms <span class="ltx_text ltx_font_typewriter">euro&lt;n&gt;&lt;sg&gt;</span>
and <span class="ltx_text ltx_font_typewriter">euro&lt;n&gt;&lt;pl&gt;</span>. Initially, the weights of the edges and the final
states are equal to zero. To disambiguate the results of the analyzer, different
paths should have different weights such that the most probable path has the
least weight.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering" width="156" height="413" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> An unweighted FST</figcaption>
</figure>
<div id="S3.p3" class="ltx_para">
<p class="ltx_p">One way to weight a FST is achieved by an operation called Composition.
Composition is an operation to join two FSTs together.
If the first FST is used to map a set of input tokens X to a set of output
tokens Y and the second FST is used to map a set of input tokens Y to a set of
output tokens Z, then composing both FSTs together will result in a FST that
maps the set of input tokens X directly to the set of output tokens Z.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p class="ltx_p">Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Background ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows an example for a FST that can be used to weight
the morphological analysis FST. The second FST weights any input with a suffix
<span class="ltx_text ltx_font_typewriter">&lt;n&gt;&lt;sg&gt;</span> to 1 and weights any input with a suffix <span class="ltx_text ltx_font_typewriter">&lt;n&gt;&lt;pl&gt;</span> to 2.
This FST is generated from two regular expressions in the form
<span class="ltx_text ltx_font_typewriter">?+&lt;n&gt;&lt;sg&gt;::1</span> and <span class="ltx_text ltx_font_typewriter">?+&lt;n&gt;&lt;pl&gt;::2</span> where ? means any token from the
FST’s alphabet and ?+ means that the input word must have a prefix of at least
one token.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering" width="166" height="250" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span> A weighted FST</figcaption>
</figure>
<div id="S3.p5" class="ltx_para">
<p class="ltx_p">Using composition, a weighted morphological analyzer is formed as shown in
Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Background ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. This example shows that in order to weight a
morphological analyzer, a set of weighted regular expressions (weightlist) needs
to be formed so that each path in the analyzer is associated with a certain
weight.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering" width="156" height="413" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> The composed FST</figcaption>
</figure>
<div id="S3.p6" class="ltx_para">
<p class="ltx_p">Another simple FST operation is the difference operation that resembles the
difference operator between two sets. It removes all the paths that are found in
the second FST from the first one. This operation is used in the case of having
multiple successive weightlists.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Development</h2>

<div id="S4.p1" class="ltx_para">
<p class="ltx_p">All the methods except the analysis length method generate a regexp weightlist
in the form of mapping an unweighted lexical form to a weighted lexical form.
This regexp weightlist is compiled and composed with the original unweighted FST
(mapping surface form to lexical form) to generate a weighted FST.
In the following section, we will describe how weightlists are generated in the
reference methods and the word2vec method.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Reference models</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p class="ltx_p">We have evaluated the usage of different reference methods to compare them with
the word2vec based method. These reference models are based on statistical
theories or based on having a heuristic for weighting analyses.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Unigram-counts based weightlist</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p class="ltx_p">The unigram-counts based method is a supervised technique. It was proved that
the unigram method can successfully disambiguate morphological analyses for
Finnish <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="Weighting finite-state morphological analyzers using HFST tools" class="ltx_ref">5</a>]</cite>.
Given a tagged corpus in the form (surface form, analysis), The method
estimates the probability of a certain analysis, <math id="S4.SS1.SSS1.p1.m1" class="ltx_Math" alttext="a" display="inline"><mi>a</mi></math> given a surface form,
<math id="S4.SS1.SSS1.p1.m2" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math> as <math id="S4.SS1.SSS1.p1.m3" class="ltx_Math" alttext="P(a|s)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>a</mi><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>s</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math>. Using Bayes’ theorem, The probability of an analysis given
a surface form can be computed as in (<a href="#S4.E1" title="(1) ‣ 4.1.1 Unigram-counts based weightlist ‣ 4.1 Reference models ‣ 4 Development ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div id="S4.SS1.SSS1.p2" class="ltx_para">
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1" class="ltx_Math" alttext="P(a|s)=\frac{P(s|a)P(a)}{P(s)}." display="block"><mrow><mrow><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>a</mi><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>s</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mfrac><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>s</mi><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>a</mi></mrow><mo stretchy="false">)</mo></mrow><mo>⁢</mo><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac></mrow><mo>.</mo></mrow></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr>
</table>
</div>
<div id="S4.SS1.SSS1.p3" class="ltx_para">
<p class="ltx_p">The method needs to estimate the values for the three terms <math id="S4.SS1.SSS1.p3.m1" class="ltx_Math" alttext="P(s|a)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>s</mi><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>a</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math>, <math id="S4.SS1.SSS1.p3.m2" class="ltx_Math" alttext="P(a)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow></math>
and <math id="S4.SS1.SSS1.p3.m3" class="ltx_Math" alttext="P(s)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></math>. The term <math id="S4.SS1.SSS1.p3.m4" class="ltx_Math" alttext="P(s|a)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>s</mi><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>a</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math> is the probability of a surface form <math id="S4.SS1.SSS1.p3.m5" class="ltx_Math" alttext="s" display="inline"><mi>s</mi></math>
given that it has a lexical form <math id="S4.SS1.SSS1.p3.m6" class="ltx_Math" alttext="a" display="inline"><mi>a</mi></math>. This term will always be equal to 1.
e.g: For the analysis <span class="ltx_text ltx_font_typewriter">cat&lt;n&gt;&lt;pl&gt;</span>, the only valid surface form is
<span class="ltx_text ltx_font_typewriter">cats</span>. Thus, <math id="S4.SS1.SSS1.p3.m7" class="ltx_Math" alttext="P(s|a)=1" display="inline"><mrow><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>s</mi><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>a</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow></math> if analysis is one of the possible analyses
for the surface form (<math id="S4.SS1.SSS1.p3.m8" class="ltx_Math" alttext="P(\mathrm{cats}|\mathrm{cat&lt;n&gt;&lt;pl&gt;})=1" display="inline"><mrow><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>cats</mi><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mrow><mi>cat</mi><mo>⁢</mo><mrow><mo>&lt;</mo><mi mathvariant="normal">n</mi><mo>&gt;</mo></mrow><mo>⁢</mo><mrow><mo>&lt;</mo><mi>pl</mi><mo>&gt;</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mn>1</mn></mrow></math>). Since
the denominator term <math id="S4.SS1.SSS1.p3.m9" class="ltx_Math" alttext="P(s)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo></mrow></mrow></math> is common for all the valid analyses of the input
surface form, then this term can be omitted without affecting the order of the
conditional probabilities. Therefore, <math id="S4.SS1.SSS1.p3.m10" class="ltx_Math" alttext="P(a|s)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>a</mi><mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mo><mi>s</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></math> can be substituted by
<math id="S4.SS1.SSS1.p3.m11" class="ltx_Math" alttext="P(a)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow></math>. Doing so will not make it possible to interpret the weights in a
probabilistic way but it will not affect the order of these probabilities. The
value <math id="S4.SS1.SSS1.p3.m12" class="ltx_Math" alttext="P(a)" display="inline"><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow></math> can be estimated as number of occurrences of analysis divided by
the size of tagged corpus.</p>
</div>
<div id="S4.SS1.SSS1.p4" class="ltx_para">
<p class="ltx_p">Instead of calculating small floating values for the probability, Tropical
semirings are used such that the weight is equivalent to -log(probability). Thus
the weight will be <math id="S4.SS1.SSS1.p4.m1" class="ltx_Math" alttext="-\mathrm{log}(P(a))" display="inline"><mrow><mo>-</mo><mrow><mi>log</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mi>P</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math> which equals <math id="S4.SS1.SSS1.p4.m2" class="ltx_Math" alttext="-\mathrm{log}(N_{a})+\mathrm{log}(|C|)" display="inline"><mrow><mrow><mo>-</mo><mrow><mi>log</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><msub><mi>N</mi><mi>a</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mi>log</mi><mo>⁢</mo><mrow><mo stretchy="false">(</mo><mrow><mo stretchy="false">|</mo><mi>C</mi><mo stretchy="false">|</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></math>, where <math id="S4.SS1.SSS1.p4.m3" class="ltx_Math" alttext="N_{a}" display="inline"><msub><mi>N</mi><mi>a</mi></msub></math> is the number of occurrences of analysis
<math id="S4.SS1.SSS1.p4.m4" class="ltx_Math" alttext="a" display="inline"><mi>a</mi></math> and <math id="S4.SS1.SSS1.p4.m5" class="ltx_Math" alttext="|C|" display="inline"><mrow><mo stretchy="false">|</mo><mi>C</mi><mo stretchy="false">|</mo></mrow></math> is the size of the corpus.</p>
</div>
<div id="S4.SS1.SSS1.p5" class="ltx_para">
<p class="ltx_p">Additionally, it was found that the unigram counts of the tags can be used to
disambiguate unweighted analyses. (e.g: If the tag <span class="ltx_text ltx_font_typewriter">&lt;n&gt;&lt;pl&gt;</span> was the most
common tag in the corpus, then an unweighted analyses in the form
<span class="ltx_text ltx_font_typewriter">*&lt;n&gt;&lt;pl&gt;</span> should have higher probability than <span class="ltx_text ltx_font_typewriter">*&lt;adj&gt;</span>). Using
this trick should also take into consideration the constraint that analyses
which were part of the tagged corpus will end-up being be more probable than
analyses disambiguated using tags’ counts. One way to achieve this is to shift
the counts of the analyses that were part of the tagged corpus by an offset
equal to <math id="S4.SS1.SSS1.p5.m1" class="ltx_Math" alttext="|C|" display="inline"><mrow><mo stretchy="false">|</mo><mi>C</mi><mo stretchy="false">|</mo></mrow></math>. To make the summation of probabilities equal to one, the
denominator should be shifted by <math id="S4.SS1.SSS1.p5.m2" class="ltx_Math" alttext="|C|*N_{ua}+|C|" display="inline"><mrow><mrow><mrow><mo stretchy="false">|</mo><mi>C</mi><mo stretchy="false">|</mo></mrow><mo>*</mo><msub><mi>N</mi><mrow><mi>u</mi><mo>⁢</mo><mi>a</mi></mrow></msub></mrow><mo>+</mo><mrow><mo stretchy="false">|</mo><mi>C</mi><mo stretchy="false">|</mo></mrow></mrow></math> where <math id="S4.SS1.SSS1.p5.m3" class="ltx_Math" alttext="N_{ua}" display="inline"><msub><mi>N</mi><mrow><mi>u</mi><mo>⁢</mo><mi>a</mi></mrow></msub></math> is the
number of unique analyses in the corpus.</p>
</div>
<div id="S4.SS1.SSS1.p6" class="ltx_para">
<p class="ltx_p">Finally, Laplace smoothing is used such that analyses that were not found in the
corpus are given a small probability (high weight). This is done by incrementing
the count of occurrences of all the analyses that were either part of the tagged
corpus or matched one of the regex tags by 1. Consequently, The denominator is
incremented by <math id="S4.SS1.SSS1.p6.m1" class="ltx_Math" alttext="1+N_{ua}+N_{ut}" display="inline"><mrow><mn>1</mn><mo>+</mo><msub><mi>N</mi><mrow><mi>u</mi><mo>⁢</mo><mi>a</mi></mrow></msub><mo>+</mo><msub><mi>N</mi><mrow><mi>u</mi><mo>⁢</mo><mi>t</mi></mrow></msub></mrow></math> where <math id="S4.SS1.SSS1.p6.m2" class="ltx_Math" alttext="N_{ua}" display="inline"><msub><mi>N</mi><mrow><mi>u</mi><mo>⁢</mo><mi>a</mi></mrow></msub></math> is the number of unique
analyses in the corpus and <math id="S4.SS1.SSS1.p6.m3" class="ltx_Math" alttext="N_{ut}" display="inline"><msub><mi>N</mi><mrow><mi>u</mi><mo>⁢</mo><mi>t</mi></mrow></msub></math> is the number of unique tag sequences in
the corpus.</p>
</div>
<div id="S4.SS1.SSS1.p7" class="ltx_para">
<p class="ltx_p">So, this method will end-up generating three different weightlists.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p class="ltx_p">A weightlist based on the unigram counts of each analysis in the corpus.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;padding-top:5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p class="ltx_p">A weightlist based on the unigram counts of each tag in the corpus.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;padding-top:5.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p class="ltx_p">A default weightlist based on Laplace smoothing.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Constraint Grammar-based weightlist</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p class="ltx_p">A constraint grammar is a set of rules for selecting or removing certain
analyses given the lexical forms of the previous/next tokens
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="Constraint grammar as a framework for parsing unrestricted text" class="ltx_ref">4</a>]</cite>. For English, a constraint grammar might have a
rule that selects the infinitive analysis of a verb if it’s preceded by the word
<span class="ltx_text ltx_font_typewriter">to</span>. The rule written in CG-3 format will be <span class="ltx_text ltx_font_typewriter">"SELECT Inf IF (0C
V) (-1C To) ;"</span> which can interpreted as select the infinitive analysis if the
current token is a verb and the previous token is to.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p class="ltx_p">Given a large unannotated corpus, First the list of candidate analyses for the
whole corpus is generated. Then, the Constraint Grammar is used to filter the
lists of analyses. The remaining candidate analyses are considered to be a
tagged corpus that is used to estimate the weightlist in the same fashion as the
unigram based method.</p>
</div>
<div id="S4.SS1.SSS2.p3" class="ltx_para">
<p class="ltx_p">Since this method makes use of a pre-built constraint grammar then it can be
considered as a semi-unsupervised model.
</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Random weightlist</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p class="ltx_p">The random method represents the baseline for all the other models. Given a a
raw unlabeled corpus and unweighted FST, Weights are generated as follows:</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p class="ltx_p">Each token is analyzed using the unweighted FST generating multiple
ambiguous analyses/lexical forms.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p class="ltx_p">A random analysis is chosen and added to a weighting corpus.</p>
</div>
</li>
<li id="S4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I2.i3.p1" class="ltx_para">
<p class="ltx_p">The randomly selected analyses for the tokens are treated as a tagged
corpus that is used to estimate the weights of the analyses.</p>
</div>
</li>
</ul>
<p class="ltx_p">The method breaks down any regularity/heuristic that could have improved the
weighting process. This method’s evaluation metrics act as a lower bound for the
metrics of all the other methods.</p>
</div>
</section>
<section id="S4.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4 </span>Equal weightlist</h4>

<div id="S4.SS1.SSS4.p1" class="ltx_para">
<p class="ltx_p">The equally-probable method assigns a weight of one to all the analyses. After
weighting the FST, the first candidate out of the list of valid lexical analyses
is selected as the correct analysis. This method evaluates the default order of
the analyses reached by the analyzer.</p>
</div>
</section>
<section id="S4.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.5 </span>Analysis Length weightlist</h4>

<div id="S4.SS1.SSS5.p1" class="ltx_para">
<p class="ltx_p">The analysis length method depends on a heuristic that shorter analyses are more
probable than long ones. Therefore, Complex analyses having many tags will have
larger weight (less probability) than simple/short ones. One way to achieve such
weighting is to assign a weight of 1 to all the edges of a FST.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>The word2vec-based weightlist</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p class="ltx_p">Word embedding models have proved to be successful in capturing the semantic
similarities between words even if they are syntactically different. Word2vec
is one of the most popular models used for training word embedding models
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="Efficient Estimation of Word Representations in Vector Space" class="ltx_ref">6</a>]</cite>. After training a word2vec model using large raw
corpus (such as: wikipedia dumps), a smaller untagged corpus is used. For each
token in the corpus, a list of the top 10 similar words to the token is
generated by finding the words whose vectors have the highest cosine similarity
with the input token’s vector. Then, the analyses of all the similar words is
found using the unweighted morphological analyzer and all the ambiguous similar
words (the words having more than one analysis) are discarded. Finally, the
analyses of unambiguous similar words are used to disambiguate the analysis of
the current token.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p class="ltx_p">Example:
For the token “wound”, the possible analyses are:</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" style="width:56.9pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">wound</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" style="width:156.5pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">wind&lt;vblex&gt;&lt;pp&gt;/</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r" style="width:56.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" style="width:156.5pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">wind&lt;vblex&gt;&lt;past&gt;/</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r" style="width:56.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" style="width:156.5pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">wound&lt;n&gt;&lt;sg&gt;/</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r" style="width:56.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" style="width:156.5pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">wound&lt;vblex&gt;&lt;pres&gt;/</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r" style="width:56.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_r" style="width:156.5pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">wound&lt;vblex&gt;&lt;inf&gt;/</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r" style="width:56.9pt;"></td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r" style="width:156.5pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">wound&lt;vblex&gt;&lt;imp&gt;</span></td>
</tr>
</tbody>
</table>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p class="ltx_p">The words similar to the token “wound” using just the cosine similarity between
the token and the vectors of the other tokens in the vocabulary without making
use of the token’s context are:</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" style="width:56.9pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">wounds</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t" style="width:156.5pt;">
<span class="ltx_text ltx_font_typewriter">wound&lt;n&gt;&lt;pl&gt;</span>/
<span class="ltx_text ltx_wrap ltx_font_typewriter">wound&lt;vblex&gt;&lt;pres&gt;&lt;p3&gt;&lt;sg&gt;</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" style="width:56.9pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">injuries</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" style="width:156.5pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">injury&lt;n&gt;&lt;pl&gt;</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" style="width:56.9pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">injury</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" style="width:156.5pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">injury&lt;n&gt;&lt;sg&gt;</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" style="width:56.9pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">neck</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" style="width:156.5pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">neck&lt;n&gt;&lt;sg&gt;</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" style="width:56.9pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">chest</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" style="width:156.5pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">chest&lt;n&gt;&lt;sg&gt;</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" style="width:56.9pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">blow</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" style="width:156.5pt;">
<span class="ltx_text ltx_font_typewriter">blow&lt;n&gt;&lt;sg&gt;</span>/
<span class="ltx_text ltx_font_typewriter">blow&lt;vblex&gt;&lt;inf&gt;</span>/
<span class="ltx_text ltx_font_typewriter">blow&lt;vblex&gt;&lt;pres&gt;</span>/
<span class="ltx_text ltx_wrap ltx_font_typewriter">blow&lt;vblex&gt;&lt;imp&gt;</span>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" style="width:56.9pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">cord</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" style="width:156.5pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">cord&lt;n&gt;&lt;sg&gt;</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" style="width:56.9pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">ulcer</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" style="width:156.5pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">ulcer&lt;n&gt;&lt;sg&gt;</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_l ltx_border_r ltx_border_t" style="width:56.9pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">tendon</span></td>
<td class="ltx_td ltx_align_justify ltx_border_r ltx_border_t" style="width:156.5pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">tendon&lt;n&gt;&lt;sg&gt;</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="width:56.9pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">surgery</span></td>
<td class="ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t" style="width:156.5pt;"><span class="ltx_text ltx_wrap ltx_font_typewriter">surgery&lt;n&gt;&lt;sg&gt;</span></td>
</tr>
</tbody>
</table>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Most common analyses of the unambiguous similar words of the word
<span class="ltx_text ltx_font_typewriter">wound</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span class="ltx_text ltx_font_bold">Tag</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Count</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span class="ltx_text ltx_font_typewriter">&lt;n&gt;&lt;sg&gt;</span></th>
<td class="ltx_td ltx_align_right ltx_border_t">7</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span class="ltx_text ltx_font_typewriter">&lt;n&gt;&lt;pl&gt;</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb">1</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS2.p6" class="ltx_para">
<p class="ltx_p">The ambiguous similar words (<span class="ltx_text ltx_font_typewriter">wounds</span> and <span class="ltx_text ltx_font_typewriter">blow</span>) will be
discarded then a tag count for the tags of the remaining similar words’ analyses
is formed. (The most common tags among the analyses of the similar words are
found in Table <a href="#S4.T1" title="Table 1 ‣ 4.2 The word2vec-based weightlist ‣ 4 Development ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).
Finally, the most common tags are compared to the possible analyses of the
current token and the matching tags are weighted using the unigram tag counts.
(i.e.: For the token “wound” the tag <span class="ltx_text ltx_font_typewriter">&lt;n&gt;&lt;sg&gt;</span> is the most probable
analysis and is a possible analysis for the input token. On the other hand, the
tag <span class="ltx_text ltx_font_typewriter">&lt;n&gt;&lt;pl&gt;</span> isn’t a possible analysis for the input token thus it’s
ignored).</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>How are weightlists used</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p class="ltx_p">The generated weightlists using different methods are in the form of weighted
analyses. These weightlists can be compiled into FSTs that transform an
unweighted analysis into a weighted one. FST composition is then used to
generate a FST that transforms an input surface form into a weighted analysis.
The first FST is the unweighted morphological analyzer that maps surface forms
to lexical forms. And the second FST is one that maps unweighted lexical forms
to weighted ones. Each weighted analysis in the weightlist is converted a
weighted FST using <span class="ltx_text ltx_font_typewriter">hfst-regexp2fst</span>. These FSTs are disjuncted together
to generate a single FST for mapping lexical forms to weighted lexical forms.
On composing both FSTs, a weighted FST mapping surface form to weighted lexical
forms will be generated.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p class="ltx_p">However, if the second FST doesn’t have a path for a certain analysis then the
surface-form:analysis pair will be dropped. To avoid this we use FST difference
to find all the paths that were part of the unweighted FST and were dropped in
the final composed FST. Then, we use another fallback weightlist for these
remaining analyses.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p class="ltx_p">Our implementation allows the usage of a set of weightlists such that the second
weightlist acts as a fallback for the first weightlist and the third weightlist
acts as a fallback for the first two and so on. We also need to make sure that
the weights in the second weightlist are larger than those of the first one such
that those paths are less favorable. Additionally, The final and default
weightlist makes use of Laplace Smoothing such that every path that was part of
the unweighted FST is added to the final weighted FST.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p class="ltx_p">Different weighting methods are implemented as a set of shell scripts and Python
3 scripts <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup>
            <span class="ltx_tag ltx_tag_note">2</span>
            
            
            
          <a href="https://github.com/apertium/apertium-weighting-tools" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/apertium/apertium-weighting-tools</a></span></span></span>.
Apertium’s lttoolbox (lexical toolbox) and hfst are used to apply different FST
operations and transformations. Linguistic resources such as: unweighted
morphological analyzers, tagged corpora and constraint grammars for English,
Kazakh <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib178" title="Finite-state morphological transducers for three kypchak languages" class="ltx_ref">12</a>]</cite> and Serbo-Croatian<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup>
            <span class="ltx_tag ltx_tag_note">3</span>
            
            
            
          We use the
language name Serbo-Croatian to refer to the varieties spoken in Bosnia,
Croatia, Montenegro and Serbia which may appear under a number of names. The
languages are morphologically very similar and the morphological analyzer treats
them as a single language system. Regional differences in phonology (e.g. the
<em class="ltx_emph ltx_font_italic">yat</em> reflex as {<em class="ltx_emph ltx_font_italic">-e-</em>, <em class="ltx_emph ltx_font_italic">-je-</em>, <em class="ltx_emph ltx_font_italic">-ije-</em>},
morphology/orthography (spelling of the future tense) and lexicon are dealt with
in the analyzer.</span></span></span> that were used throughout the experiments are actually
available as part of Apertium’s resources.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Word2vec models</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p class="ltx_p">Word2vec model has two common architectures: Continous Bag of Words (CBoW) and
Skip-gram (SG). According to the results reached by
Mikolov et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="Efficient Estimation of Word Representations in Vector Space" class="ltx_ref">6</a>]</cite> on the Semantic-Syntactic Word
Relationship test set, Models based on the skip-gram architecture perform better
on semantic similarity tasks while the continuous bag of words perform better on
syntactic similarity tasks. In our model, we are more interested in using
word2vec models that can find the words semantically similar to a given word so
we used the Skip-gram version of the word2vec models (e.g: for the word
<span class="ltx_text ltx_font_typewriter">wound</span>, a semantically similar word would be <span class="ltx_text ltx_font_typewriter">injury</span> which can
be used to disambiguate the morphological analysis of <span class="ltx_text ltx_font_typewriter">wound</span>. On the
other hand, a syntactically similar word would be <span class="ltx_text ltx_font_typewriter">wounding</span> which isn’t
useful in disambiguating the input word as they don’t share similar
morphological analyses).</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p class="ltx_p">Fares et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="Word Vectors, Reuse, and Replicability: Towards a Community Repository of Large-text Resources" class="ltx_ref">2</a>]</cite> showed that researchers don’t pay great
attention to optimizing the training process of word2vec models. The trained
models aren’t reproducible due to the randomized nature of the architecture.
Additionally, hyper-parameter optimizations are required to build models that
can perform well on the given task.
They have also publicly shared a set of pre-trained models as a solution to the
reproducibility issues. Pre-trained models for English and Kazakh only are
avialable and we have used these models to evaluate our weighting method.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p class="ltx_p">The three pretrained models that were used are: A Word2Vec Continuous Skipgram
(ID:18) trained using gensim with vector size of 300 and window size of 5
trained using the English Wikipedia Dump of February 2017 with vocabulary
size 291186, a Word2Vec Continuous Skipgram (ID:40) trained using gensim
with vector size of 100 and window size of 10 trained using the English
CoNLL17 corpus with vocabulary size 4027169 and a Word2Vec Continuous
Skipgram (ID:54) with vector size of 100 and window size of 10 trained
using the Kazakh CoNLL17 corpus of vocabulary size 176643.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p class="ltx_p">Moreover, we have trained multiple word2vec models from scratch to investigate
the effect of different parameters on the overall performance of the system.
Wikipedia dumps for English, Kazakh and Serbo-Croatian are used to train
models. We used a Python package called gensim to train the word2vec models.
After thoroughly checking the gensim Python package, we found that using a
single thread and setting the random seed are required to ensure the
reproducibility of the results.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p class="ltx_p">Some of the hyperparameters are already optimized by
Mikolov et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="Efficient Estimation of Word Representations in Vector Space" class="ltx_ref">6</a>]</cite> where they have recommended
a set of values for the parameters.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Hyperparameters settings of the word2vec model</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span class="ltx_text ltx_font_bold">Hyperparameter</span></th>
<td class="ltx_td ltx_align_right ltx_border_tt"><span class="ltx_text ltx_font_bold">Value</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">model architecture</th>
<td class="ltx_td ltx_align_right ltx_border_t">Skip-gram</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">minimum count</th>
<td class="ltx_td ltx_align_right">5</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">window size</th>
<td class="ltx_td ltx_align_right">2, 5, 10</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">vector size</th>
<td class="ltx_td ltx_align_right">100, 200, 300</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">downsampling threshold</th>
<td class="ltx_td ltx_align_right"><math id="S5.T2.m1" class="ltx_Math" alttext="10^{-5}" display="inline"><msup><mn>10</mn><mrow><mo>-</mo><mn>5</mn></mrow></msup></math></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">initial learning rate</th>
<td class="ltx_td ltx_align_right">0.025</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">minimum learning rate</th>
<td class="ltx_td ltx_align_right">0.0001</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">negative sampling</th>
<td class="ltx_td ltx_align_right">20</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">epochs</th>
<td class="ltx_td ltx_align_right">5</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">random seed</th>
<td class="ltx_td ltx_align_right ltx_border_bb">42</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Effect of vector size on skip-gram models (window size is 2)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">English</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">Serbo-Croatian</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">Kazakh</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span class="ltx_text ltx_font_bold">Vector size</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T3.m1" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T3.m2" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T3.m3" class="ltx_Math" alttext="F" display="inline"><mi>F</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T3.m4" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T3.m5" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T3.m6" class="ltx_Math" alttext="F" display="inline"><mi>F</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T3.m7" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T3.m8" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T3.m9" class="ltx_Math" alttext="F" display="inline"><mi>F</mi></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">100 (la)</th>
<td class="ltx_td ltx_align_right ltx_border_t">0.710</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.713</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.712</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.468</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.470</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.469</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.646</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.636</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.641</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">200 (la)</th>
<td class="ltx_td ltx_align_right">0.701</td>
<td class="ltx_td ltx_align_right">0.705</td>
<td class="ltx_td ltx_align_right">0.703</td>
<td class="ltx_td ltx_align_right">0.469</td>
<td class="ltx_td ltx_align_right">0.471</td>
<td class="ltx_td ltx_align_right">0.470</td>
<td class="ltx_td ltx_align_right">0.640</td>
<td class="ltx_td ltx_align_right">0.630</td>
<td class="ltx_td ltx_align_right">0.635</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">300 (la)</th>
<td class="ltx_td ltx_align_right">0.705</td>
<td class="ltx_td ltx_align_right">0.709</td>
<td class="ltx_td ltx_align_right">0.707</td>
<td class="ltx_td ltx_align_right">0.473</td>
<td class="ltx_td ltx_align_right">0.476</td>
<td class="ltx_td ltx_align_right">0.474</td>
<td class="ltx_td ltx_align_right">0.649</td>
<td class="ltx_td ltx_align_right">0.639</td>
<td class="ltx_td ltx_align_right">0.644</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">100 (sgt)</th>
<td class="ltx_td ltx_align_right ltx_border_t">0.701</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.704</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.702</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.460</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.462</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.461</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.647</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.637</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.642</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">200 (sgt)</th>
<td class="ltx_td ltx_align_right">0.698</td>
<td class="ltx_td ltx_align_right">0.702</td>
<td class="ltx_td ltx_align_right">0.700</td>
<td class="ltx_td ltx_align_right">0.463</td>
<td class="ltx_td ltx_align_right">0.466</td>
<td class="ltx_td ltx_align_right">0.464</td>
<td class="ltx_td ltx_align_right">0.646</td>
<td class="ltx_td ltx_align_right">0.636</td>
<td class="ltx_td ltx_align_right">0.641</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">300 (sgt)</th>
<td class="ltx_td ltx_align_right ltx_border_bb">0.696</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.700</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.698</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.476</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.480</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.478</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.652</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.642</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.647</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Effect of window size on skip-gram models (vector size is 100 for English and 300 for Kazakh and Serbo-Croatian)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">English</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">Serbo-Croatian</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">Kazakh</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span class="ltx_text ltx_font_bold">Window size</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T4.m1" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T4.m2" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T4.m3" class="ltx_Math" alttext="F" display="inline"><mi>F</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T4.m4" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T4.m5" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T4.m6" class="ltx_Math" alttext="F" display="inline"><mi>F</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T4.m7" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T4.m8" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T4.m9" class="ltx_Math" alttext="F" display="inline"><mi>F</mi></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">2 (la)</th>
<td class="ltx_td ltx_align_right ltx_border_t">0.710</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.713</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.712</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.473</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.476</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.474</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.649</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.639</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.644</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">5 (la)</th>
<td class="ltx_td ltx_align_right">0.737</td>
<td class="ltx_td ltx_align_right">0.736</td>
<td class="ltx_td ltx_align_right">0.736</td>
<td class="ltx_td ltx_align_right">0.450</td>
<td class="ltx_td ltx_align_right">0.454</td>
<td class="ltx_td ltx_align_right">0.452</td>
<td class="ltx_td ltx_align_right">0.644</td>
<td class="ltx_td ltx_align_right">0.634</td>
<td class="ltx_td ltx_align_right">0.639</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">10 (la)</th>
<td class="ltx_td ltx_align_right">0.740</td>
<td class="ltx_td ltx_align_right">0.739</td>
<td class="ltx_td ltx_align_right">0.739</td>
<td class="ltx_td ltx_align_right">0.437</td>
<td class="ltx_td ltx_align_right">0.440</td>
<td class="ltx_td ltx_align_right">0.438</td>
<td class="ltx_td ltx_align_right">0.652</td>
<td class="ltx_td ltx_align_right">0.642</td>
<td class="ltx_td ltx_align_right">0.647</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">2 (sgt)</th>
<td class="ltx_td ltx_align_right ltx_border_t">0.701</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.704</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.702</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.476</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.480</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.478</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.652</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.642</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.647</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row">5 (sgt)</th>
<td class="ltx_td ltx_align_right">0.730</td>
<td class="ltx_td ltx_align_right">0.729</td>
<td class="ltx_td ltx_align_right">0.730</td>
<td class="ltx_td ltx_align_right">0.431</td>
<td class="ltx_td ltx_align_right">0.434</td>
<td class="ltx_td ltx_align_right">0.432</td>
<td class="ltx_td ltx_align_right">0.642</td>
<td class="ltx_td ltx_align_right">0.632</td>
<td class="ltx_td ltx_align_right">0.637</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">10 (sgt)</th>
<td class="ltx_td ltx_align_right ltx_border_bb">0.734</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.732</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.733</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.434</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.437</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.436</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.654</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.645</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.650</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS1.p6" class="ltx_para">
<p class="ltx_p">We have trained the models with the settings shown in Table
<a href="#S5.T2" title="Table 2 ‣ 5.1 Word2vec models ‣ 5 Experiments ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The values for minimum count (the minimum count
of word in corpus to be considered as a unique word), downsampling threshold
(for downsampling high frequency words), initial learning rate and minimum
learning rate are set to the values that were recommended by
Mikolov et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="Efficient Estimation of Word Representations in Vector Space" class="ltx_ref">6</a>]</cite>. Regarding the number of
epochs, it is set to only 5 as we found that increasing it results in worse
models. This occurs due to the fact that word2vec models are based on neural
networks with huge number of parameters and training them for a long time using
the same data will cause over-fitting. Finally, we have investigated the effect
of the window size and the vector size on the system’s performance.
</p>
</div>
<div id="S5.SS1.p7" class="ltx_para">
<p class="ltx_p">All the models are trained using wikipedia dumps (dumped on the
22<sup class="ltx_sup">nd</sup> of February 2020). Since the size of the English dump is
much larger than the size of the Kazakh and Serbo-Croatian dumps, a portion of
the English dump was used to make the sizes of the corpora comparable.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Results of the reference and the word2vec methods. The unigram-count and constraint-grammar methods
both rely on either annotated data or rule-based disambiguation.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_tt"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">English</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">Serbo-Croatian</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span class="ltx_text ltx_font_bold">Kazakh</span></th>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column"><span class="ltx_text ltx_font_bold">Model name</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T5.m1" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T5.m2" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T5.m3" class="ltx_Math" alttext="F" display="inline"><mi>F</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T5.m4" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T5.m5" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T5.m6" class="ltx_Math" alttext="F" display="inline"><mi>F</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T5.m7" class="ltx_Math" alttext="P" display="inline"><mi>P</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T5.m8" class="ltx_Math" alttext="R" display="inline"><mi>R</mi></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column"><math id="S5.T5.m9" class="ltx_Math" alttext="F" display="inline"><mi>F</mi></math></th>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">unigram-counts (la smoothing)</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.848</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.844</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.846</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.830</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.841</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.835</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.849</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.841</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.845</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">unigram-counts (simple good-turing smoothing)</td>
<td class="ltx_td ltx_align_right">0.856</td>
<td class="ltx_td ltx_align_right">0.853</td>
<td class="ltx_td ltx_align_right">0.855</td>
<td class="ltx_td ltx_align_right">0.830</td>
<td class="ltx_td ltx_align_right">0.840</td>
<td class="ltx_td ltx_align_right">0.835</td>
<td class="ltx_td ltx_align_right">0.849</td>
<td class="ltx_td ltx_align_right">0.841</td>
<td class="ltx_td ltx_align_right">0.845</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">constraint grammar (la smoothing)</td>
<td class="ltx_td ltx_align_right">0.776</td>
<td class="ltx_td ltx_align_right">0.774</td>
<td class="ltx_td ltx_align_right">0.775</td>
<td class="ltx_td ltx_align_right">0.607</td>
<td class="ltx_td ltx_align_right">0.622</td>
<td class="ltx_td ltx_align_right">0.615</td>
<td class="ltx_td ltx_align_right">0.702</td>
<td class="ltx_td ltx_align_right">0.697</td>
<td class="ltx_td ltx_align_right">0.699</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">constraint grammar (simple good-turing smoothing)</td>
<td class="ltx_td ltx_align_right">0.771</td>
<td class="ltx_td ltx_align_right">0.768</td>
<td class="ltx_td ltx_align_right">0.769</td>
<td class="ltx_td ltx_align_right">0.557</td>
<td class="ltx_td ltx_align_right">0.572</td>
<td class="ltx_td ltx_align_right">0.564</td>
<td class="ltx_td ltx_align_right">0.641</td>
<td class="ltx_td ltx_align_right">0.637</td>
<td class="ltx_td ltx_align_right">0.639</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">random (la smoothing)</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.705</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.703</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.704</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.434</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.448</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.441</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.577</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.573</td>
<td class="ltx_td ltx_align_right ltx_border_t">0.575</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">random (simple good-turing smoothing)</td>
<td class="ltx_td ltx_align_right">0.705</td>
<td class="ltx_td ltx_align_right">0.703</td>
<td class="ltx_td ltx_align_right">0.704</td>
<td class="ltx_td ltx_align_right">0.450</td>
<td class="ltx_td ltx_align_right">0.459</td>
<td class="ltx_td ltx_align_right">0.455</td>
<td class="ltx_td ltx_align_right">0.606</td>
<td class="ltx_td ltx_align_right">0.600</td>
<td class="ltx_td ltx_align_right">0.603</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">equal</td>
<td class="ltx_td ltx_align_right">0.687</td>
<td class="ltx_td ltx_align_right">0.687</td>
<td class="ltx_td ltx_align_right">0.687</td>
<td class="ltx_td ltx_align_right">0.554</td>
<td class="ltx_td ltx_align_right">0.558</td>
<td class="ltx_td ltx_align_right">0.556</td>
<td class="ltx_td ltx_align_right">0.647</td>
<td class="ltx_td ltx_align_right">0.633</td>
<td class="ltx_td ltx_align_right">0.640</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">analysis length</td>
<td class="ltx_td ltx_align_right">0.685</td>
<td class="ltx_td ltx_align_right">0.685</td>
<td class="ltx_td ltx_align_right">0.685</td>
<td class="ltx_td ltx_align_right">0.542</td>
<td class="ltx_td ltx_align_right">0.549</td>
<td class="ltx_td ltx_align_right">0.545</td>
<td class="ltx_td ltx_align_right">0.670</td>
<td class="ltx_td ltx_align_right">0.662</td>
<td class="ltx_td ltx_align_right">0.666</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">word2vec (la smoothing)</td>
<td class="ltx_td ltx_align_right">0.740</td>
<td class="ltx_td ltx_align_right">0.739</td>
<td class="ltx_td ltx_align_right">0.739</td>
<td class="ltx_td ltx_align_right">0.473</td>
<td class="ltx_td ltx_align_right">0.476</td>
<td class="ltx_td ltx_align_right">0.474</td>
<td class="ltx_td ltx_align_right">0.652</td>
<td class="ltx_td ltx_align_right">0.642</td>
<td class="ltx_td ltx_align_right">0.647</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">word2vec (sgt smoothing)</td>
<td class="ltx_td ltx_align_right">0.734</td>
<td class="ltx_td ltx_align_right">0.732</td>
<td class="ltx_td ltx_align_right">0.733</td>
<td class="ltx_td ltx_align_right">0.476</td>
<td class="ltx_td ltx_align_right">0.480</td>
<td class="ltx_td ltx_align_right">0.478</td>
<td class="ltx_td ltx_align_right">0.654</td>
<td class="ltx_td ltx_align_right">0.645</td>
<td class="ltx_td ltx_align_right">0.650</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">word2vec (la smoothing) (pretrain - WP dump 17)</td>
<td class="ltx_td ltx_align_right">0.708</td>
<td class="ltx_td ltx_align_right">0.707</td>
<td class="ltx_td ltx_align_right">0.708</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">word2vec (sgt smoothing) (pretrain - WP dump 17)</td>
<td class="ltx_td ltx_align_right">0.702</td>
<td class="ltx_td ltx_align_right">0.702</td>
<td class="ltx_td ltx_align_right">0.702</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">word2vec (la smoothing) (pretrain - CoNLL17)</td>
<td class="ltx_td ltx_align_right">0.720</td>
<td class="ltx_td ltx_align_right">0.720</td>
<td class="ltx_td ltx_align_right">0.720</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">word2vec (sgt smoothing) (pretrain - CoNLL17)</td>
<td class="ltx_td ltx_align_right">0.713</td>
<td class="ltx_td ltx_align_right">0.712</td>
<td class="ltx_td ltx_align_right">0.713</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">word2vec (la smoothing) (pretrain - CoNLL17)</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">-</td>
<td class="ltx_td ltx_align_right">0.642</td>
<td class="ltx_td ltx_align_right">0.632</td>
<td class="ltx_td ltx_align_right">0.637</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb">word2vec (sgt smoothing) (pretrain - CoNLL17)</td>
<td class="ltx_td ltx_align_right ltx_border_bb">-</td>
<td class="ltx_td ltx_align_right ltx_border_bb">-</td>
<td class="ltx_td ltx_align_right ltx_border_bb">-</td>
<td class="ltx_td ltx_align_right ltx_border_bb">-</td>
<td class="ltx_td ltx_align_right ltx_border_bb">-</td>
<td class="ltx_td ltx_align_right ltx_border_bb">-</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.653</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.643</td>
<td class="ltx_td ltx_align_right ltx_border_bb">0.648</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Evaluation and Results</h2>

<div id="S6.p1" class="ltx_para">
<p class="ltx_p">To evaluate the performance of a generated weightlist, first a weighted FST is
generated. For each labeled token in the tagged corpus, the list of candidate
morphological analyses are generated using the weighted FST. Finally, the most
probable analysis (the one having the least weight) is considered as the model’s
prediction.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Evaluation corpora</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p class="ltx_p">Apertium<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup>
              <span class="ltx_tag ltx_tag_note">4</span>
              
              
              
            Apertium is a machine translation program targeting lower
resourced languages.</span></span></span> has tagged corpora for the supported languages. The
evaluation process relied on Apertium’s tagged corpora for English, Kazakh and
Serbo-Croatian. These corpora are distributed under the GNU General Public
License.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p class="ltx_p">For the English corpus, the number of token/analysis pairs in corpus is 29,650.
The number of unique tag combinations in the corpus is 198. The distribution for
the number of candidate analyses for each token before disambiguation is shown
in Figure <a href="#S6.F4" title="Figure 4 ‣ 6.1 Evaluation corpora ‣ 6 Evaluation and Results ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S6.F4" class="ltx_figure"><img src="x4.png" id="S6.F4.g1" class="ltx_graphics ltx_centering" width="677" height="464" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> Ratio of tokens having 1, 2, .. number of
candidate analyses in the English corpus</figcaption>
</figure>
<div id="S6.SS1.p3" class="ltx_para">
<p class="ltx_p">Clearly, 65.8% of the corpus is actually not ambiguous. This is a result of the
nature of the English Morphology.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p class="ltx_p">On the other hand, Kazakh is a morphologically complex language. For the Kazakh
corpus, the number of token/analysis pairs in corpus is 9762. The number of
unique tag combinations in the corpus is 655. And the distribution for the
number of candidate analyses for each token before disambiguation is shown in
Figure <a href="#S6.F5" title="Figure 5 ‣ 6.1 Evaluation corpora ‣ 6 Evaluation and Results ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. A single token in Kazakh might have more than
20 candidate analysis. More than 0.5% of the corpus has 16 or more analyses
which makes the disambiguation process much harder. Additionally, the Kazakh
corpus has 655 unique tag combinations compared to the 198 tags in the English
corpus despite the fact that the Kazakh corpus size is about one third that of
the English corpus.</p>
</div>
<div id="S6.SS1.p5" class="ltx_para">
<p class="ltx_p">Similarly, Serbo-Croatian has 20127 token/analysis pairs with 598 unique tag
combinations. Figure <a href="#S6.F6" title="Figure 6 ‣ 6.1 Evaluation corpora ‣ 6 Evaluation and Results ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows that more than 5% of
the tokens in the tagged corpus had 16 or more candidate morphological analyses.</p>
</div>
<figure id="S6.F5" class="ltx_figure"><img src="x5.png" id="S6.F5.g1" class="ltx_graphics ltx_centering" width="677" height="539" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span> Ratio of tokens having 1, 2, .. number of
candidate analyses in the Kazakh corpus</figcaption>
</figure>
<figure id="S6.F6" class="ltx_figure"><img src="x6.png" id="S6.F6.g1" class="ltx_graphics ltx_centering" width="678" height="521" alt="">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span> Ratio of tokens having 1, 2, ..
number of candidate analyses in the Serbo-Croatian corpus</figcaption>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Results</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p class="ltx_p">To evaluate the weighted FSTs, precision, recall and F1 score are used. For each
tag in the tagged corpus, precision is calculated as <math id="S6.SS2.p1.m1" class="ltx_Math" alttext="P=\frac{TP}{TP+FP}" display="inline"><mrow><mi>P</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mo>⁢</mo><mi>P</mi></mrow><mrow><mrow><mi>T</mi><mo>⁢</mo><mi>P</mi></mrow><mo>+</mo><mrow><mi>F</mi><mo>⁢</mo><mi>P</mi></mrow></mrow></mfrac></mrow></math>,
recall is calculated as <math id="S6.SS2.p1.m2" class="ltx_Math" alttext="R=\frac{TP}{TP+FN}" display="inline"><mrow><mi>R</mi><mo>=</mo><mfrac><mrow><mi>T</mi><mo>⁢</mo><mi>P</mi></mrow><mrow><mrow><mi>T</mi><mo>⁢</mo><mi>P</mi></mrow><mo>+</mo><mrow><mi>F</mi><mo>⁢</mo><mi>N</mi></mrow></mrow></mfrac></mrow></math> and F1 score is calculated the
harmonic mean of the precision and recall <math id="S6.SS2.p1.m3" class="ltx_Math" alttext="F1=\frac{2*(P*R)}{P+R}" display="inline"><mrow><mrow><mi>F</mi><mo>⁢</mo><mn>1</mn></mrow><mo>=</mo><mfrac><mrow><mn>2</mn><mo>*</mo><mrow><mo stretchy="false">(</mo><mrow><mi>P</mi><mo>*</mo><mi>R</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>P</mi><mo>+</mo><mi>R</mi></mrow></mfrac></mrow></math>
where TP is the number of True positives, FP is the number of False positives
and FN is the number of False negatives. For example, To calculate the
precision, recall and F1 score for the analysis (classification class)
<span class="ltx_text ltx_font_typewriter">euro&lt;n&gt;&lt;sg&gt;</span>, the correct labels and the predictions of the
disambiguation model are divided into two bins: positive (the model’s prediction
is equal to the class in question <span class="ltx_text ltx_font_typewriter">euro&lt;n&gt;&lt;sg&gt;</span>) and negative (the
model’s prediction isn’t equal to the class in question <span class="ltx_text ltx_font_typewriter">euro&lt;n&gt;&lt;sg&gt;</span>).
The four combinations of the label and prediction are:</p>
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p class="ltx_p">True Positive (TP): The correct label is <span class="ltx_text ltx_font_typewriter">euro&lt;n&gt;&lt;sg&gt;</span> and the
model’s prediction is <span class="ltx_text ltx_font_typewriter">euro&lt;n&gt;&lt;sg&gt;</span>.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p class="ltx_p">False Positive (FP): The correct label isn’t <span class="ltx_text ltx_font_typewriter">euro&lt;n&gt;&lt;sg&gt;</span> and the
model’s prediction is <span class="ltx_text ltx_font_typewriter">euro&lt;n&gt;&lt;sg&gt;</span>.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p class="ltx_p">False Negative (FN): The correct label is <span class="ltx_text ltx_font_typewriter">euro&lt;n&gt;&lt;sg&gt;</span> but the
model prediction isn’t <span class="ltx_text ltx_font_typewriter">euro&lt;n&gt;&lt;sg&gt;</span>.</p>
</div>
</li>
<li id="S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i4.p1" class="ltx_para">
<p class="ltx_p">True Negative (TN): The correct label isn’t <span class="ltx_text ltx_font_typewriter">euro&lt;n&gt;&lt;sg&gt;</span> and the
model’s prediction isn’t <span class="ltx_text ltx_font_typewriter">euro&lt;n&gt;&lt;sg&gt;</span>.</p>
</div>
</li>
</ul>
<p class="ltx_p">After counting the values of TP / FP / FN for the current analysis
<span class="ltx_text ltx_font_typewriter">euro&lt;n&gt;&lt;sg&gt;</span>, the precision, recall and F1 scores are computed as shown
above. This operation is repeated for all the analyses (classification classes)
in the evaluation data-set. The precision, recall and F1 metrics are finally
merged using a weighted macro-average among all these tags.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p class="ltx_p">Table <a href="#S5.T5" title="Table 5 ‣ 5.1 Word2vec models ‣ 5 Experiments ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> reports the evaluation metrics for the reference methods
in addition to our word2vec based method for the English, Kazakh and
Serbo-Croatian languages.</p>
</div>
<section id="S6.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1 </span>Hyperparameter optimization</h4>

<div id="S6.SS2.SSS1.p1" class="ltx_para">
<p class="ltx_p">Results of the hyperparameter optimizations for the vector and window sizes are
shown in Tables <a href="#S5.T3" title="Table 3 ‣ 5.1 Word2vec models ‣ 5 Experiments ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, <a href="#S5.T4" title="Table 4 ‣ 5.1 Word2vec models ‣ 5 Experiments ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. First, three models are
trained for each language with all the other parameters set as the values
indicated in <a href="#S5.T2" title="Table 2 ‣ 5.1 Word2vec models ‣ 5 Experiments ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and vector sizes are 100, 200, 300.
It was found that the vector size of 100 suited English better while vector size
of 300 was better for both Kazakh and Serbo-Croatian. For the window size, it
was found that smaller window sizes generated more precise models for the three
languages.</p>
</div>
</section>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Discussion</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p class="ltx_p">We have noticed that the results of Skip-gram word2vec models is sensitive to
values like the random seed used during the training of the model. We found that
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="Don’t Get Fooled by Word Embeddings - Better Watch their Neighborhood" class="ltx_ref">3</a>]</cite> have made deeper investigation into the effect of the
randomness of the word2vec models on the reliability of these models. They
showed that the most similar words for an input word are highly affected by
changing the random seed concluding that the results of these models aren’t
reliable enough and are sensitive to the seed.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p class="ltx_p">Additionally, training word2vec models from scratch and tuning them seemed
better than using the pretrained models even if these pretrained models used
much larger corpora.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p class="ltx_p">Moreover, all the reference methods performed better than the random method.
This indicates that even having a simple heuristic for sorting the analyses is
an improvement to the unweighted morphological analyzer.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p class="ltx_p">The paper explores exploiting the word2vec model’s (as an embedding model)
ability to capture the semantic similarities between words for disambiguating
the results of a morphological analyzer. This method proved to be useful for
both morphological analysis and tagging tasks. Our novel word2vec-based method
performed better than the basic unsupervised reference models. And despite the
fact that it didn’t improve over the supervised unigram method, it proved to be
able to improve the analyses order and consequently the FST’s accuracy without
relying on manually tagged corpora. This method was very successful in
disambiguating English analyses but its performance dropped for morphologically
complex languages like Kazakh and Serbo-Croatian. We have also shown that
directly weighting the analyses of words instead of relying on the context to
disambiguate the results is a successful technique to deploy.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Bibliographical References</h2>

</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul id="bib.L1" class="ltx_biblist">
<li id="bib.bib10" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. R. Beesley</span><span class="ltx_text ltx_bib_year"> (2003)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Computational Morphology and Finite-state Methods</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">NATO SCIENCE SERIES SUB SERIES III COMPUTER AND SYSTEMS SCIENCES</span> <span class="ltx_text ltx_bib_volume">188</span>, <span class="ltx_text ltx_bib_pages"> pp. 61–100</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib172" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Fares, A. Kutuzov, S. Oepen, and E. Velldal</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Word Vectors, Reuse, and Replicability: Towards a Community Repository of Large-text Resources</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 21st Nordic Conference on Computational
Linguistics, NoDaLiDa, 22-24 May 2017, Gothenburg, Sweden</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_publisher">Language Technology Group, Department of Informatics,
University of Oslo, Norway</span>, <span class="ltx_text ltx_bib_pages"> pp. 271–276</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 1650-3740</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.SS1.p2" title="5.1 Word2vec models ‣ 5 Experiments ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Hellrich and U. Hahn</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Don’t Get Fooled by Word Embeddings - Better Watch their Neighborhood</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">DH</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S6.SS3.p1" title="6.3 Discussion ‣ 6 Evaluation and Results ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§6.3</span></a>.
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. Karlsson</span><span class="ltx_text ltx_bib_year"> (1990)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Constraint grammar as a framework for parsing unrestricted text</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 13th International Conference of Computational Linguistics</span>,  <span class="ltx_text ltx_bib_editor">H. Karlgren (Ed.)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">3</span>, <span class="ltx_text ltx_bib_place">Helsinki</span>, <span class="ltx_text ltx_bib_pages"> pp. 168–173</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.SSS2.p1" title="4.1.2 Constraint Grammar-based weightlist ‣ 4.1 Reference models ‣ 4 Development ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.1.2</span></a>.
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Lindén and T. Pirinen</span><span class="ltx_text ltx_bib_year"> (2009-07)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Weighting finite-state morphological analyzers using HFST tools</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">FSMNLP 2009</span>,  <span class="ltx_text ltx_bib_editor">B. Watson, D. Courie, L. Cleophas, and P. Rautenbach (Eds.)</span>,
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-1-86854-743-2</span>,
<a href="http://www.ling.helsinki.fi/klinden/pubs/fsmnlp2009weighting.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS1.SSS1.p1" title="4.1.1 Unigram-counts based weightlist ‣ 4.1 Reference models ‣ 4 Development ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.1.1</span></a>.
</span>
</li>
<li id="bib.bib99" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Mikolov, K. Chen, G. Corrado, and J. Dean</span><span class="ltx_text ltx_bib_year"> (2013)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Efficient Estimation of Word Representations in Vector Space</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1301.3781</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S4.SS2.p1" title="4.2 The word2vec-based weightlist ‣ 4 Development ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§4.2</span></a>,
<a href="#S5.SS1.p1" title="5.1 Word2vec models ‣ 5 Experiments ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>,
<a href="#S5.SS1.p5" title="5.1 Word2vec models ‣ 5 Experiments ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>,
<a href="#S5.SS1.p6" title="5.1 Word2vec models ‣ 5 Experiments ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li id="bib.bib106" class="ltx_bibitem ltx_bib_incollection">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Mohri</span><span class="ltx_text ltx_bib_year"> (2004)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Weighted Finite-state Transducer Algorithms. An Overview</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Formal Languages and Applications</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 551–563</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S1.p2" title="1 Introduction ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li id="bib.bib153" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. Sánchez-Martínez, J. A. Pérez-Ortiz, and M. L. Forcada</span><span class="ltx_text ltx_bib_year"> (2008-03)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Using Target-language Information to Train Part-of-speech Taggers for Machine Translation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Machine Translation</span> <span class="ltx_text ltx_bib_volume">22</span> (<span class="ltx_text ltx_bib_number">1-2</span>), <span class="ltx_text ltx_bib_pages"> pp. 29–66</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text issn ltx_bib_external">ISSN 0922-6567</span>,
<a href="http://dx.doi.org/10.1007/s10590-008-9044-3" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.1007/s10590-008-9044-3" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p4" title="2 Related Work ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib154" class="ltx_bibitem ltx_bib_article">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Schiller</span><span class="ltx_text ltx_bib_year"> (2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">German compound analysis with wfsc</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Finite-State Methods and Natural Language Processing</span>, <span class="ltx_text ltx_bib_pages"> pp. 239–246</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p3" title="2 Related Work ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib160" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Q. Shen, D. Clothiaux, E. Tagtow, P. Littell, and C. Dyer</span><span class="ltx_text ltx_bib_year"> (2016-12)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The Role of Context in Neural Morphological Disambiguation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of COLING 2016, the 26th International
Conference on Computational Linguistics: Technical Papers</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Osaka, Japan</span>, <span class="ltx_text ltx_bib_pages"> pp. 181–191</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://www.aclweb.org/anthology/C16-1018" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p5" title="2 Related Work ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib170" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">E. Uí Dhonnchadha and J. Van Genabith</span><span class="ltx_text ltx_bib_year"> (2006-05)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A Part-of-speech Tagger for Irish Using Finite-state Morphology and Constraint Grammar Disambiguation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Fifth International Conference on Language
Resources and Evaluation (LREC’06)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Genoa, Italy</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.lrec-conf.org/proceedings/lrec2006/pdf/193_pdf.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p2" title="2 Related Work ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib178" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Washington, I. Salimzyanov, and F. Tyers</span><span class="ltx_text ltx_bib_year"> (2014-05)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Finite-state morphological transducers for three kypchak languages</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Reykjavik, Iceland</span>, <span class="ltx_text ltx_bib_pages"> pp. 3378–3385</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/1207_Paper.pdf" title="" class="ltx_ref ltx_bib_external">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S5.p1" title="5 Experiments ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§5</span></a>.
</span>
</li>
<li id="bib.bib180" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Webb</span><span class="ltx_text ltx_bib_year"> (1993)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning Decision Lists by Prepending Inferred Rules.</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">In Proceedings of the Australian Workshop on Machine Learning and Hybrid Systems</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 6–10</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li id="bib.bib186" class="ltx_bibitem ltx_bib_inproceedings">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Yuret and F. Türe</span><span class="ltx_text ltx_bib_year"> (2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning Morphological Disambiguation Rules for Turkish</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Main Conference on Human Language Technology
Conference of the North American Chapter of the Association of
Computational Linguistics</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">HLT-NAACL ’06</span>, <span class="ltx_text ltx_bib_place">Stroudsburg, PA, USA</span>, <span class="ltx_text ltx_bib_pages"> pp. 328–334</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a href="https://doi.org/10.3115/1220835.1220877" title="" class="ltx_ref ltx_bib_external">Link</a>,
<a href="http://dx.doi.org/10.3115/1220835.1220877" title="" class="ltx_ref doi ltx_bib_external">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a href="#S2.p1" title="2 Related Work ‣ An unsupervised method for weighting finite-state morphological analyzers 1 footnote 1 1 footnote 1 LREC proceedings is CC-BY-NC, copyrighted to ELRA, see http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.474.pdf for original" class="ltx_ref"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon May 25 23:09:27 2020 by <a href="http://dlmf.nist.gov/LaTeXML/">LaTeXML <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="[LOGO]"></a>
</div></footer>
</div>
</body>
</html>
